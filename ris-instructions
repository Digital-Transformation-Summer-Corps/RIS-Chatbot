# RIS Instructions

# We store some libraries such as cuda in the dt-summer-corp storage, so we need to mount it to the container
export LSF_DOCKER_VOLUMES="/storage2/fs1/dt-summer-corp/Active/common/projects/ai-on-washu-infrastructure/chatbot/libs:/usr/local/modules /storage2/fs1/dt-summer-corp:/storage2/fs1/dt-summer-corp $HOME:$HOME"

# Submit the job requesting 1 GPU. Map the port 8501 to host port 8003. Use the ris_chatbot docker image.
LSF_DOCKER_PORTS='8003:8501' bsub -Is -G compute-artsci -q artsci-interactive -n 8 -R 'select[port8003=1]' -R 'gpuhost rusage[mem=120GB]' -gpu 'num=1' -a 'docker(fizban007/ris_chatbot)'  /usr/bin/bash

# This the work directory
cd /storage2/fs1/dt-summer-corp/Active/common/projects/ai-on-washu-infrastructure/chatbot/

# Clone the RIS-Chatbot repository
git clone https://github.com/fizban007/RIS-Chatbot.git

# Change to the RIS-Chatbot directory
cd RIS-Chatbot

# Change .env.example to .env and potentially tweak the settings
mv .env.example .env

# Create a virtual environment
python3 -m venv venv

# Activate the virtual environment
source venv/bin/activate

# Install the requirements
pip install -r requirements.txt

### Setting up the RAG database
python confluence.py
# The first time you run this, it may ask for some authentication details. Choose the first option and just input the root URL of the RIS Confluence instance: https://washu.atlassian.net/. This will take a while to run (~5 minutes) and export all the pages to the RIS\ User\ Documentation directory.

# Clean up the markdown files to change the code block syntax from ```java to ```
python fix_markdown.py

# Compute the embeddings and store them in the RAG database
python manage_rag.py load-docs --dir ./RIS\ User\ Documentation/RIS\ User\ Documentation

# If you need to clear the database and re-index it, run this
python manage_rag.py clear-collection

### Setting up the LLM server

# Clone the llama.cpp repository. We use it to run the LLM server that powers the chatbot.
git clone https://github.com/ggml-org/llama.cpp.git

cd llama.cpp

# We target a cuda-based build [TODO: fill in other options]
cmake -B build -DGGML_CUDA=ON -DBUILD_SHARED_LIBS=OFF -DLLAMA_CURL=OFF -DCMAKE_BUILD_TYPE=Release

# Build the server. If you only requested 1 CPU core then it may take a while.
cmake --build build -j

# Download the model. May need to authenticate when running the first time?
huggingface-cli download unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF --include "Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL.gguf" --local-dir models/

# Run the server. Eventually we may need to explore switching to vllm since it's better optimized for many users.
LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/ ./build/bin/llama-server --jinja --port 8000 --model models/Mistral-Small-3.2-24B-Instruct-2506-UD-Q8_K_XL.gguf -ngl 99 -fa -c 65536 --mlock --cache-reuse 256 --temp 0.15 --top-k -1 --top-p 1.00 &

# Run the streamlit app
cd ..
STREAMLIT_SERVER_ADDRESS=0.0.0.0 streamlit run streamlit_app_simple.py &