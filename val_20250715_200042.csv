question_number,question,chatbot_response,gemini_response,score,file,file_path
1,I'm not comfortable with the command line. Is there a graphical interface I can use to run jobs?,"Based on the provided context, there is no mention of a graphical interface for running jobs on the Compute1 or Compute2 platforms. Both platforms primarily use command-line interfaces (CLI) for job submission and management, such as `bsub` for Compute1 and `srun`/`sbatch` for Compute2.

If you're not comfortable with the command line, you might want to explore alternative options or seek assistance from your system administrators or IT support team, as they may have additional tools or interfaces available that are not documented in the provided context.

**Sources:** Compute2+Quickstart, Job+Execution+Examples","2
",2,RIS User Documentation.md,RIS User Documentation/RIS User Documentation/RIS User Documentation.md
2,"How do I find and use a specific software application, like R or Python, on the compute platform?","To find and use specific software applications like R or Python on the Compute2 platform, follow these steps:

1. **Access the Compute2 Platform**: You can connect via SSH or use Open On Demand (OOD) for a web-based interface. For SSH, use your WashU Key and password to connect to `compute1-client-1.ris.wustl.edu`.

2. **Using OOD**:
   - Log in to OOD at <http://ood.ris.wustl.edu>.
   - You can start applications like Jupyter, Matlab, Rstudio, or use the Compute RIS Desktop.

3. **Using R or Python**:
   - Both R and Python are listed as RIS Maintained Module Applications. You can load them using the `ml` command.
   - For example, to load Python, use:
     ```bash
     ml python
     ```
   - To load R, use:
     ```bash
     ml R
     ```

4. **Using Modules**:
   - To view available modules, use:
     ```bash
     module avail
     ```
   - To list loaded modules, use:
     ```bash
     module list
     ```
   - To unload a module, use:
     ```bash
     module unload package
     ```

For more detailed instructions, refer to the [Compute2 Applications](https://washu.atlassian.net/wiki/spaces/RUD/pages/2087747658/Compute2+Applications) documentation.

**Sources:** Compute1+Quickstart, Compute2+Applications","2
",2,RIS User Documentation.md,RIS User Documentation/RIS User Documentation/RIS User Documentation.md
3,How can I connect to my RIS storage folder from my personal Windows or Mac computer?,"To connect to your RIS storage folder from your personal Windows or Mac computer, follow these steps:

### For Windows:
1. **Browse the Network Folder**:
   - Open Windows File Explorer.
   - Enter the UNC path in the address bar: `\\storageN.ris.wustl.edu\PI WashU Key or Share Name` (replace `storageN` with `storage1` or `storage2`).
   - Press Enter.
   - You may be prompted for credentials. Enter `accounts\WashU Key` and your WashU Key password.

2. **Create a Desktop Shortcut** (Optional):
   - Once connected, hover over the drive name and drag it to your desktop to create a shortcut.

### For MacOS:
1. **Prepare Your Mac**:
   - Disable `.DS_Store` files by running in Terminal:
     ```
     defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true
     ```
   - Create or update the `/etc/nsmb.conf` file:
     ```
     sudo tee /etc/nsmb.conf <<EOF
     [default]
     smb_neg=smb2_only
     dir_cache_off=yes
     notify_off=yes
     soft=yes
     streams=yes
     file_ids_off=yes
     EOF
     ```
   - Synchronize the SMB config:
     ```
     /usr/libexec/smb-sync-preferences
     ```

2. **Connect to the Server**:
   - In Finder, click “Go” > “Connect to Server” (or press ⌘ K).
   - Enter the server address: `smb://storageN.ris.wustl.edu/PI WashU Key ID`.
   - Enter your WashU Key credentials when prompted.

3. **Make the Shortcut Permanent** (Optional):
   - Go to System Preferences > Users and Groups > Login Items.
   - Click the “+” button and navigate to your Research Drive to add it to your login items.

For detailed steps and troubleshooting, refer to the [Windows guide](https://washu.atlassian.net/wiki/spaces/RUD/pages/1795588135/Connecting+to+Storage+from+Windows) or the [MacOS guide](https://washu.atlassian.net/wiki/spaces/RUD/pages/1795784747/Connecting+to+Storage+from+MacOS).

**Sources:** Connecting+to+Storage+from+MacOS, Connecting+to+Storage+from+Windows","4
",4,RIS User Documentation.md,RIS User Documentation/RIS User Documentation/RIS User Documentation.md
4,When will applications for Compute2 be available?,"The context provided does not specify an exact date for when applications for Compute2 will be available. However, it mentions that the documentation is actively being developed and is in flux, suggesting that the applications and related resources are currently under development. For the most up-to-date information, you may want to check the linked documentation or contact the support team directly.

**Sources:** Compute2, Compute2+Applications","5
",5,Applications.md,RIS User Documentation/RIS User Documentation/Applications.md
5,What is the difference between a RIS Developed Container and a Non-RIS Docker Image?,"Based on the provided context, here's the difference between a RIS Developed Container and a Non-RIS Docker Image:

1. **RIS Developed Containers**:
   - These are Docker images developed in collaboration with RIS (Research Infrastructure Services).
   - They are categorized into three groups:
     - **Hosted and Maintained Images**: These are hosted in an RIS repository and maintained by RIS. Examples include AFNI/TORTOISE, AlphaFold, MATLAB, etc.
     - **Developed But Not Maintained**: These are developed with RIS but their upkeep is the responsibility of the users. Examples include Globus CLI and Integrative Genomics Viewer.
     - **Images in Development**: These are in a trial phase for usage.

2. **Non-RIS Docker Images**:
   - These are Docker images that are not developed or maintained by RIS.
   - They are not listed on the RIS Developed Containers page and their maintenance and support are not handled by RIS.

In summary, RIS Developed Containers are specifically created and managed (to varying extents) by RIS, while Non-RIS Docker Images are any other Docker images not associated with RIS.

**Sources:** Docker, RIS+Developed+Containers","3
",3,Applications.md,RIS User Documentation/RIS User Documentation/Applications.md
6,How can I request a new application that is not on this list?,"To request a new application that is not on the list, you can reach out to the RIS Service Desk. You can find the request form [here](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/55).

**Sources:** Compute2+Applications, THPC+Installed+Applications","1
",1,Applications.md,RIS User Documentation/RIS User Documentation/Applications.md
7,How do I perform these steps on a Windows computer?,"Based on the provided context, the instructions for connecting to storage from a Windows computer are as follows:

1. **Browsing a Network Folder**:
   - Open Windows File Explorer.
   - Enter the UNC folder path in the address bar: `\\storageN.ris.wustl.edu\PI WashU Key or Share Name` (replace `storageN` with the specific storage platform like `storage1` or `storage2`).
   - Press Enter.
   - If prompted for credentials, click on `Use another account` and enter your credentials as `accounts\WashU Key` and your WashU Key password.

2. **Creating a Desktop Shortcut**:
   - Once your storage drive is open in File Explorer, hover over the drive name.
   - Click and drag to create a shortcut on your Desktop, Start menu, taskbar, or File Explorer Quick Access tab.

Regarding the Local VNC Viewer instructions, the context specifies that the Windows platform is currently not supported for connecting to RIS compute services via VNC GUI. Therefore, these steps cannot be performed on a Windows computer.

**Sources:** Connecting+to+Storage+from+Windows, Local+VNC+Viewer","1
",1,SSH Private-Public Key Pair.md,RIS User Documentation/RIS User Documentation/SSH Private-Public Key Pair.md
8,I already have a default SSH key (`~/.ssh/id_rsa`). Can I use that one instead of creating a new one?,"Yes, you can use your existing default SSH key (`~/.ssh/id_rsa`) instead of creating a new one. The process for setting it up with the compute client remains the same. You would follow the steps to add your key to the `ssh-agent` and then copy it to the compute client using `ssh-copy-id`. Here’s a summary of the steps:

1. **Add your key to the `ssh-agent`:**
   ```bash
   eval `ssh-agent`
   ssh-add ~/.ssh/id_rsa
   ```

2. **Copy the SSH key to the compute client:**
   ```bash
   ssh-copy-id -i ~/.ssh/id_rsa ${compute_username}@compute1-client-1.ris.wustl.edu
   ```

After completing these steps, you should be able to SSH into the compute client without entering a passphrase.

**Sources:** Connecting+to+Storage+from+MacOS, SSH+Private-Public+Key+Pair","5
",5,SSH Private-Public Key Pair.md,RIS User Documentation/RIS User Documentation/SSH Private-Public Key Pair.md
9,"I followed all the steps, but I'm still being prompted for my password when I connect. What could be wrong?","If you're still being prompted for your password when connecting to the storage platform, there are a few things to check:

1. **Ensure Correct Credentials**: Make sure you are entering your credentials in the correct format. For DOM users or those on a Windows Domain, you should use `accounts\WashU Key` as the username followed by your WashU Key password.

2. **Network Connection**: Verify that you have a stable connection to the WashU network. The login window might take some time to appear the first time you connect, so be patient.

3. **UNC Path**: Double-check that you have entered the correct UNC path in the format `\\storageN.ris.wustl.edu\PI WashU Key or Share Name`.

If you've confirmed all these details and are still experiencing issues, you may need to contact the [Service Desk](https://servicedesk.ris.wustl.edu/) for further assistance.

**Sources:** Connecting+to+Storage+from+Windows, Troubleshooting+Connection+to+the+Storage+Platforms","1
",1,SSH Private-Public Key Pair.md,RIS User Documentation/RIS User Documentation/SSH Private-Public Key Pair.md
10,How much do RIS services cost and where can I find a price list?,"RIS services offer different pricing models depending on the type of service you need. For the **Data Storage Platform**, you can choose between **Consumption** (pay only for storage consumed above the free 5 TB) or **Subscription** (substantial savings for subscriptions in 100 TB increments). For the **Scientific Compute Platform**, options include **General** (best-effort scheduling), **Subscription** (pre-pay for priority access), and **Consumption** (pay for on-demand access).

For detailed pricing, you can refer to the following resources:
- **Data Storage Platform rate sheet**: [https://wustl.box.com/s/flqip0b0npzqqoopzv72k86m01bp3pc0](https://wustl.box.com/s/flqip0b0npzqqoopzv72k86m01bp3pc0)
- **Compute Platform rate sheet**: [https://wustl.box.com/s/dwmwgnpctqeqjhxanfi4e5yw24626zmx](https://wustl.box.com/s/dwmwgnpctqeqjhxanfi4e5yw24626zmx)

For more information, visit the [RIS website](https://ris.wustl.edu/services/).

**Sources:** FAQ, Requesting+RIS+Services","1
",1,FAQ.md,RIS User Documentation/RIS User Documentation/FAQ.md
11,I'm not sure where to store my files. What is the difference between my home directory and my storage allocation?,"The context provided does not directly address the difference between your home directory and your storage allocation. However, based on general knowledge and the context about storage platforms, here is a concise explanation:

- **Home Directory**: This is typically a smaller, personal storage space provided for your immediate use. It is often used for configuration files, scripts, and temporary files. It usually has limited storage capacity and is directly accessible from your compute environment.

- **Storage Allocation**: This is a larger, dedicated storage space that you can request for your projects. It is designed for storing larger datasets, shared files, and long-term data. Storage allocations can be accessed via various methods such as SMB mounts, Globus, or directly from compute platforms like Compute1 and Compute2.

For specific details about your home directory and storage allocation, you may need to consult your system documentation or contact your IT support team.

**Sources:** FAQ, Storage2+Access+Control","3
",3,FAQ.md,RIS User Documentation/RIS User Documentation/FAQ.md
12,What is the best way to share data with a collaborator from another university?,"Based on the provided context, the best way to share data with a collaborator from another university using Globus is as follows:

1. **Create a Guest Collection**: In Globus, create a guest collection for the data you want to share. This can be done by selecting the folder containing the files and clicking ""Share"" in the right command pane.

2. **Set Permissions**: Grant your collaborator access by adding their identity or email address in the ""Identity/E-mail"" field. You can specify whether they have read or write access.

3. **Send Notification**: Your collaborator will receive an email notification with a link to the shared endpoint. They can then log into Globus to access the data.

This method allows you to share data securely without requiring your collaborator to have an account on the server where your data is stored. For more detailed instructions, refer to the ""Sharing Data ('Collections')"" section in the context.

**Sources:** Moving+Data+With+Globus, WashU+ELN+FAQ","4
",4,FAQ.md,RIS User Documentation/RIS User Documentation/FAQ.md
13,What skills and software are required to use the Compute1 scientific compute platform?,"To use the Compute1 scientific compute platform, you need the following skills and software:

1. **Basic Command Line Interface (CLI) Skills**: Familiarity with SSH and command-line operations is essential, especially for connecting to the platform and managing jobs.

2. **SSH Client**: You need an SSH client to connect to the Compute1 platform. This can be:
   - Built-in terminal on Mac or Linux
   - Command prompt or PowerShell on Windows 10/11
   - Software like PuTTY or MobaXterm for Windows

3. **VPN Access**: If you are off-campus, you need to use a VPN (e.g., `msvpn.wusm.wustl.edu` or `danforthvpn.wustl.edu`) to access the platform.

4. **Web Browser**: For accessing Open OnDemand (OOD), a web browser is required.

5. **Optional GUI Tools**: While not strictly required, you might find it helpful to use GUI tools like Open OnDemand for a more user-friendly interface.

6. **Knowledge of Job Scheduling**: Understanding of the LSF job scheduler is beneficial for managing and submitting jobs.

7. **Docker Knowledge (Optional)**: If you plan to use custom Docker images or the noVNC image for personalized software, familiarity with Docker is helpful.

8. **Module Management**: Knowledge of module management (e.g., `module load`, `module unload`) is useful for managing software environments in the THPC environment.

These skills and tools will help you effectively use the Compute1 platform for scientific computing tasks.

**Sources:** Compute1, Compute1+Quickstart","3
",3,RIS Services Policies.md,RIS User Documentation/RIS User Documentation/RIS Services Policies.md
14,When should I use the general queue versus the general-interactive queue?,"The **general queue** and **general-interactive queue** serve different purposes in the Compute1 platform:

- **General Queue**: This is used for **non-interactive (batch) jobs**. These jobs are submitted to run in the background without requiring immediate user interaction. They are ideal for tasks that can run autonomously, such as long-running computations or scripts.

- **General-Interactive Queue**: This is used for **interactive jobs**, where you need direct access to a shell on a compute node. This is useful for tasks like development, debugging, or running commands that require real-time input or output.

### When to Use Each:
- Use the **general queue** for automated, background tasks.
- Use the **general-interactive queue** when you need an interactive shell for development, testing, or real-time operations.

For example:
- **Batch job** (general queue):
  ```bash
  bsub -q general -G ${compute-group} -a 'docker(alpine)' /bin/sleep 60
  ```
- **Interactive job** (general-interactive queue):
  ```bash
  bsub -Is -q general-interactive -G ${compute-group} -a 'docker(alpine)' /bin/bash
  ```

**Sources:** Compute1+Condo+and+Subscription+Queues, Job+Execution+Examples","4
",4,RIS Services Policies.md,RIS User Documentation/RIS User Documentation/RIS Services Policies.md
15,"What is the difference between my $HOME directory, scratch space, and the storage platform, and which should I use for my important data?","The difference between your `$HOME` directory, scratch space, and the storage platform is as follows:

1. **`$HOME` Directory**:
   - **Size**: Limited to 9 GiB.
   - **Purpose**: Intended for small, essential files and configurations.
   - **Access**: Accessible from the Compute Platform but not from outside.
   - **Usage**: Not suitable for large datasets or important data due to its small size.

2. **Scratch Space** (`/scratch1/fs1/${COMPUTE_ALLOCATION}`):
   - **Size**: Typically allocated per lab, with a default quota of 5% of your storage allocation (minimum 1Tb).
   - **Purpose**: High-performance temporary space for active computations.
   - **Access**: Only accessible from within the Compute Platform.
   - **Usage**: Ideal for temporary files and active job data. Data here is automatically deleted after 28 days of inactivity.
   - **Important Note**: Not suitable for long-term storage of important data.

3. **Storage Platform** (`/storage1/fs1/${STORAGE_ALLOCATION}` or `/storage2/fs1/${STORAGE_ALLOCATION}`):
   - **Size**: Larger capacity, designed for long-term storage.
   - **Purpose**: Permanent storage of important data.
   - **Access**: Accessible via SMB from outside the Compute Platform and from within the Compute Platform.
   - **Usage**: Best for storing important, long-term data that needs to be preserved and accessed regularly.

### Recommendation:
- Use your **`$HOME` directory** for small, essential files and configurations.
- Use **scratch space** for temporary files and active job data that do not need long-term retention.
- Use the **storage platform** for important data that needs to be stored permanently and accessed regularly. This is the most reliable option for critical datasets.

**Sources:** Access+Storage+Volumes, Space+Management","4
",4,RIS Services Policies.md,RIS User Documentation/RIS User Documentation/RIS Services Policies.md
16,What is the difference between the Active and Archive storage tiers?,"The Active and Archive storage tiers differ primarily in their intended use and performance characteristics:

- **Active Tier**: This is the default storage tier for daily work. It is optimized for frequent access and includes features like fast memory caching. Think of it as ""spinning disks"" for regular, ongoing projects.

- **Archive Tier**: This tier is designed for long-term storage of data that is not accessed regularly. It includes a caching layer to simplify user experience but is more cost-effective for infrequently used data. Think of it as a ""tape archive"" for data storage that you don't need to access often.

Data can be moved between these tiers as needed, with the preferred method being to use tar or zip and then rsync for the transfer.

**Sources:** Storage+Platforms, Storage2+Access+Control","1
",1,Requesting RIS Services.md,RIS User Documentation/RIS User Documentation/Requesting RIS Services.md
17,How do I decide between 'Consumption' and 'Subscription' billing for data storage?,"To decide between 'Consumption' and 'Subscription' billing for data storage, consider the following:

- **Consumption**: You pay only for the storage that is actually used, not for what is allocated. This option is suitable if your storage needs are variable or if you want to pay only for what you use. Note that you only pay for storage consumption above the free 5 TB.

- **Subscription**: This option offers substantial savings if you commit to storing data in 100 TB increments. It is ideal if you have predictable and consistent storage needs, as it provides a fixed cost for a set amount of storage.

For more details, you can refer to the [RIS Data Storage Platform website](https://ris.wustl.edu/services/research-storage).

**Sources:** Requesting+RIS+Services, Storage2+Access+Control","5
",5,Requesting RIS Services.md,RIS User Documentation/RIS User Documentation/Requesting RIS Services.md
18,"Which compute model (General, Subscription, or Consumption) is right for my project?","The choice of compute model depends on your project's needs:

1. **General**: Best for projects with flexible timing and no guaranteed resource requirements. Jobs run on a ""best effort"" basis as resources become available.

2. **Subscription**: Ideal if you need guaranteed resources (vCPUs and GPUs) and priority access. You pre-pay monthly for specific tiers (Tier 1, 2, or 3) with defined resource allocations.

3. **Consumption**: Suitable for on-demand access where you pay only for the resources you use, without pre-payment or guaranteed access.

For more details, refer to the [RIS Compute Resources page](https://ris.wustl.edu/services/compute/resources/).

**Sources:** Compute1+Condo+and+Subscription+Queues, Requesting+RIS+Services","5
",5,Requesting RIS Services.md,RIS User Documentation/RIS User Documentation/Requesting RIS Services.md
19,Where can I find a schedule for upcoming workshops?,"Based on the provided context, the page mentions that they are currently updating their user-facing offerings, including workshops, and more will be coming in the future. However, it does not provide a specific schedule for upcoming workshops.

For the most up-to-date information, you may want to check the [RIS User Documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/2053963910/Compute+Workshops) or contact the organizers directly.

**Sources:** Compute+Workshops, RIS+Compute+Seminars+from+April+2020","4
",4,Compute Workshops.md,RIS User Documentation/RIS User Documentation/Compute Workshops.md
20,How can I be notified when new workshops are announced?,"The provided context does not include information on how to be notified when new workshops are announced. However, you can check the [Compute Workshops page](https://washu.atlassian.net/wiki/spaces/RUD/pages/2053963910/Compute+Workshops) regularly for updates or consider reaching out to the RIS team directly for notification options.

**Sources:** Compute+Workshops, RIS+Compute+Seminars+from+April+2020","4
",4,Compute Workshops.md,RIS User Documentation/RIS User Documentation/Compute Workshops.md
21,Are recordings or slides available for the previous workshops?,"Yes, recordings of the previous workshops are available. You can watch the seminars from April 2020 on [YouTube](https://www.youtube.com/playlist?list=PLc5dxOEco26RhSbhBaRLeZoUFOTn7GM-Y). The specific topics covered include an introduction to RIS, working with RIS, storage allocation, traditional HPC users, introductory computing, Docker, and Python in Docker. Unfortunately, the context does not mention any slides being available.

**Sources:** Compute+Workshops, RIS+Compute+Seminars+from+April+2020","3
",3,Compute Workshops.md,RIS User Documentation/RIS User Documentation/Compute Workshops.md
22,Where do I start if I'm new to Docker?,"If you're new to Docker, the best place to start is with the **Docker Tutorial** provided in the RIS User Documentation. This tutorial covers the basics of Docker, including:

1. **What Docker is** – An introduction to Docker and its purpose.
2. **Creating a Docker Container** – Step-by-step instructions on building your first Docker container.
3. **Using Docker on the Compute1 Platform** – How to leverage Docker for analysis on the RIS Compute Platform.
4. **Advanced Topics** – Additional features like expanding images and using Docker for development.

You can access the full tutorial [here](https://washu.atlassian.net/wiki/spaces/RUD/pages/1865285780/Docker+Tutorial). Additionally, you may want to explore the **Docker and the RIS Compute1 Platform** documentation for more specific use cases.

**Sources:** Compute1, Docker+Tutorial","4
",4,Docker.md,RIS User Documentation/RIS User Documentation/Docker.md
23,"How do I install packages for a language other than Python, like R or Java?","To install packages for languages other than Python, such as R or Java, you can use the appropriate package managers for those languages. Here are some general guidelines:

1. **For R**:
   - Use the `install.packages()` function within R to install packages from CRAN (Comprehensive R Archive Network). For example:
     ```R
     install.packages(""package_name"")
     ```
   - Alternatively, you can use system package managers like `apt-get` (on Debian/Ubuntu) or `yum` (on CentOS/RHEL) to install R itself and then use R's package manager to install additional packages.

2. **For Java**:
   - Use build tools like Maven or Gradle to manage Java dependencies. For example, in Maven, you can specify dependencies in the `pom.xml` file:
     ```xml
     <dependencies>
         <dependency>
             <groupId>group_id</groupId>
             <artifactId>artifact_id</artifactId>
             <version>version</version>
         </dependency>
     </dependencies>
     ```
   - Alternatively, you can use system package managers like `apt-get` to install Java packages.

3. **Using System Package Managers**:
   - For Debian/Ubuntu, you can use `apt-get`:
     ```bash
     sudo apt-get install package_name
     ```
   - For CentOS/RHEL, you can use `yum`:
     ```bash
     sudo yum install package_name
     ```

4. **Using Conda**:
   - If you are using Conda, you can install packages for multiple languages, including R and Java, by specifying the appropriate channels. For example:
     ```bash
     conda install -c conda-forge r-essentials
     conda install -c conda-forge openjdk
     ```

For specific instructions on installing packages for R or Java in a Docker container, you can refer to the documentation for those languages or use the appropriate package manager within your Dockerfile.

**Sources:** Installing+Python+Packages+In+Your+Docker+Container, THPC+Installed+Applications","1
",1,Docker.md,RIS User Documentation/RIS User Documentation/Docker.md
24,"I've built my Docker image, where can I run it?","You can run your Docker image on the RIS Compute Platform. Here’s how to do it:

1. **Connect to the Compute Platform**:
   Use the command:
   ```
   ssh washukey@compute1-client-1.ris.wustl.edu
   ```

2. **Submit a Job**:
   Use the `bsub` command to run your Docker container. For example:
   ```
   export LSF_DOCKER_PRESERVE_ENVIRONMENT=false
   bsub -Is -q workshop-interactive -G compute-workshop -a 'docker(username/your-image:tag)' /bin/bash -c ""your-command""
   ```
   Replace `username/your-image:tag` with your Docker Hub username and image details, and `your-command` with the command you want to run inside the container.

For more details, refer to the [Docker Tutorial](https://washu.atlassian.net/wiki/spaces/RUD/pages/1865285780/Docker+Tutorial) and [Docker and the RIS Compute1 Platform](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705115761/Docker+and+the+RIS+Compute1+Platform) documentation.

**Sources:** Docker+Tutorial, Docker+and+the+RIS+Compute1+Platform","5
",5,Docker.md,RIS User Documentation/RIS User Documentation/Docker.md
25,I need to install custom software. Should I use Docker or create a Conda environment?,"The choice between using Docker or creating a Conda environment depends on your specific needs:

- **Use Docker** if you need to:
  - Package your application with all its dependencies into a single, portable container.
  - Ensure consistency across different environments.
  - Install non-Python dependencies easily.
  - Share your environment with others in a reproducible way.

- **Use Conda** if you need to:
  - Manage Python environments and dependencies specifically.
  - Install Python packages and their dependencies without needing a full container.
  - Use pre-built environments for data science and scientific computing.

For more details on creating a Conda environment, refer to the [Create Custom Conda Environment](https://washu.atlassian.net/wiki/spaces/RUD/pages/1793949820/Create+Custom+Conda+Environment) guide. For Docker, see the [Installing Python Packages In Your Docker Container](https://washu.atlassian.net/wiki/spaces/RUD/pages/1866072140/Installing+Python+Packages+In+Your+Docker+Container) guide.

**Sources:** Create+Custom+Conda+Environment, Installing+Python+Packages+In+Your+Docker+Container","4
",4,Compute1.md,RIS User Documentation/RIS User Documentation/Compute1.md
26,Where can I find a basic example of how to submit a job?,"To submit a basic job, you can use the `bsub` command. Here is a simple example:

```bash
bsub -q general-interactive -Is -a 'docker(ubuntu)' /bin/bash
```

This command:
- Submits an interactive job (`-Is`) to the `general-interactive` queue (`-q general-interactive`).
- Uses the `ubuntu` Docker image (`-a 'docker(ubuntu)'`).
- Opens a `/bin/bash` shell.

For more details, refer to the [Job Execution Examples](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705182249/Job+Execution+Examples) documentation.

**Sources:** Job+Execution+Examples","4
",4,Compute1.md,RIS User Documentation/RIS User Documentation/Compute1.md
27,"How can I see what types of hardware (e.g., GPUs, high-memory nodes) are available?","To see what types of hardware, such as GPUs or high-memory nodes, are available on the Compute2 platform, you can use the `sinfo` command. This command provides information about the available partitions/queues, including their state and the nodes associated with them.

Here is an example of how to use the `sinfo` command:

```bash
sinfo
```

This will display output similar to the following, showing the available partitions and their details:

```
PARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST
bigmem           up   infinite     82   idle c2-bigmem-[001-002],c2-node-[001-080]
general*         up   infinite     80   idle c2-node-[001-080]
general-short    up      30:00     80   idle c2-node-[001-080]
gpu              up   infinite      2  down* c2-gpu-[010,012]
gpu              up   infinite     94   idle c2-gpu-[001-009,011,013-016],c2-node-[001-080]
```

In this example:
- The `bigmem` partition indicates high-memory nodes.
- The `gpu` partition indicates nodes with GPUs available.

You can also use the `sinfo` command with specific options to filter the information, such as:

```bash
sinfo -p gpu
```

This will show only the information about the GPU partitions.

**Sources:** Compute2+Quickstart, Parabricks","1
",1,Compute1.md,RIS User Documentation/RIS User Documentation/Compute1.md
28,My storage quota seems wrong when I check it from the compute cluster. How do I get an accurate report of my remaining space?,"To get an accurate report of your remaining storage space, you should use the Storage Platform SMB Interface. Here’s how you can do it:

1. **Mount the Allocation to Your Desktop**: Right-click the mounted folder and select the appropriate menu option to view storage details.
2. **Use `smbclient`**: Run the following command to get detailed storage usage:
   ```bash
   smbclient -A .smb_creds -k //storageN.ris.wustl.edu/ris -c du
   ```
   This will provide the total space limit and the available space. You can convert these values to TiB for better understanding.

For example, the output might look like this:
```
             137438953472 blocks of size 1024. 135136394752 blocks available
Total number of bytes: 14619
```
To convert these values to TiB, you can use `bc`:
```bash
bc -q
scale=2
137438953472/1024^3  # This gives the total space limit in TiB
135136394752/1024^3  # This gives the available space in TiB
(137438953472-135136394752)/1024^3  # This gives the used space in TiB
```

Using the SMB Interface ensures you get the most accurate and up-to-date information about your storage allocation.

**Sources:** FAQ, Space+Management","2
",2,Storage Platforms.md,RIS User Documentation/RIS User Documentation/Storage Platforms.md
29,"How can I set up a folder for my project that specific people can write to, while others only have read-only access?","To set up a folder for your project with specific write and read-only access, follow these steps based on the provided context:

1. **Create Project-Specific Groups**:
   - Create a group for read-write access, e.g., `storage-key-project-rw`.
   - Create a group for read-only access, e.g., `storage-key-project-ro`.

2. **Assign Group Memberships**:
   - Add users who need write access to the `storage-key-project-rw` group.
   - Add users who need read-only access to the `storage-key-project-ro` group.

3. **Set Up Directory Permissions**:
   - Use NFSv4 ACLs to control access. For example:
     ```bash
     setfacl -m g:storage-key-project-rw:rwx /path/to/project
     setfacl -m g:storage-key-project-ro:r-x /path/to/project
     ```
   - Ensure the primary `storage-key` group is the most trusted (smallest) group.

4. **Manage Inheritance**:
   - Configure ACLs to inherit permissions for new files and directories within the project folder.

5. **Verify Permissions**:
   - Use `getfacl /path/to/project` to verify the ACL settings.

This setup ensures that only specified users can write to the folder, while others have read-only access.

**Sources:** Storage1+Access+Control, Storage2+Access+Control","2
",2,Storage Platforms.md,RIS User Documentation/RIS User Documentation/Storage Platforms.md
30,What is the recommended way to move my data from the Active tier to the Archive tier for long-term storage?,"The recommended way to move data from the Active tier to the Archive tier for long-term storage is to use tar or zip the data and then use rsync for the movement. This method ensures that the data is properly packaged and transferred efficiently. Additionally, once the data is moved to the Archive tier, it can be migrated back to the Active tier if needed, and it will be restored from tape to disk.

**Sources:** Moving+Data+With+Globus, Storage+Platforms","5
",5,Storage Platforms.md,RIS User Documentation/RIS User Documentation/Storage Platforms.md
31,How do I submit a batch job instead of an interactive one?,"To submit a batch job instead of an interactive one, you need to use the `sbatch` command with a script file. Here’s how you can do it:

1. **Create a Script File**: Write your job commands in a script file (e.g., `myscript.sh`). Include `#SBATCH` directives at the top to specify job options.

   Example of a basic `sbatch` script:
   ```bash
   #!/bin/bash
   #SBATCH -p general  # Request the general partition/queue

   <your_commands_here>
   ```

2. **Submit the Script**: Use the `sbatch` command to submit your script.
   ```bash
   sbatch myscript.sh
   ```

### Key Differences:
- **Interactive Jobs**: Use `bsub -Is` and provide commands directly or interactively.
- **Batch Jobs**: Use `sbatch` with a script file containing your commands and `#SBATCH` directives.

For more details on `sbatch` options, refer to the [Batch Jobs (sbatch) documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1737785530/Batch+Jobs+(sbatch)).

**Sources:** Batch+Jobs+(sbatch), Job+Execution+Examples","4
",4,Compute2.md,RIS User Documentation/RIS User Documentation/Compute2.md
32,What are the available partitions or queues and what are their resource limits?,"Based on the provided context, here are the available partitions or queues and their resource limits for Compute1 and Compute2:

### Compute1 Queues:
1. **General Queue**:
   - No guaranteed resources.
   - Used as the base subscription for all active compute users.

2. **Condo Queue**:
   - Resources depend on the physical resources purchased as part of the condo.
   - No specific resource limits mentioned in the context.

3. **Subscription Tier Queues**:
   - **Tier 1**: 25 vCPUs, 1 GPU.
   - **Tier 2**: 50 vCPUs, 2 GPUs.
   - **Tier 3**: 100 vCPUs, 3 GPUs.
   - Jobs exceeding guaranteed vCPUs or GPUs may not run or stay pending.

### Compute2 Partitions/Queues:
1. **General**:
   - No specific time limit mentioned (infinite).
   - Various node states (idle, alloc, drain, etc.) are listed.

2. **General-preemptable**:
   - No specific resource limits mentioned.

3. **General-short**:
   - Time limit: 30 minutes.
   - Various node states (idle, alloc, drain, etc.) are listed.

4. **Subscription**:
   - No specific resource limits mentioned.

For more detailed information on Compute2 partitions, you can use the `sinfo` command as shown in the context.

**Sources:** Compute1+Condo+and+Subscription+Queues, Interactive+Jobs+(srun), Monitoring+Jobs+and+Partitions_Queues","1
",1,Compute2.md,RIS User Documentation/RIS User Documentation/Compute2.md
33,How can I install my own software or request a new application to be installed?,"To install your own software or request a new application to be installed, you can follow these steps:

1. **For Compute1**: If you need an application that is not listed in the [THPC Installed Applications](https://washu.atlassian.net/wiki/spaces/RUD/pages/1696039041/THPC+Installed+Applications) documentation, you can reach out to the [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/55) to request it.

2. **For Compute2**: If you need an application that is not listed in the [Compute2 Applications](https://washu.atlassian.net/wiki/spaces/RUD/pages/2087747658/Compute2+Applications) documentation, you can also contact the RIS Service Desk or explore using `ml` (module system) to manage your software environment. The documentation on `ml` usage can be found [here](https://washu.atlassian.net/wiki/spaces/RUD/pages/2087747658/Compute2+Applications).

For specific instructions or further assistance, it's best to contact the RIS Service Desk directly.

**Sources:** Compute2+Applications, THPC+Installed+Applications","3
",3,Compute2.md,RIS User Documentation/RIS User Documentation/Compute2.md
34,How do I request access to one of the supported applications?,"To request access to one of the supported applications, you should reach out to the RIS Service Desk. You can find the request form [here](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/55).

**Sources:** Compute1+Applications, THPC+Installed+Applications","1
",1,Research Applications and Services.md,RIS User Documentation/RIS User Documentation/Research Applications and Services.md
35,How can I access these services if I am working from off-campus?,"To access the Research Applications and Services (RIS) from off-campus, you need to be connected to either the WashU Medical School VPN or the Danforth VPN. These VPNs will allow you to access the services as if you were on the local WashU network. Here are the steps you can follow:

1. **Obtain a WashU Key Identity**: Ensure you have a WashU Key Identity, which is required to access these services.
2. **Connect to the VPN**:
   - For the Medical School VPN, follow the instructions provided by WashU IT.
   - For the Danforth VPN, follow the instructions provided by WashU IT.
3. **Access the Services**: Once connected to the VPN, you can access the supported applications such as GitHub Enterprise, LabArchives Notebook, and Omero as described in the RIS User Documentation.

For more detailed instructions on setting up the VPN, you can refer to the WashU IT documentation or contact the RIS Service Desk for assistance.

**Sources:** FAQ, Research+Applications+and+Services","5
",5,Research Applications and Services.md,RIS User Documentation/RIS User Documentation/Research Applications and Services.md
36,Who should I contact if I need support for one of these applications?,"Based on the provided context, the applications listed are supported by RIS (Research Infrastructure Services) for the Compute1 and Compute2 platforms. If you need support for one of these applications, you should contact RIS. The context does not provide specific contact details, so you may need to look up RIS's contact information through other means, such as their official website or support portal.

**Sources:** Compute1+Applications, Compute2+Applications","4
",4,Research Applications and Services.md,RIS User Documentation/RIS User Documentation/Research Applications and Services.md
37,Are more recent versions of these seminars or any upcoming live workshops available?,"Based on the provided context, the page mentions that they are currently updating their user-facing offerings, including workshops, and that more will be available in the future. However, there is no specific information about recent versions of the seminars or upcoming live workshops.

For the most up-to-date information, I recommend checking the [Compute Workshops page](https://washu.atlassian.net/wiki/spaces/RUD/pages/2053963910/Compute+Workshops) or contacting the organizers directly.

**Sources:** Compute+Workshops, RIS+Compute+Seminars+from+April+2020","2
",2,RIS Compute Seminars from April 2020.md,RIS User Documentation/RIS User Documentation/Compute Workshops/RIS Compute Seminars from April 2020.md
38,Are the presentation slides or code examples from these seminars available for download?,"Based on the provided context, there is no mention of presentation slides or code examples being available for download from the seminars. The context only references YouTube links for watching the seminars. If you need this information, I recommend checking the YouTube playlist or contacting the organizers directly for any additional resources.

**Sources:** Compute+Workshops, RIS+Compute+Seminars+from+April+2020","5
",5,RIS Compute Seminars from April 2020.md,RIS User Documentation/RIS User Documentation/Compute Workshops/RIS Compute Seminars from April 2020.md
39,Is there a recommended order to watch these seminars for a beginner?,"Based on the context provided, the seminars from the ""RIS Compute Seminars from April 2020"" are listed in a logical order that could be suitable for beginners. Here is the recommended order to watch them:

1. **Introduction to RIS**
2. **Working With RIS And Getting Started**
3. **Working With Your Storage Allocation**
4. **Intro to RIS Computing For Traditional HPC Users**
5. **Intro To Computing 101**
6. **Intro To Computing 102**
7. **Intro To Computing 103 and 104**
8. **Introduction to Docker**
9. **Intro To Python In Docker**

This order starts with the basics and gradually moves to more advanced topics, making it a good progression for beginners.

**Sources:** Compute+Workshops, RIS+Compute+Seminars+from+April+2020","5
",5,RIS Compute Seminars from April 2020.md,RIS User Documentation/RIS User Documentation/Compute Workshops/RIS Compute Seminars from April 2020.md
40,"Do I have to upload my image to a public registry like Docker Hub, or can I use a locally built image directly on the Compute1 platform?","Based on the provided context, you do not necessarily have to upload your image to a public registry like Docker Hub. The RIS Compute1 Platform allows you to build Docker images directly on the compute cluster using the `docker_build` LSF application. This means you can create and use locally built images without needing to push them to a public registry.

Here’s a summary of the relevant points:
1. **Building Images on Compute1**: You can use the `docker_build` LSF application to build Docker images directly on the Compute1 platform. This is useful for situations where you need more resources than your local machine can provide or for easier debugging.
2. **Using Private Registries**: If you prefer not to use a public registry, you can log into a private registry (like Docker Hub’s private registries) and build/push images from there. The context provides instructions on how to log into and use private registries.
3. **Direct Usage**: Once an image is built (either locally or on the Compute1 platform), it can be used directly on the Compute1 platform without needing to be uploaded to a public registry.

So, you have the flexibility to either build and use images locally on the Compute1 platform or use private registries, depending on your needs.

**Sources:** Docker+Tutorial, Docker+and+the+RIS+Compute1+Platform","5
",5,Docker Tutorial.md,RIS User Documentation/RIS User Documentation/Docker/Docker Tutorial.md
41,The example installs software using `apt-get`. How can I add my own scripts or source code from my local machine into the Docker image?,"To add your own scripts or source code from your local machine into the Docker image, you can use the `COPY` or `ADD` command in your Dockerfile. Here’s how you can do it:

1. **Using `COPY` Command**:
   - Place your script or source code file in the same directory as your Dockerfile.
   - Use the `COPY` command in your Dockerfile to copy the file into the Docker image. For example:
     ```dockerfile
     COPY hello.py /path/in/container/
     ```
   - This will copy `hello.py` from your local machine to the specified path inside the container.

2. **Using `ADD` Command**:
   - Similar to `COPY`, but with additional features like automatic extraction of tar files.
   - Example:
     ```dockerfile
     ADD hello.py /path/in/container/
     ```

3. **Using `git clone`**:
   - If your code is hosted in a Git repository, you can clone it directly into the Docker image using the `git clone` command in your Dockerfile. For example:
     ```dockerfile
     RUN git clone https://github.com/yourusername/yourrepository.git
     ```

For more details, refer to the [Docker Basics: Building, Tagging, & Pushing A Custom Docker Image](https://washu.atlassian.net/wiki/spaces/RUD/pages/1864892726/Docker+Basics_+Building,+Tagging,+&+Pushing+A+Custom+Docker+Image) documentation.

**Sources:** Docker+Basics_+Building,+Tagging,+&+Pushing+A+Custom+Docker+Image, Docker+Tutorial","5
",5,Docker Tutorial.md,RIS User Documentation/RIS User Documentation/Docker/Docker Tutorial.md
42,"Why is it necessary to set `LSF_DOCKER_PRESERVE_ENVIRONMENT=false` before running my container, and when might I want to leave it as the default?","Setting `LSF_DOCKER_PRESERVE_ENVIRONMENT=false` instructs the job launcher to provide the container with a minimal environment, which includes only a few variables such as `HOSTNAME` (set to the execution host's hostname) and those defined in the container's Dockerfile with `ENV`. This can be useful when you want to ensure that the container starts with a clean environment, free from any potentially conflicting or unnecessary variables from your submission environment.

On the other hand, the default value (`true`) preserves most environment variables from the submission shell, except for a few like `HOSTNAME`, `LSB_INTERACTIVE`, and variables containing newlines. This is generally useful when you want the container to inherit the environment of the shell from which the job was submitted, ensuring consistency and access to necessary variables.

You might want to leave it as the default (`true`) when:
- You need the container to have access to environment variables set in your submission shell.
- You rely on specific environment variables for your application to function correctly.
- You want to avoid manually redefining necessary environment variables inside the container.

You might want to set it to `false` when:
- You want to ensure a clean environment for the container, free from any potential conflicts or unwanted variables.
- Your application requires a minimal set of environment variables and you want to avoid any interference from the submission environment.
- You are debugging or testing and want to isolate the container's environment.

**Sources:** Docker+Wrapper+Environment+Variables, Job+Execution+Examples","4
",4,Docker Tutorial.md,RIS User Documentation/RIS User Documentation/Docker/Docker Tutorial.md
43,When should I choose pip over conda for installing packages in my Docker container?,"You should choose `pip` over `conda` for installing packages in your Docker container when:

1. **Faster Build Times and Smaller Container Sizes**: Using `pip` along with your Linux package manager (e.g., `apt-get`) can result in quicker build times and smaller final container sizes compared to `conda`.

2. **Python-Specific Packages**: If you only need to install Python packages and do not require non-Python dependencies, `pip` is sufficient. However, you may still need to use your package manager to install non-Python dependencies.

3. **Package Availability**: If installation instructions are only available for `pip` and not for `conda`, you will have to use `pip`.

4. **Avoiding Larger Containers**: `conda` includes non-Python dependencies, which can significantly increase the size of your final container. If minimizing container size is a priority, `pip` is a better choice.

For more details, refer to the [Installing Python Packages In Your Docker Container](https://washu.atlassian.net/wiki/spaces/RUD/pages/1866072140/Installing+Python+Packages+In+Your+Docker+Container) documentation.

**Sources:** Create+Custom+Conda+Environment, Installing+Python+Packages+In+Your+Docker+Container","5
",5,Installing Python Packages In Your Docker Container.md,RIS User Documentation/RIS User Documentation/Docker/Installing Python Packages In Your Docker Container.md
44,How do I install non-Python dependencies if I'm using the pip method?,"To install non-Python dependencies when using the `pip` method, you will need to use your Linux package manager in addition to `pip`. For example, if you are using a Debian-based system like Ubuntu, you can use `apt-get` to install non-Python dependencies. Here’s how you can do it:

1. **Use `apt-get` (or your system's package manager) to install non-Python dependencies**:
   ```dockerfile
   RUN apt-get install -y dependency
   ```
   Replace `dependency` with the name of the package you need to install.

2. **Use `pip` to install Python packages**:
   ```dockerfile
   RUN pip install --no-cache-dir -U package_name
   ```
   Replace `package_name` with the Python package you want to install.

For example, if you need to install `libxml2` (a non-Python dependency) and `spacy` (a Python package), your Dockerfile might look like this:
```dockerfile
FROM python:3.7.7-slim-stretch
RUN apt-get update && apt-get install -y libxml2
RUN pip install --no-cache-dir -U spacy
```

This approach ensures that all dependencies, both Python and non-Python, are installed in your Docker container.

**Sources:** Create+Custom+Conda+Environment, Installing+Python+Packages+In+Your+Docker+Container","5
",5,Installing Python Packages In Your Docker Container.md,RIS User Documentation/RIS User Documentation/Docker/Installing Python Packages In Your Docker Container.md
45,Why is the final container image so much larger when using conda compared to pip?,"The final container image is significantly larger when using conda compared to pip because conda includes non-Python dependencies by default, which adds to the overall size. Conda environments tend to be larger due to the inclusion of additional libraries and tools that are not strictly necessary for Python package management. In contrast, pip only installs Python packages, resulting in smaller and faster build times. This is why the documentation suggests using pip and a Linux package manager for quicker builds and smaller container sizes.

**Sources:** Create+Custom+Conda+Environment, Installing+Python+Packages+In+Your+Docker+Container","4
",4,Installing Python Packages In Your Docker Container.md,RIS User Documentation/RIS User Documentation/Docker/Installing Python Packages In Your Docker Container.md
46,How can I modify this Dockerfile to install different software or a specific version of Python?,"To modify the Dockerfile to install different software or a specific version of Python, you can follow these steps:

1. **Change the Base Image**: If you want a specific version of Python, you can change the base image in the `FROM` line. For example, to use Python 3.8 instead of 3.7, you would change:
   ```dockerfile
   FROM python:3.7.7-slim-stretch
   ```
   to:
   ```dockerfile
   FROM python:3.8-slim-stretch
   ```

2. **Install Different Software**: To install different software, you can use the `RUN` command with the appropriate package manager. For example, to install `numpy` and `pandas` instead of `spacy`, you would add:
   ```dockerfile
   RUN pip install --no-cache-dir -U numpy pandas
   ```

3. **Combine Commands**: You can combine multiple `RUN` commands into a single line to optimize the Dockerfile. For example:
   ```dockerfile
   RUN apt-get update \
       && apt-get install -y --no-install-recommends some-software \
       && pip install --no-cache-dir -U some-python-package \
       && apt-get clean
   ```

Here is an example Dockerfile that installs Python 3.8 and the `numpy` and `pandas` packages:
```dockerfile
FROM python:3.8-slim-stretch
RUN pip install --no-cache-dir -U numpy pandas
COPY script.py /
```

You can then build and run this Docker image as described in the context.

**Sources:** Docker+Tutorial, Installing+Python+Packages+In+Your+Docker+Container","3
",3,"Docker Basics_ Building, Tagging, & Pushing A Custom Docker Image.md","RIS User Documentation/RIS User Documentation/Docker/Docker Basics_ Building, Tagging, & Pushing A Custom Docker Image.md"
47,"Why am I getting an ""access denied"" error when I try to `docker push` my image to Docker Hub?","The ""access denied"" error when trying to `docker push` your image to Docker Hub typically occurs due to authentication issues. Here are the most common reasons and solutions:

1. **Not Logged In**: Ensure you are logged in to Docker Hub. Run `docker login` in your terminal and enter your Docker Hub credentials.

2. **Incorrect Tagging**: Make sure your image is tagged correctly with your Docker Hub username. The format should be `your_dockerhub_username/image_name:tag`.

3. **Permission Issues**: Verify that you have the necessary permissions to push to the repository. If the repository is private, ensure you have write access.

4. **Network Issues**: Check your internet connection and ensure there are no firewall or proxy settings blocking access to Docker Hub.

If you've followed the steps in the provided context and still encounter issues, double-check your Docker Hub account settings and ensure you have the correct permissions.

**Sources:** Docker, Docker+Basics_+Building,+Tagging,+&+Pushing+A+Custom+Docker+Image","4
",4,"Docker Basics_ Building, Tagging, & Pushing A Custom Docker Image.md","RIS User Documentation/RIS User Documentation/Docker/Docker Basics_ Building, Tagging, & Pushing A Custom Docker Image.md"
48,What is the advantage of chaining all the `apt-get` commands into a single `RUN` line instead of separating them?,"The provided context does not contain any information about chaining `apt-get` commands into a single `RUN` line in Dockerfiles. Therefore, I cannot provide an answer based on the given context.

However, in general, chaining `apt-get` commands into a single `RUN` line in a Dockerfile is advantageous for several reasons:
1. **Layer Caching**: Docker caches each layer. By combining commands, you reduce the number of layers, which can help optimize the build process and reduce image size.
2. **Efficiency**: Fewer layers mean fewer steps during the build process, which can speed up the build time.
3. **Cleaner Image**: It helps keep the image cleaner by ensuring that temporary files and caches are removed in the same layer, reducing the final image size.

If you have specific context or a Dockerfile example, I can provide a more detailed explanation.

**Sources:** Non-RIS+Docker+Images","3
",3,"Docker Basics_ Building, Tagging, & Pushing A Custom Docker Image.md","RIS User Documentation/RIS User Documentation/Docker/Docker Basics_ Building, Tagging, & Pushing A Custom Docker Image.md"
49,Why are these non-RIS images provided if the document recommends using the more secure RIS-hosted images?,"The document provides non-RIS images to offer flexibility and accommodate specific user needs that may not be fully met by the RIS-hosted images. While RIS-hosted images are recommended for their security and maintenance, non-RIS images allow users to utilize other Docker images that might be necessary for their particular workflows or applications. This ensures that users have access to a broader range of tools and resources, even if they are not maintained by RIS.

**Sources:** Applications, RIS+Developed+Containers","4
",4,Non-RIS Docker Images.md,RIS User Documentation/RIS User Documentation/Applications/Non-RIS Docker Images.md
50,"The document lists a specific version for an application, but I need to use a different one. How do I modify the command to pull another version?","To modify the command to pull a different version of an application, you need to change the version number in the Docker image tag within the `bsub` command. Here's how you can do it:

1. **Identify the current Docker image tag** in the command. For example, in the command for `samtools`, the Docker image tag is `quay.io/biocontainers/samtools:1.2-0`.

2. **Replace the version number** with the desired version. For instance, if you want to use version `1.3-0` instead, the modified command would look like this:
   ```bash
   bsub -G ${group_name} -Is -q general-interactive -a 'docker(quay.io/biocontainers/samtools:1.3-0)' /bin/bash
   ```

3. **Ensure the new version is available** in the Docker registry. You can check the registry location provided in the documentation (e.g., [samtools registry](https://bioconda.github.io/recipes/samtools/README.html)) to confirm the availability of the desired version.

By following these steps, you can adjust the command to pull and use a different version of the application.

**Sources:** Compute2+Applications, Non-RIS+Docker+Images","5
",5,Non-RIS Docker Images.md,RIS User Documentation/RIS User Documentation/Applications/Non-RIS Docker Images.md
51,"I copied an example command, but it's not working. What do I need to replace `${group_name}` with?","To use the example command, you need to replace `${group_name}` with the name of your specific compute group. This is typically provided to you by your system administrator or can be found in your account settings on the RIS Compute1 platform.

For example, if your group name is `compute-mylab`, you would replace `${group_name}` with `compute-mylab` in the command.

**Sources:** Docker+and+the+RIS+Compute1+Platform, Job+Execution+Examples","5
",5,Non-RIS Docker Images.md,RIS User Documentation/RIS User Documentation/Applications/Non-RIS Docker Images.md
52,"How do I load one of the listed applications, like Python or R, into my environment?","To load applications like Python or R into your environment, you can follow these steps based on the provided context:

1. **For Python**:
   - Python is listed as one of the installed applications. To use it, you can load it via a module system if available on your platform. Typically, you would use a command like:
     ```bash
     module load Python
     ```
   - If you are using a Docker container, ensure that the Python environment is included in your container setup.

2. **For R**:
   - R is also listed as an installed application. To load R, you can use:
     ```bash
     module load R
     ```
   - For a more interactive experience, you can use RStudio, which is available through Docker containers. Follow the instructions in the [Rstudio documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1782382858/Rstudio) to set up and launch RStudio.

3. **General Instructions**:
   - If you are using Open OnDemand or THPC, refer to the specific documentation for launching interactive sessions with these applications.
   - For applications that require additional setup or dependencies, you may need to extend the Docker image as described in the documentation.

If you encounter any issues or need further assistance, you can reach out to the [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/55) for support.

**Sources:** Rstudio, THPC+Installed+Applications","4
",4,Compute2 Applications.md,RIS User Documentation/RIS User Documentation/Applications/Compute2 Applications.md
53,"How can I see all the available versions for a specific module, like gcc or Python?","To see all the available versions for a specific module like `gcc` or `Python` on the Compute2 platform, you can use the `ml` command with the `av` (available versions) option. Here’s how you can do it:

1. **For `gcc`:**
   ```bash
   ml av gcc
   ```

2. **For `Python`:**
   ```bash
   ml av python
   ```

This command will list all the available versions of the specified module that are installed and supported on the Compute2 platform. If you need further details on how to use the `ml` command, refer to the documentation linked in the context.

**Sources:** Compute2+Applications, THPC+Installed+Applications","5
",5,Compute2 Applications.md,RIS User Documentation/RIS User Documentation/Applications/Compute2 Applications.md
54,What is the process for requesting new software that is not included on this list?,"To request new software that is not included on the list, you should reach out to the RIS Service Desk. You can submit your request through the following link: [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/55).

**Sources:** Github+Enterprise, THPC+Installed+Applications","5
",5,Compute2 Applications.md,RIS User Documentation/RIS User Documentation/Applications/Compute2 Applications.md
55,What is the difference between a THPC installed application and an RIS developed container?,"Based on the provided context, here's the difference between a THPC installed application and an RIS developed container:

1. **THPC Installed Applications**:
   - These are applications available as part of the THPC (Traditional High Performance Computing) platform.
   - THPC uses the module system to load software, which is installed in a location accessible to the THPC image but not directly within the image itself.
   - This approach reduces image size and load times.
   - The software is loaded into the image and job via the module system during runtime.

2. **RIS Developed Containers**:
   - These are Docker containers developed and supported by RIS (Research Infrastructure Services) specifically for the Compute1 platform.
   - The THPC container is one example of these RIS-developed containers.
   - These containers provide a pre-configured environment with specific software and tools.
   - They can be used for various purposes like command-line sessions, batch jobs, or GUI sessions.

In summary, THPC installed applications refer to the software available through the THPC platform's module system, while RIS developed containers are pre-built Docker environments that include various tools and applications for different computing needs.

**Sources:** Compute1+Applications, THPC","3
",3,Compute1 Applications.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications.md
56,How do I use or load one of the applications listed here?,"To use or load one of the applications listed in the documentation, you typically need to use the `ml` command (module load command) on the Compute2 platform. Here’s a general guide on how to do this:

1. **Access the Compute2 Platform**: Ensure you have access to the Compute2 platform provided by RIS.

2. **Load the Application**: Use the `ml` command followed by the name of the application you want to load. For example:
   ```bash
   ml <application_name>
   ```
   Replace `<application_name>` with the specific application you want to use. For instance, to load `gcc`, you would use:
   ```bash
   ml gcc
   ```

3. **Verify the Load**: You can verify that the application is loaded by checking the environment variables or running a command specific to the application.

4. **Refer to Documentation**: For more detailed instructions or specific applications, refer to the documentation provided in the links or sections mentioned in the context.

If you need further assistance or if the application is not listed under the `ml` command, you may need to reach out to the [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/55) for additional support.

**Sources:** Compute2+Applications, THPC+Installed+Applications","1
",1,Compute1 Applications.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications.md
57,What if the software I need is not on either of these lists?,"If the software you need is not listed on either of the provided application lists for Compute1 or Compute2 platforms, you can request it through the RIS Service Desk. According to the context, you can reach out to the [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/55) to request the installation of the application you need. They will assist you with further steps or provide information on how to access the software.

**Sources:** Compute2+Applications, THPC+Installed+Applications","1
",1,Compute1 Applications.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications.md
58,How do I find my `allocation_name` for the scratch directory path?,"To find your `allocation_name` for the scratch directory path, you can use the following steps:

1. **Log in to the Compute Platform**: Access the Compute1 platform via SSH or any other method provided by your institution.

2. **Check Environment Variables**: Your `allocation_name` is typically stored in the environment variable `COMPUTE_ALLOCATION`. You can check this variable by running:
   ```bash
   echo $COMPUTE_ALLOCATION
   ```
   This will display your allocation name.

3. **Construct the Scratch Path**: Once you have your `allocation_name`, you can construct the scratch directory path using the format:
   ```bash
   /scratch1/fs1/${COMPUTE_ALLOCATION}
   ```
   For example, if your `COMPUTE_ALLOCATION` is `joe.user`, the scratch path would be:
   ```bash
   /scratch1/fs1/joe.user
   ```

If you do not have access to the environment variable or need further assistance, you may need to contact your system administrator or check the documentation provided by your institution for additional guidance.

**Sources:** Access+Storage+Volumes, FAQ","1
",1,Compute2 Nextflow.md,RIS User Documentation/RIS User Documentation/Applications/Compute2 Applications/Compute2 Nextflow.md
59,The guide says to run Nextflow inside a job. What would an example SLURM submission script look like?,"Based on the provided context, there isn't a specific example of a SLURM submission script for running Nextflow. However, I can provide a general template for a SLURM script that you can adapt for Nextflow jobs. Here's an example:

```bash
#!/bin/bash
#SBATCH --job-name=nextflow_job
#SBATCH --output=nextflow_%j.out
#SBATCH --error=nextflow_%j.err
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
#SBATCH --time=24:00:00
#SBATCH --partition=general

# Load necessary modules
module load nextflow

# Set working directory
cd /path/to/your/nextflow/workdir

# Run Nextflow
nextflow run nf-core/rnaseq \
  -profile test \
  -c /scratch1/fs1/ris/application/nextflow/conf/rnaseq.config \
  --outdir /path/to/outdir
```

You'll need to adjust the parameters (like CPU, memory, time, etc.) according to your specific requirements. Also, make sure to replace the paths with your actual paths. The context mentions using Docker with Nextflow, so you might need to include Docker-related commands if your setup requires it.

**Sources:** Compute2+General+Guidelines, Nextflow","3
",3,Compute2 Nextflow.md,RIS User Documentation/RIS User Documentation/Applications/Compute2 Applications/Compute2 Nextflow.md
60,Can I use a different pipeline besides the `nf-core/rnaseq` example?,"Yes, you can use a different pipeline besides the `nf-core/rnaseq` example. The context provides instructions for running Nextflow in both interactive and batch sessions, and you can replace the pipeline name (`nf-core/rnaseq`) with any other Nextflow pipeline of your choice. For example, you could use `nf-core/sarek` or any other pipeline available in the Nextflow ecosystem. Just ensure you adjust the command accordingly, including any specific options or configurations required by the pipeline you choose.

**Sources:** Nextflow, Non-RIS+Docker+Images","5
",5,Compute2 Nextflow.md,RIS User Documentation/RIS User Documentation/Applications/Compute2 Applications/Compute2 Nextflow.md
61,Why is my home2 directory filling up when I use Podman?,"Your home2 directory is filling up when you use Podman because Podman, by default, stores its configuration and runtime data in your home directory. To prevent this, you need to set specific environment variables to redirect Podman's data to a different location, such as `/scratch2/fs1/allocation_name/podman`.

Here are the steps to follow:

1. Load the relevant modules:
   ```bash
   module load ris podman slurm
   ```

2. Create directories to hold VM and container data:
   ```bash
   mkdir -p /scratch2/fs1/allocation_name/podman/runtime
   ```

3. Start an interactive SLURM job with the required environment variables:
   ```bash
   XDG_CONFIG_HOME=/scratch2/fs1/allocation_name/podman \
   XDG_DATA_HOME=/scratch2/fs1/allocation_name/podman \
   XDG_RUNTIME_DIR=/scratch2/fs1/allocation_name/podman/runtime \
   srun --pty /bin/bash
   ```

4. Initialize and start the Podman machine:
   ```bash
   podman machine init
   podman machine start
   ```

By setting these environment variables, you ensure that Podman's data is stored in the specified directories under `/scratch2` instead of your home2 directory, preventing it from filling up.

**Sources:** Compute2+Podman, FAQ","5
",5,Compute2 Podman.md,RIS User Documentation/RIS User Documentation/Applications/Compute2 Applications/Compute2 Podman.md
62,What happens to my container images and the Podman VM after my SLURM job ends?,"When your SLURM job ends, both your container images and the Podman VM will be terminated. The context explicitly states that ""Killing the SLURM job will stop the VM,"" which implies that all associated resources, including container images, are cleaned up. This is part of the design to manage resources efficiently on the Compute2 platform.

**Sources:** Compute2+Podman, Using+Slurm+in+Containers","4
",4,Compute2 Podman.md,RIS User Documentation/RIS User Documentation/Applications/Compute2 Applications/Compute2 Podman.md
63,Do I need to run `podman machine init` and `podman machine start` every time I begin a new SLURM session?,"Based on the provided context, you do need to run `podman machine init` and `podman machine start` at the beginning of each new SLURM session. This is because the Podman machine is tied to the SLURM job, and killing the SLURM job will stop the VM. Therefore, these commands should be executed each time you start a new interactive SLURM job to initialize and start the Podman machine for that session.

**Sources:** Compute2+Podman, Using+Slurm+in+Containers","5
",5,Compute2 Podman.md,RIS User Documentation/RIS User Documentation/Applications/Compute2 Applications/Compute2 Podman.md
64,How do I access and run one of the RIS-maintained Docker images?,"To access and run one of the RIS-maintained Docker images, you can follow these general steps:

1. **Identify the Docker Image**: Choose the specific Docker image you need from the list of [Hosted and Maintained Images](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705738300/RIS+Developed+Containers#Hosted-and-Maintained-Images). Each image has a dedicated page with detailed instructions.

2. **Pull the Image**: Use the `docker pull` command to download the image from the RIS repository. The exact command will be provided on the specific image's documentation page.

3. **Run the Image**: Use the `docker run` command to start a container from the pulled image. The specific parameters and commands will also be detailed in the image's documentation.

For example, if you want to run the MATLAB Docker image, you would:
- Visit the [MATLAB Docker page](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705738300/RIS+Developed+Containers/MATLAB.md) for specific instructions.
- Pull the image using a command like `docker pull ris/matlab`.
- Run the container with a command like `docker run -it ris/matlab`.

For detailed instructions, refer to the specific Docker image's documentation page linked in the [RIS Developed Containers](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705738300/RIS+Developed+Containers) overview.

**Sources:** Docker, RIS+Developed+Containers","4
",4,RIS Developed Containers.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers.md
65,"What is the difference between a ""Hosted and Maintained"" image and one that is ""Developed But Not Maintained""?","Based on the provided context, here's the difference between ""Hosted and Maintained"" images and ""Developed But Not Maintained"" images:

- **Hosted and Maintained Images**: These are Docker images that are hosted in an RIS repository and are actively maintained by RIS. This means RIS takes responsibility for updating, fixing, and ensuring these images remain functional and secure.

- **Developed But Not Maintained**: These are Docker files that were developed in collaboration with RIS, but the upkeep and maintenance of these images are the responsibility of the users. RIS does not actively update or support these images after their initial development.

In summary, the key difference lies in the maintenance responsibility: RIS handles maintenance for ""Hosted and Maintained"" images, while users are responsible for maintaining ""Developed But Not Maintained"" images.

**Sources:** Applications, RIS+Developed+Containers","5
",5,RIS Developed Containers.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers.md
66,My application isn't on this list. Can I request a new container to be developed?,"Based on the provided context, the documentation lists Docker images and containers that have been developed and are supported by RIS for the Compute1 platform. However, it does not explicitly mention a process for requesting new containers.

If your application isn't on the list and you'd like to request a new container to be developed, I recommend reaching out directly to RIS (Research Infrastructure Services) for guidance. They may have a process in place for evaluating and developing new Docker containers based on user requests.

You can likely find contact information or a support request form on the RIS website or through your institutional resources.

**Sources:** Compute1+Applications, RIS+Developed+Containers","5
",5,RIS Developed Containers.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers.md
67,What should I do if the software I need isn't on this list?,"If the software you need isn't listed in the provided applications, you should reach out to the RIS Service Desk to request it. You can find the link to the RIS Service Desk in the context provided. They can assist you with accessing or installing the required software.

**Sources:** Compute2+Applications, THPC+Installed+Applications","5
",5,THPC Installed Applications.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/THPC Installed Applications.md
68,What does the asterisk (*) next to some application names mean?,"The asterisk (*) next to some application names indicates that while these applications are installed by RIS (Research Infrastructure Services), they are community-owned and may require separate licensing. Instructions for accessing these applications can be found in the THPC documentation.

**Sources:** Compute2+Applications, THPC+Installed+Applications","5
",5,THPC Installed Applications.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/THPC Installed Applications.md
69,How do I run or load one of these listed applications?,"To run or load one of the listed applications, you can use the `ml` command (module loader) on the Compute2 platform. Here’s a general guide:

1. **Load the Application**: Use the `ml` command followed by the application name. For example:
   ```bash
   ml <application_name>
   ```
   Replace `<application_name>` with the name of the application you want to load (e.g., `gcc`, `cuda`, `python`).

2. **Run the Application**: After loading the application, you can run it directly from the command line. For example:
   ```bash
   ml gcc
   gcc --version
   ```

3. **Check Available Applications**: To see a list of available applications, you can use:
   ```bash
   ml avail
   ```

For more specific instructions or if you encounter issues, refer to the [THPC documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1696039041/THPC+Installed+Applications) or contact the [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/55).

**Sources:** Compute2+Applications, THPC+Installed+Applications","2
",2,THPC Installed Applications.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/THPC Installed Applications.md
70,How do I install my own software into this environment?,"To install your own software into a custom Conda environment on the RIS Compute1 platform, follow these steps:

1. **Create a Conda Environment File**:
   - Create a `environment.yml` file that specifies your desired packages and dependencies. For example:
     ```yaml
     name: my_environment
     channels:
       - conda-forge
       - bioconda
     dependencies:
       - package1
       - package2
       - pip
       - pip:
           - my_pip_package
     ```

2. **Submit an Interactive Job**:
   - Start an interactive job using a Conda Docker image:
     ```bash
     bsub -Is -q general-interactive -a 'docker(continuumio/anaconda3:2021.11)' /bin/bash
     ```

3. **Install the Conda Environment**:
   - Once the job is running, install the environment using the `environment.yml` file:
     ```bash
     conda env create -f ~/environment.yml
     ```

4. **Activate the Environment**:
   - Activate your newly created environment:
     ```bash
     conda activate my_environment
     ```

5. **Install Additional Software**:
   - If you need to install additional software not listed in the `environment.yml` file, you can use `conda install` or `pip install` within the activated environment:
     ```bash
     conda install additional_package
     # or
     pip install additional_pip_package
     ```

For more detailed instructions, refer to the [Create Custom Conda Environment](https://washu.atlassian.net/wiki/spaces/RUD/pages/1793949820/Create+Custom+Conda+Environment) documentation.

**Sources:** Create+Custom+Conda+Environment, Software+Development+Using+Compute1","1
",1,noVNC.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/noVNC.md
71,"The document shows `PASSWORD=password`. Should I use the literal word 'password', or can I set my own?","The provided context does not contain any information about a `PASSWORD=password` setting or instructions on how to set a password. Therefore, I cannot provide an accurate answer to your question based on the given context.

If you have any other questions or need further assistance, please provide more details or clarify your query.

**Sources:** LabArchives+Notebook, RIS+Services+Policies","1
",1,noVNC.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/noVNC.md
72,"What is the difference between the `runtime` and `devel` CUDA image tags, and which one should I choose?","The context provided does not explicitly detail the differences between the `runtime` and `devel` CUDA image tags. However, based on general knowledge of CUDA Docker images:

- **`runtime` images**: These are optimized for running CUDA applications. They include the necessary CUDA libraries and drivers to execute CUDA-accelerated applications but do not include the development tools (like compilers, debuggers, and profiling tools) needed to build CUDA applications.

- **`devel` images**: These are designed for developing CUDA applications. They include all the components of the `runtime` images plus the development tools required to compile and debug CUDA code.

**Which one to choose?**
- Use the `runtime` image if you only need to run pre-built CUDA applications.
- Use the `devel` image if you need to build or modify CUDA applications within the container.

For more specific details, you may need to refer to Nvidia's official documentation or the documentation linked in the context.

**Sources:** RIS+Developed+Containers, Using+CUDA+in+Docker+Images","3
",3,noVNC.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/noVNC.md
73,What should I do if I forgot which compute node I ran the initial setup on?,"If you forgot which compute node you ran the initial setup on, you can use the `squeue` command to check the status of your jobs and identify the node. Here’s how:

1. **Check your running jobs**:
   ```bash
   squeue -u your_username
   ```
   This will list all your jobs along with the nodes they are running on.

2. **Identify the node**:
   Look for the `NODELIST` column in the output, which will show the node(s) your job is running on.

If you need more detailed information about the nodes, you can use the `sinfo` command:
```bash
sinfo
```
This will provide a list of available partitions and nodes, including their status.

For more specific guidance, refer to the [Compute2 Quickstart documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1720615105/Compute2+Quickstart).

**Sources:** Compute1+Quickstart, Compute2+Quickstart","1
",1,Globus Connect Personal.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Globus Connect Personal.md
74,"How do I make multiple directories accessible to Globus, not just the single one in the example?","To make multiple directories accessible to Globus Connect Personal, you can follow these steps:

1. **During Setup**:
   - When you initially set up Globus Connect Personal, you can specify multiple directories by including them in the `LSF_DOCKER_VOLUMES` environment variable. For example:
     ```bash
     PATH=/opt/globus/globusconnectpersonal-3.1.4:$PATH LSF_DOCKER_VOLUMES=""$HOME:$HOME /path/to/directory1:/path/to/directory1 /path/to/directory2:/path/to/directory2"" bsub -q general-interactive -Is -a 'docker(gcr.io/ris-registry-shared/globus)' /bin/bash
     ```

2. **After Setup**:
   - You can add additional directories through the Globus Connect Personal settings/preferences. Here’s how:
     - Open the Globus Connect Personal application.
     - Go to the settings/preferences.
     - Under the ""Access"" section, you can add and remove directories.
     - Select the new directory you want to add and specify whether it should be writable or readable.

3. **Running on Compute1**:
   - When running the client, you can specify multiple directories in the `restrict-paths` option. For example:
     ```bash
     PATH=/opt/globus/globusconnectpersonal-3.1.4:$PATH LSF_DOCKER_VOLUMES=""$HOME:$HOME /path/to/directory1:/path/to/directory1 /path/to/directory2:/path/to/directory2"" bsub -q general -m compute1-exec-N -a 'docker(gcr.io/ris-registry-shared/globus)' globusconnectpersonal -start -restrict-paths rw~/,rw/path/to/directory1,rw/path/to/directory2,/tmp
     ```

By following these steps, you can make multiple directories accessible to Globus Connect Personal.

**Sources:** Globus+Connect+Personal, Moving+Data+With+Globus, Using+Globus+Connect+Personal","4
",4,Globus Connect Personal.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Globus Connect Personal.md
75,"Must I always run the client on the exact same compute node that I used for the initial setup, and what if that node is unavailable?","Based on the provided context, there is no specific information indicating that you must always run the client on the exact same compute node that you used for the initial setup. The context mentions multiple login clients for Compute2 (e.g., `c2-login-001.ris.wustl.edu`, `c2-login-002.ris.wustl.edu`, `c2-login-003.ris.wustl.edu`), suggesting that you can connect via any of these login nodes.

If a specific node is unavailable, you can use one of the other available login nodes to connect to the Compute2 platform. The system is designed to handle such scenarios, and you should be able to continue your work without issues.

**Sources:** Compute1+Quickstart, Compute2+Quickstart","1
",1,Globus Connect Personal.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Globus Connect Personal.md
76,How do I determine the correct number of cores and amount of memory to request for my job?,"To determine the correct number of cores and amount of memory for your job, consider the following guidelines based on the provided context:

1. **Number of Cores (CPU Options)**:
   - Use the `-n` flag to specify the total number of tasks (cores) your job requires. For example, `-n 16` requests 16 tasks.
   - If you need a specific number of tasks per node, use `--ntasks-per-node`. For example, `-N 2 --ntasks-per-node=4` requests 2 nodes with 4 tasks per node.

2. **Memory/RAM Options**:
   - Use the `--mem` flag to specify the total amount of memory your job requires. For example, `--mem=100G` requests 100GB of memory.
   - You can also specify memory per CPU using `--mem-per-cpu`. Ensure that `--mem` is greater than or equal to `--mem-per-cpu` if both are specified.

3. **Additional Considerations**:
   - **GPU Resources**: If your job requires GPUs, use the `--gpus` flag to specify the number of GPUs needed. For example, `--gpus=2` requests 2 GPUs.
   - **Constraints**: Use the `--constraint` flag to specify required resources, such as CPU type (e.g., `--constraint=amd` for AMD CPUs).
   - **Specific Hosts**: If you need to run your job on a specific host, use the `-w` flag followed by the host name (e.g., `-w c2-node-001`).

For more detailed information, refer to the [Slurm documentation on srun](https://slurm.schedmd.com/srun.html) or [sbatch](https://slurm.schedmd.com/sbatch.html).

**Sources:** Batch+Jobs+(sbatch), Interactive+Jobs+(srun)","2
",2,Parabricks.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Parabricks.md
77,"How do I apply the fix for the `NVIDIA_VISIBLE_DEVICES` issue mentioned in the ""Known Issues"" section?","To apply the fix for the `NVIDIA_VISIBLE_DEVICES` issue mentioned in the ""Known Issues"" section, you need to prepend the `pbrun` command with the following line:

```bash
for VAR in $(printenv | grep CUDA_VISIBLE_DEVICES); do export ${VAR/CUDA/NVIDIA}; done
```

This command will ensure that the `NVIDIA_VISIBLE_DEVICES` environment variable is set correctly based on the `CUDA_VISIBLE_DEVICES` variable, which helps avoid issues with Parabricks attempting to use GPUs that the job does not have access to.

**Sources:** Parabricks, noVNC","5
",5,Parabricks.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Parabricks.md
78,"What are the available `pbrun` commands, and where can I find details on their specific options?","The available `pbrun` commands include `fq2bam`, `bqsr`, `applybqsr`, and `haplotypecaller`. For detailed information on their specific options, you can refer to the official [Parabricks documentation](https://docs.nvidia.com/clara/parabricks/4.0.0/index.html).

**Sources:** Interactive+Jobs+(srun), Parabricks","5
",5,Parabricks.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Parabricks.md
79,"Why can't I find the Intel compilers (e.g., `icc`, `ifx`, `ifort`) after starting an interactive session?","The Intel compilers (`icc`, `ifx`, `ifort`) are not directly available in the interactive session because the environment needs to be set up first. After starting an interactive session with the Intel® Compiler Base Docker image, you need to source the Intel oneAPI environment setup script to make the compilers available.

To do this, run the following command in your interactive session:

```bash
. /opt/intel/oneapi/setvars.sh
```

This will set up the necessary environment variables and paths, allowing you to use the Intel compilers. For more details, refer to the [Interactive Command-Line Session](#interactive-command-line-session) section in the provided context.

**Sources:** Intel®+Compiler+Base, Intel®+Compiler+Base+Tutorial","5
",5,Intel® Compiler Base.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Intel® Compiler Base.md
80,What are the main advantages of using a multi-stage build over a single-stage build for my application?,"The main advantages of using a multi-stage build over a single-stage build for your application are:

1. **Smaller Docker Image**: A multi-stage build results in a smaller Docker image because it only includes the compiled binaries and necessary runtime dependencies, excluding the build tools and source code. This reduces computing time, resources, and costs.

2. **Security**: By keeping the source code out of the final image, a multi-stage build helps prevent the exposure of sensitive information, which is particularly important if the image is to be shared publicly.

3. **Efficiency**: The smaller image size leads to faster downloads and deployments, making the application more efficient to run and manage.

For more details, you can refer to the [Intel® Compiler Base Tutorial](https://washu.atlassian.net/wiki/spaces/RUD/pages/1793949810/Intel%C2%AE+Compiler+Base+Tutorial) and the [Intel® Compiler Base](https://washu.atlassian.net/wiki/spaces/RUD/pages/1782644784/Intel%C2%AE+Compiler+Base) documentation.

**Sources:** Intel®+Compiler+Base, Intel®+Compiler+Base+Tutorial","5
",5,Intel® Compiler Base.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Intel® Compiler Base.md
81,"My application compiled successfully, but when I run the final multi-stage image, I get a ""shared library not found"" error. What should I check?","When you encounter a ""shared library not found"" error after running your final multi-stage image, there are several things you should check:

1. **Verify Copied Libraries**: Ensure that all necessary shared libraries from the build stage are copied to the runtime stage. In your Dockerfile, you have lines like `COPY --from=build /usr/local/lib /usr/local/lib` and `COPY --from=build /usr/local/include /usr/local/include`. Double-check that these paths contain all the required libraries.

2. **Library Dependencies**: Use the `ldd` command on the binary in the build stage to list all shared library dependencies. For example:
   ```bash
   ldd /usr/local/bin/mpi_hello_world
   ```
   Ensure all listed libraries are included in the runtime stage.

3. **Environment Variables**: Verify that environment variables like `LD_LIBRARY_PATH` are set correctly in the runtime stage to include the directories where the shared libraries are located.

4. **Runtime Dependencies**: Ensure all runtime dependencies are installed in the runtime stage. For example, your Dockerfile includes commands like:
   ```bash
   RUN apt-get update && apt-get install -y wget perl-base libnuma-dev \
       libgtk2.0 libatk1.0-0 libcairo2 gfortran tcsh libnl-3-dev \
       libmnl0 tcl tk \
       libusb-1.0-0-dev pciutils lsof ethtool libfuse2
   ```
   Make sure these packages are necessary and correctly installed.

5. **Multi-Stage Build**: Ensure that the multi-stage build process is correctly copying all necessary components from the build stage to the runtime stage. The example Dockerfile provided in the context shows how to do this, but you should verify that your specific implementation follows the same principles.

If you've checked all these points and the issue persists, you may need to provide more specific details about the error message or the libraries that are missing.

**Sources:** Intel®+Compiler+Base, Intel®+Compiler+Base+Tutorial","4
",4,Intel® Compiler Base.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Intel® Compiler Base.md
82,Can I use the AFNI graphical user interface (GUI) with this container?,"Based on the provided context, the AFNI/TORTOISE Docker image does not include the AFNI graphical user interface (GUI). The documentation explicitly states that ""GUI is not currently included in this image."" Therefore, you cannot use the AFNI GUI with this container.

**Sources:** AFNI_TORTOISE, TensorFlow","5
",5,AFNI_TORTOISE.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/AFNI_TORTOISE.md
83,"Do I need to perform the ""Initial Setup"" every time I want to run a job?","Based on the provided context, there is no mention of an ""Initial Setup"" requirement for running jobs. The documentation focuses on job execution examples, guidelines, and various commands for managing jobs, but it does not specify that an initial setup is needed every time you want to run a job.

If you are referring to setting up your environment or configuring specific parameters for your jobs, you may need to do that once, but not necessarily every time you submit a job. For example, setting up job groups, arrays, or resource requirements might be done once and then reused for multiple job submissions.

If you have more specific details about what you mean by ""Initial Setup,"" please provide them so I can give a more accurate answer.

**Sources:** Compute2+General+Guidelines, Job+Execution+Examples","1
",1,AFNI_TORTOISE.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/AFNI_TORTOISE.md
84,How can I run a script as a non-interactive batch job?,"To run a script as a non-interactive batch job, you can use the `bsub` command for Compute1 or the `sbatch` command for Compute2. Here are the steps for each:

### For Compute1 (using `bsub`):
1. Create a job file with your script and the necessary `bsub` options. For example:
   ```bash
   #!/bin/bash
   #BSUB -J my_job_name
   #BSUB -q general
   #BSUB -n 2
   #BSUB -M 8000000
   #BSUB -W 10
   #BSUB -u myemail@wustl.edu
   #BSUB -i input_file
   #BSUB -oo ${HOME}/path/to/output_file
   #BSUB -R 'select[mem>8000] rusage[mem=8000] span[hosts=1]'

   /usr/bin/some_program
   ```
2. Submit the job file using `bsub`:
   ```bash
   bsub < job_file.bsub
   ```

### For Compute2 (using `sbatch`):
1. Create a job script with your script and the necessary `sbatch` options. For example:
   ```bash
   #!/bin/bash
   #SBATCH -J my_job_name
   #SBATCH --partition=general
   #SBATCH -n 2
   #SBATCH --mem=8000M
   #SBATCH -t 10
   #SBATCH --mail-user=myemail@wustl.edu
   #SBATCH --input=input_file
   #SBATCH --output=${HOME}/path/to/output_file

   /usr/bin/some_program
   ```
2. Submit the job script using `sbatch`:
   ```bash
   sbatch job_script.sh
   ```

These commands will submit your script as a non-interactive batch job, allowing it to run in the background without requiring an interactive session.

**Sources:** Batch+Jobs+(sbatch), Job+Execution+Examples","1
",1,AFNI_TORTOISE.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/AFNI_TORTOISE.md
85,What is the difference between launching SAS through the dedicated `SAS` application versus the `Compute RIS Desktop`?,"When launching SAS through the dedicated `SAS` application versus the `Compute RIS Desktop` application, there are a few key differences:

1. **Automatic Launch**:
   - If you use the `SAS` application, SAS will start up automatically once the job is launched.

2. **Manual Launch**:
   - If you use the `Compute RIS Desktop` application, you need to manually load and launch SAS using the following commands:
     ```
     module load SAS
     sas
     ```

3. **Version Availability**:
   - You can check the available versions of SAS using the command `module avail SAS` in either method.

The `SAS` application provides a more streamlined experience by automatically starting SAS, while the `Compute RIS Desktop` application offers more flexibility but requires manual setup.

**Sources:** Compute2, SAS","5
",5,SAS.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/SAS.md
86,The document shows how to list available SAS versions; how do I load a specific version instead of the default?,"To load a specific version of SAS instead of the default, you can use the `module load` command followed by the specific version you want. First, list the available versions using:

```
module avail SAS
```

Then, load the desired version with:

```
module load SAS/<version>
```

Replace `<version>` with the specific version number you want to use. For example, if you want to load version 9.4, you would use:

```
module load SAS/9.4
```

**Sources:** IBM+Aspera, SAS","5
",5,SAS.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/SAS.md
87,How do I run a SAS program as a non-interactive batch job?,"To run a SAS program as a non-interactive batch job, you can use the `bsub` command to submit your job to the LSF (IBM Spectrum LSF) system. Here’s a general approach based on the provided context:

1. **Prepare Your SAS Script**: Ensure your SAS program is saved as a `.sas` file.

2. **Submit the Job Using `bsub`**: Use the `bsub` command to submit your SAS job. You’ll need to load the SAS module and specify your script. Here’s an example command:

   ```bash
   bsub -J ""your_job_name"" -q general -o ""%J.out"" -e ""%J.err"" -a 'docker(sas_image)' ""module load SAS; sas /path/to/your_script.sas""
   ```

   - `-J ""your_job_name""`: Specifies the job name.
   - `-q general`: Specifies the queue (adjust as needed).
   - `-o ""%J.out""`: Redirects standard output to a file named with the job ID.
   - `-e ""%J.err""`: Redirects standard error to a file named with the job ID.
   - `-a 'docker(sas_image)'`: Specifies the Docker image to use (replace `sas_image` with the appropriate SAS image).
   - `module load SAS; sas /path/to/your_script.sas`: The command to load SAS and run your script.

3. **Check Job Status**: You can check the status of your submitted job using the `bjobs` command.

4. **Review Output**: After the job completes, review the output and error files (e.g., `job_id.out` and `job_id.err`) for results and any issues.

For more details on job submission and resource requirements, refer to the [Job Execution Examples](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705182249/Job+Execution+Examples) documentation. If you need to specify additional resources (e.g., memory, CPU), you can use the `-R` flag as described in the context.

**Sources:** Job+Execution+Examples, SAS","2
",2,SAS.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/SAS.md
88,"In the `bsub` command, what should I use for the `location-of-docker-image`?","In the `bsub` command, the `location-of-docker-image` refers to the Docker image you want to use for your job. This can be specified in the format `docker(image_name:tag)` or `docker_build(repo.example.com/repo_username/example_container_name:latest)` if you are building a Docker image from a non-standard Dockerfile.

For example:
- To use an existing Docker image: `-a 'docker(alpine)'`
- To build a Docker image from a non-standard Dockerfile: `-a 'docker_build(repo.example.com/repo_username/example_container_name:latest)' -- -f /path/to/the/NonStandardDockerfile example_container_build_directory`

Make sure the Docker image includes `/bin/sh` to avoid errors during execution.

**Sources:** Docker+and+the+RIS+Compute1+Platform, Job+Execution+Examples","3
",3,Integrative Genomics Viewer.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Integrative Genomics Viewer.md
89,How can I modify the Dockerfile to use a different version of IGV?,"To modify the Dockerfile to use a different version of IGV, you need to change the `IGV_URL` environment variable in the Dockerfile. The current URL points to IGV version 2.9.4. To use a different version, replace the URL with the one corresponding to the desired version of IGV for Linux.

Here’s how you can do it:

1. Locate the line in the Dockerfile that sets the `IGV_URL`:
   ```Dockerfile
   ENV IGV_URL=""https://data.broadinstitute.org/igv/projects/downloads/2.9/IGV_Linux_2.9.4_WithJava.zip""
   ```

2. Replace the URL with the one for the version you want to use. For example, if you want to use IGV version 2.14.0, you would change it to:
   ```Dockerfile
   ENV IGV_URL=""https://data.broadinstitute.org/igv/projects/downloads/2.14/IGV_Linux_2.14.0_WithJava.zip""
   ```

3. After making this change, rebuild and push your Docker container as described in the documentation.

This will ensure that the Docker image includes the specified version of IGV.

**Sources:** Integrative+Genomics+Viewer, Non-RIS+Docker+Images","5
",5,Integrative Genomics Viewer.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Integrative Genomics Viewer.md
90,My `bsub` command is failing with an error about user groups. What should I do?,"The error you're encountering with your `bsub` command is likely related to the `-G` option, which specifies the user group. Here are a few things you can check or try:

1. **Verify the Group Name**: Ensure that the group name you're using with the `-G` option is correct and that you have permission to use it. The group name should be specified as `${group_name}` in the examples, so make sure you're replacing it with the actual group name you're supposed to use.

2. **Check Group Membership**: Confirm that your user account is a member of the group you're trying to use. You can check your group memberships with the `groups` command in the terminal.

3. **Use the Correct Syntax**: Make sure you're using the correct syntax for the `-G` option. It should be `-G group_name` without any additional characters or spaces.

4. **Contact Support**: If you're still having trouble, you may need to contact your system administrator or support team for assistance. They can help you verify your group memberships and ensure that your `bsub` command is correctly configured.

If you provide more details about the specific error message you're seeing, I can offer more targeted advice.

**Sources:** Job+Execution+Examples, LSF+to+Slurm+Translator","4
",4,Integrative Genomics Viewer.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Integrative Genomics Viewer.md
91,"For the PASSWORD environment variable, should I set my own password or use the literal word ""password""?","The provided context does not contain any information about a `PASSWORD` environment variable. Therefore, I cannot provide a specific answer based on the given context.

In general, when setting environment variables for passwords, it is a security best practice to use a secure, unique password rather than the literal word ""password"". Always follow your organization's security guidelines for handling sensitive information like passwords.

**Sources:** Docker+Wrapper+Environment+Variables, Software+Development+Using+Compute1","1
",1,cisTEM.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/cisTEM.md
92,"After launching the job in Open On Demand, where do I run the `cisTEM` command to start the software?","After launching the job in Open OnDemand, you need to run the `cisTEM` command in the interactive session that is started. Once your interactive session is in the ""Running"" status and you have accessed the GUI, you can open a terminal within that session and execute the following command:

```
cisTEM
```

This will start the cisTEM software.

**Sources:** Open+OnDemand, cisTEM","5
",5,cisTEM.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/cisTEM.md
93,"The document provides the Docker image location under the ""Dockerfile"" heading. Where can I find the actual Dockerfile used to build this image?","The context provided does not explicitly mention the location of the actual Dockerfile used to build the image. However, it does describe the process of creating a Dockerfile and using it to build an image.

If you are looking for the Dockerfile used in the tutorial, you can follow the steps outlined in the ""Creating a Docker Container"" section of the documentation. The Dockerfile is created within a directory (folder) designed for the image, and in the example, this directory is named `docker-example`.

To find or create the Dockerfile:
1. Create a directory named `docker-example`.
2. Inside this directory, create a file named `Dockerfile` with the content provided in the tutorial.

For more detailed instructions, refer to the ""Creating a Docker Container"" section in the [Docker Tutorial](https://washu.atlassian.net/wiki/spaces/RUD/pages/1865285780/Docker+Tutorial).

**Sources:** Docker, Docker+Tutorial","1
",1,cisTEM.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/cisTEM.md
94,The software I need is not available as a module. How can I install or request it?,"Based on the provided context, the documentation primarily focuses on applications available on the Compute2 platform and some specific instructions for SAS on Compute1. However, it does not provide explicit instructions on how to install or request software that is not available as a module.

If the software you need is not listed as a module or application in the documentation, you may need to follow these general steps:

1. **Check Available Modules**: Ensure the software is not already available by listing all modules using the `ml avail` or `module avail` command.
2. **Request Software**: If the software is not available, you can request it through the RIS Service Desk. Follow the link provided in the documentation: [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/43).
3. **Install Locally**: If permitted, you might be able to install the software locally in your home directory or a project directory, depending on the policies of the Compute platform.

For more specific guidance, you may need to contact the RIS support team directly.

**Sources:** Compute2+Applications, SAS","4
",4,THPC.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/THPC.md
95,How do I find out which `THPC_BUILD` versions are available and compatible with the hardware I want to use?,"To find out which `THPC_BUILD` versions are available and compatible with the hardware you want to use, follow these steps:

1. **Check Available Build Releases**:
   - Run the command `ls /opt/thpc/` inside a running THPC job to list all available build releases. These are formatted as `YYYY.MM`.

2. **Determine Micro Architecture Compatibility**:
   - Use the command `bhosts -R 'select[cpumicro==micro_architecture]' queue_name` to check which hosts in a specific queue support your desired micro architecture (e.g., `skylake`, `cascadelake`, `zen5`).
   - For GPU hosts, you can add the `-gpu` flag to the command to see GPU-specific compatibility.

3. **Verify Valid Combinations**:
   - Not all combinations of `THPC_BUILD` and micro architectures are valid. Ensure the build release you choose aligns with the micro architecture of the host you intend to use.

For example, to check for `skylake` micro architecture in the `general-interactive` queue, you would run:
```bash
bhosts -R 'select[cpumicro==skylake]' general-interactive
```

For GPU hosts:
```bash
bhosts -R 'select[cpumicro==skylake]' general-interactive -gpu
```

The current release as of the context is `2023.06`, but you can find the latest by running the `ls /opt/thpc/` command.

**Sources:** THPC, THPC+Installed+Applications","5
",5,THPC.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/THPC.md
96,What is the simplest way to start a THPC session if I'm not familiar with all the command-line options?,"The simplest way to start a THPC session if you're not familiar with all the command-line options is to use the `thpc-terminal` wrapper script. This script starts a command-line session with default resource values. Here’s the command you would use:

```
thpc-terminal /bin/bash
```

This will launch an interactive terminal session with the default settings, making it easy to get started without needing to specify all the options manually.

**Sources:** THPC, THPC+Installed+Applications","5
",5,THPC.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/THPC.md
97,How do I configure the YAML file if I only have a protein sequence and no small molecule?,"To configure the YAML file for RoseTTAFold All-Atom when you only have a protein sequence and no small molecule, you should focus on the `protein_inputs` section and omit the `sm_inputs` section. Here’s how you can set it up:

1. **Inherit the base settings**:
   ```yaml
   defaults:
     - base
     - _self_
   ```

2. **Set the job name**:
   ```yaml
   job_name: ""your_protein_name""
   ```

3. **Specify the protein sequence input**:
   ```yaml
   protein_inputs:
     A:
       fasta_file: inputs/your_protein.fasta
   ```

4. **Omit the `sm_inputs` section** since you don’t have a small molecule.

Here’s an example of what the complete YAML file might look like:

```yaml
defaults:
  - base
  - _self_

job_name: ""K7N608""

protein_inputs:
  A:
    fasta_file: inputs/K7N608.fasta

# No sm_inputs section needed
```

This configuration will ensure that RoseTTAFold All-Atom processes only the protein sequence provided in the FASTA file.

**Sources:** AlphaFold, RoseTTAFold+All-Atom","5
",5,RoseTTAFold All-Atom.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/RoseTTAFold All-Atom.md
98,How much memory and how many CPU slots should I request for my job?,"The context provided does not specify exact memory or CPU requirements for your job. However, it does offer guidelines on how to request these resources using the `srun` command:

- To request a specific amount of memory, use the `--mem` option followed by the desired amount (e.g., `--mem=100G` for 100GB).
- To request CPU slots, use the `-n` option for the total number of tasks (e.g., `-n 16` for 16 tasks) or specify tasks per node with `-N` and `--ntasks-per-node` (e.g., `-N 2 --ntasks-per-node=4` for 2 nodes with 4 tasks each).

For more detailed options, refer to the [Slurm documentation on srun](https://slurm.schedmd.com/srun.html). If you have specific requirements for your job, you may need to adjust these values accordingly.

**Sources:** Compute2+General+Guidelines, Interactive+Jobs+(srun)","1
",1,RoseTTAFold All-Atom.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/RoseTTAFold All-Atom.md
99,"My job is failing with a ""file not found"" error, but I've set the `INPUT_PATH`. What else do I need to check?","If your job is failing with a ""file not found"" error despite setting the `INPUT_PATH`, here are a few things to check:

1. **Verify the Path**: Ensure that the path specified in `INPUT_PATH` is correct and accessible. The path should be relative to your storage allocation or an absolute path if needed.

2. **Check File Permissions**: Make sure you have the necessary permissions to access the file. You can check permissions using commands like `ls -l` on Linux or Mac.

3. **Home Directory Space**: If the file is in your home directory, check if it's full. You can use the command `du -hsx .[^.]* * 2>/dev/null | sort -rh | head -10` to see disk usage.

4. **Storage Allocation Space**: Verify that your storage allocation has enough space. You can check this with `df --output -h /storage2/fs1/${STORAGE_ALLOCATION}`.

5. **File Existence**: Confirm that the file actually exists at the specified path. You can use commands like `ls` or `find` to locate the file.

6. **Job Execution Environment**: Ensure that the job is running in an environment where the specified path is accessible. If you're using a compute node, make sure the path is mounted correctly.

If you've checked all these and the issue persists, you may need to submit a ticket to the service desk for further assistance.

**Sources:** FAQ, Troubleshooting+Connection+to+the+Storage+Platforms","2
",2,RoseTTAFold All-Atom.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/RoseTTAFold All-Atom.md
100,How do I modify the example command to use the full database (`full_dbs`) or predict a multimer model?,"The provided context does not contain information about modifying commands to use the full database (`full_dbs`) or predict a multimer model. Therefore, I cannot provide an answer based on the given context.

If you have specific details or additional context about the commands or tools you are referring to, please provide them, and I will do my best to assist you.

**Sources:** Non-RIS+Docker+Images, R+Tutorial","5
",5,AlphaFold.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/AlphaFold.md
101,The example requests 8 CPUs and 8GB of memory. How do I determine the appropriate resource allocation for my specific protein?,"The provided context does not contain specific information about determining resource allocation for protein-related tasks. However, based on general knowledge, here are some guidelines to help you determine the appropriate resource allocation for your specific protein analysis:

1. **CPU Requirements**:
   - For simple protein sequence analysis or alignment, 2-4 CPUs may suffice.
   - For more complex tasks like molecular dynamics simulations or protein folding, you might need 8-16 CPUs or more.

2. **Memory (RAM) Requirements**:
   - Basic tasks might require 4-8GB of RAM.
   - Complex simulations or large-scale analyses could need 16GB or more.

3. **GPU Requirements**:
   - If your task involves machine learning or advanced simulations, consider requesting GPUs. The number depends on the complexity and scale of your task.

4. **Specific Host Options**:
   - If your task requires specific hardware (e.g., AMD or Intel CPUs), use the `--constraint` option to specify the required resources.

5. **Job Priority**:
   - If your task is time-sensitive, you can set a higher priority using the `--priority` flag.

For precise recommendations, consider the following steps:
- **Consult Documentation**: Look for documentation specific to the software or tool you are using for protein analysis.
- **Benchmarking**: Run a small test job to see how much resources it consumes and scale up accordingly.
- **Ask Experts**: Reach out to experts or support teams who might have experience with similar tasks.

If you provide more details about the specific protein analysis task you are performing, I can offer more tailored advice.

**Sources:** Interactive+Jobs+(srun), Job+Execution+Examples","2
",2,AlphaFold.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/AlphaFold.md
102,How do I correctly set the input (`--fasta_paths`) and output (`--output_dir`) paths for my job?,"The provided context does not contain specific information about setting input (`--fasta_paths`) and output (`--output_dir`) paths for a job. However, based on general knowledge of job submission scripts, you can typically set these paths in your script as follows:

1. **Input Path (`--fasta_paths`)**:
   - Specify the path to your input FASTA files. For example:
     ```bash
     --fasta_paths /path/to/your/input.fasta
     ```

2. **Output Path (`--output_dir`)**:
   - Specify the directory where you want the output to be saved. For example:
     ```bash
     --output_dir /path/to/your/output/directory
     ```

If you are using a script for job submission (like `sbatch` or `bsub`), you would include these options in your script. For example, in an `sbatch` script, it might look like this:

```bash
#!/bin/bash
#SBATCH -p general
--fasta_paths /path/to/your/input.fasta
--output_dir /path/to/your/output/directory
```

If you need more specific guidance, please provide additional details about the job submission system or the tool you are using.

**Sources:** Batch+Jobs+(sbatch), Job+Execution+Examples","3
",3,AlphaFold.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/AlphaFold.md
103,What value should I use for `location-of-docker-image` in the `bsub` command?,"The value for `location-of-docker-image` in the `bsub` command should be the name of the Docker image you want to use, typically in the format `repository/username/image-name:tag`. For example:

```
bsub -a 'docker(ubuntu:latest)' ...
```

If you are using a custom Docker image, replace `ubuntu:latest` with the appropriate image name and tag. The context does not provide specific examples of valid Docker image locations, so ensure the image is available in your Docker environment or accessible via a registry.

**Sources:** Docker+and+the+RIS+Compute1+Platform, Job+Execution+Examples","4
",4,Web Development Container.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Web Development Container.md
104,"How can I set a password for the MariaDB 'root' user, since it has no password by default?","To set a password for the MariaDB 'root' user, you can follow these steps:

1. Access the MariaDB command line by running the following command in your terminal:
   ```bash
   mysql -u root
   ```

2. Once logged in, execute the following SQL command to set a password for the 'root' user:
   ```sql
   ALTER USER 'root'@'localhost' IDENTIFIED BY 'your_password';
   ```
   Replace `'your_password'` with the password you want to set.

3. Flush the privileges to ensure the changes take effect:
   ```sql
   FLUSH PRIVILEGES;
   ```

4. Exit the MariaDB command line by typing:
   ```sql
   EXIT;
   ```

This will set a password for the MariaDB 'root' user. Make sure to use a strong password for security purposes.

**Sources:** Connecting+to+Storage+from+MacOS, Web+Development+Container","3
",3,Web Development Container.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Web Development Container.md
105,My website files are not in the example path. How do I correctly set the `LSF_DOCKER_VOLUMES` variable to point to my own project directory?,"To set the `LSF_DOCKER_VOLUMES` variable to point to your own project directory, you need to specify the source and destination paths in the format `src:dst`. The source (`src`) should be the path to your project directory on the host system, and the destination (`dst`) should be the path where you want it mounted inside the Docker container.

Here’s an example of how to set it:

```bash
export LSF_DOCKER_VOLUMES=""/path/to/your/project:/path/in/container""
bsub -Is -G ${group_name} -q general-interactive -a 'docker(ubuntu)' /bin/bash
```

Replace `/path/to/your/project` with the actual path to your project directory on the host system, and `/path/in/container` with the desired path inside the container.

If you need to mount multiple directories, separate them with spaces:

```bash
export LSF_DOCKER_VOLUMES=""/path/to/project1:/container/path1 /path/to/project2:/container/path2""
```

Ensure that the paths you use are accessible and valid on both the host and within the container.

**Sources:** Docker+Wrapper+Environment+Variables, Docker+and+the+RIS+Compute1+Platform","4
",4,Web Development Container.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Web Development Container.md
106,"Where do I get the ""Provided Password"" and the `aspera_key` file?","The ""Provided Password"" and the `aspera_key` file are not detailed in the provided context. However, based on the information available, it seems these credentials are likely provided to you by the IBM Aspera service administrators or through the RIS Service Desk. You may need to contact the RIS Service Desk for assistance with obtaining these credentials. You can reach them through the [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/43).

**Sources:** FAQ, IBM+Aspera","3
",3,IBM Aspera.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/IBM Aspera.md
107,The example `ascp` command appears to be for uploading a file. How would I modify this to download a file instead?,"To modify the `ascp` command to download a file instead of uploading, you need to reverse the source and destination paths in the command. Here's how you can adjust the example command for downloading:

Original command (uploading):
```
ascp -i ""/path/to/aspera_key/aspera_tokenauth_id_rsa"" -Q -l 200m -k 1 test.file 'asp-dbgap@gap-submit.ncbi.nlm.nih.gov:test'
```

Modified command (downloading):
```
ascp -i ""/path/to/aspera_key/aspera_tokenauth_id_rsa"" -Q -l 200m -k 1 'asp-dbgap@gap-submit.ncbi.nlm.nih.gov:test' test.file
```

In this modified command:
- The source path (`asp-dbgap@gap-submit.ncbi.nlm.nih.gov:test`) comes first
- The destination path (`test.file`) comes last
- The file will be downloaded from the remote server to your local machine

**Sources:** IBM+Aspera, Moving+Data+With+Globus, Moving+Data+With+Globus+CLI","5
",5,IBM Aspera.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/IBM Aspera.md
108,Can you provide an example of a script to transfer multiple files in a loop?,"To transfer multiple files in a loop using Globus, you can use the Globus CLI with a script. Below is an example of a bash script that demonstrates how to transfer multiple files from one endpoint to another using a loop:

```bash
#!/bin/bash

# Define the source and destination endpoints
SOURCE_ENDPOINT=""b9545fe1-f647-40bf-9eaf-e66d2d1aaeb4""
DEST_ENDPOINT=""b9545fe1-f647-40bf-9eaf-e66d2d1aaeb4""

# Define the source and destination paths
SOURCE_PATH=""/storageN/fs1/your_allocation/Active/source_directory""
DEST_PATH=""/storageN/fs1/your_allocation/Active/destination_directory""

# List of files to transfer
FILES=(""file1.txt"" ""file2.txt"" ""file3.txt"")

# Loop through each file and transfer it
for FILE in ""${FILES[@]}""; do
    echo ""Transferring $FILE...""
    globus transfer ""$SOURCE_ENDPOINT:$SOURCE_PATH/$FILE"" ""$DEST_ENDPOINT:$DEST_PATH/$FILE""
done

echo ""All files transferred successfully.""
```

### Explanation:
1. **Define Endpoints and Paths**: Set the source and destination endpoints and paths.
2. **List of Files**: Create an array of files you want to transfer.
3. **Loop Through Files**: Use a `for` loop to iterate over each file and transfer it using the `globus transfer` command.

### Notes:
- Replace `your_allocation` with your actual storage allocation name.
- Ensure you have the Globus CLI installed and are logged in (`globus login`).
- You can modify the `FILES` array to include the files you want to transfer.

This script provides a basic example. You can expand it to include error handling, logging, and other features as needed.

**Sources:** Moving+Data+With+Globus, Moving+Data+With+Globus+CLI","1
",1,IBM Aspera.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/IBM Aspera.md
109,What is the difference between the `fsl6` and `fsl6_min` Docker images?,"The `fsl6` and `fsl6_min` Docker images differ primarily in their size and the software they include. The `fsl6` image is the full version, containing all the software listed in the context, such as tools for functional MRI, structural MRI, diffusion MRI, GLM/Stats, and other utilities. On the other hand, the `fsl6_min` image is a smaller, minimal version of the `fsl6` image, likely excluding some of the less commonly used software to reduce its size. Both images are hosted at `gcr.io/ris-registry-shared/`.

**Sources:** Docker+Tutorial, fsl6","4
",4,fsl6.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/fsl6.md
110,How do I access my data files from within the FSL container?,"To access your data files from within the FSL container, you need to set up the environment variables and paths correctly when running the container. Here are the steps:

1. **Preserve Environment Variables**: Ensure that the `LSF_DOCKER_PRESERVE_ENVIRONMENT` variable is set to `false` to avoid having to provide the full path for each application. You can do this by prepending the variable to your `bsub` command:
   ```bash
   LSF_DOCKER_PRESERVE_ENVIRONMENT=false bsub -Is -q general-interactive -a 'docker(gcr.io/ris-registry-shared/<VERSION>)' /bin/bash
   ```
   Replace `<VERSION>` with either `fsl6` or `fsl6_min` depending on the image you are using.

2. **Mount Data Directories**: If you need to access specific directories (like your home directory or scratch storage), you can mount them using the `LSF_DOCKER_VOLUMES` environment variable. For example:
   ```bash
   LSF_DOCKER_VOLUMES=""$HOME:$HOME /path/to/data:/path/in/container"" bsub -Is -q general-interactive -a 'docker(gcr.io/ris-registry-shared/<VERSION>)' /bin/bash
   ```
   Replace `/path/to/data` with the path to your data on the host system and `/path/in/container` with the desired path inside the container.

3. **Run Commands**: Once inside the container, you can access your data files using the paths you mounted. For example, if you mounted your home directory, you can access it at `/home/your_username` inside the container.

For more detailed instructions, refer to the [RIS User Documentation on fsl6](https://washu.atlassian.net/wiki/spaces/RUD/pages/1782054930/fsl6) and [Globus Connect Personal](https://washu.atlassian.net/wiki/spaces/RUD/pages/1782415379/Globus+Connect+Personal).

**Sources:** Globus+Connect+Personal, fsl6","4
",4,fsl6.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/fsl6.md
111,Can you provide a complete example of a non-interactive command to run an FSL tool like BET on a file?,"To run the FSL tool BET (Brain Extraction Tool) on a file in a non-interactive mode using the `fsl6` container, you can use the following `bsub` command:

```bash
bsub -a 'docker(gcr.io/ris-registry-shared/fsl6)' bet /path/to/input_file /path/to/output_file
```

Replace `/path/to/input_file` with the path to your input file and `/path/to/output_file` with the desired path for the output file.

If you need to preserve environment variables or specify additional options, you can modify the command accordingly. For example:

```bash
LSF_DOCKER_PRESERVE_ENVIRONMENT=false bsub -a 'docker(gcr.io/ris-registry-shared/fsl6)' bet /path/to/input_file /path/to/output_file -f 0.5 -g 0 -n -m
```

This command sets the `LSF_DOCKER_PRESERVE_ENVIRONMENT` variable to `false` and includes additional BET options (`-f`, `-g`, `-n`, `-m`). Adjust the paths and options as needed for your specific use case.

**Sources:** Job+Execution+Examples, fsl6","5
",5,fsl6.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/fsl6.md
112,"I only have a serial number and activation key from Stata, not a `stata.lic` file. How do I create the license file needed to run a job?","To create the `stata.lic` file needed to run a Stata job, follow these steps:

1. **Obtain the necessary information**: Ensure you have the Serial Number, Code, and Authorization from Stata, which you received after purchasing the license.

2. **Submit an interactive command-line session**: Use the following commands to start an interactive session:
   ```bash
   export PATH=/app/stata:$PATH
   export LSF_DOCKER_VOLUMES=""/storageN/fs1/${STORAGE_ALLOCATION}/Active/Stata_License:/opt/stata""
   bsub -Is -q general-interactive -a 'docker(ghcr.io/washu-it-ris/stata:18)' /bin/bash
   ```

3. **Navigate to the Stata application folder**:
   ```bash
   cd /app/stata
   ```

4. **Run the Stata license generator**:
   ```bash
   ./stinit
   ```

5. **Fill out the required information**: Enter the Serial Number, Code, and Authorization when prompted by the license generator.

6. **Save the license file**: The `stata.lic` file will be generated in the `/app/stata` folder. Copy this file to a secure location, such as your storage allocation (e.g., `/storageN/fs1/${STORAGE_ALLOCATION}/Active/Stata_License`), to use it in future Stata jobs.

For more details, refer to the [Generating a Stata License File](https://washu.atlassian.net/wiki/spaces/RUD/pages/1782645003/Stata#Generating-a-Stata-License-File) section in the documentation.

**Sources:** SAS, Stata","5
",5,Stata.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Stata.md
113,What specific values do I need to enter in Open On Demand to launch a Stata GUI session?,"To launch a Stata GUI session in Open OnDemand, you need to enter the following specific values:

1. **Mounts**:
   ```
   /storageN/fs1/${STORAGE_ALLOCATION}/Active/Stata_License:/opt/stata
   ```
   Replace `/storageN/fs1/${STORAGE_ALLOCATION}/Active/Stata_License` with the path to the folder containing your Stata license.

2. **Environment Variables**:
   ```
   PASSWORD=password PATH=/app/stata:$PATH
   ```
   Replace `password` with your chosen password. You can also optionally specify the display size with:
   ```
   DISPLAY_WIDTH=<width> DISPLAY_HEIGHT=<height>
   ```
   (Default values are 1024 for width and 768 for height.)

3. **Docker Image**:
   ```
   ghcr.io/washu-it-ris/stata:18
   ```

After launching the job, open Stata by entering the version of Stata you are licensed to use (`xstata`, `xstata-mp`, or `xstata-se`) in the GUI terminal.

**Sources:** Open+OnDemand, Stata","5
",5,Stata.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Stata.md
114,I have licenses for multiple versions of Stata. How do I make sure my job uses the correct one?,"To ensure your Stata job uses the correct license for the specific version you are licensed for, follow these steps:

1. **Create Separate License Folders**: Create a distinct folder in your storage allocation for each Stata version you are licensed to use. For example:
   ```
   /storageN/fs1/${STORAGE_ALLOCATION}/Active/Stata18_License/
   /storageN/fs1/${STORAGE_ALLOCATION}/Active/Stata17_License/
   ```

2. **Store Licenses Appropriately**: Place the corresponding license file (`stata.lic`) in each respective folder. Ensure no other files are in these folders.

3. **Mount the Correct License Folder**: When submitting your job, mount the folder containing the license for the specific Stata version you intend to use. For example, if using Stata 18, include the following in your `Mounts` field:
   ```
   /storageN/fs1/${STORAGE_ALLOCATION}/Active/Stata18_License:/opt/stata
   ```

4. **Specify the Correct Stata Version**: In the GUI terminal, enter the version of Stata you are licensed to use (e.g., `xstata`, `xstata-mp`, or `xstata-se`).

By following these steps, you ensure that your job uses the correct license for the specific Stata version.

**Sources:** Non-RIS+Docker+Images, Stata","5
",5,Stata.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Stata.md
115,The document lists two different Docker image paths. Which one should I use?,"The document lists two different Docker image paths:

1. **`username/docker-example:latest`** - This is used in the Docker Tutorial for creating and running a custom Docker image with software like `cowsay` and `fortune`.
2. **`docker(username/docker-example:latest)`** - This is used specifically for running Docker containers on the RIS Compute1 Platform.

If you are working on the **RIS Compute1 Platform**, you should use the second path: **`docker(username/docker-example:latest)`**. This format is required for the platform's job submission system.

For general Docker usage (e.g., building, testing, or pushing images locally), use the first path: **`username/docker-example:latest`**.

**Sources:** Docker, Docker+Tutorial","1
",1,RELION.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/RELION.md
116,"For the PASSWORD environment variable, should I replace 'password' with my own password?","Based on the provided context, there is no mention of a `PASSWORD` environment variable. Therefore, I cannot provide specific guidance on whether to replace 'password' with your own password in this context.

If you are referring to a different context or documentation, please provide more details or clarify your question.

**Sources:** Docker+Wrapper+Environment+Variables, Software+Development+Using+Compute1","1
",1,RELION.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/RELION.md
117,How do I access my data files once the interactive session has started?,"The provided context does not include specific instructions on how to access data files once an interactive session has started. However, typically in such environments, you would access your data files through the following methods:

1. **Network Storage**: If your data is stored on a network drive or shared storage, you can access it using the appropriate path (e.g., `/path/to/your/data`).

2. **Uploading Files**: Some interactive sessions allow you to upload files directly through the interface before starting the session.

3. **Command Line**: If you are in a command-line session, you can use commands like `ls` to list files and `cp` to copy files from your local directory or network storage to your working directory.

4. **Open On Demand (OOD)**: If you are using OOD, there might be options to upload or access files through the web interface.

For specific instructions, you may need to refer to the documentation or support resources provided by your institution or the platform you are using. If you have access to a support portal or help desk, reaching out to them for detailed guidance would be beneficial.

**Sources:** MATLAB, SAS","3
",3,RELION.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/RELION.md
118,How do I install R packages so they are saved permanently and I don't have to reinstall them every time I start a new session?,"To install R packages permanently so you don't have to reinstall them every time you start a new session, follow these steps:

1. **Create a `.Rprofile` file** in your home directory if it doesn't already exist:
   ```bash
   touch ~/.Rprofile
   ```

2. **Create a folder** to store your R packages. For example:
   ```bash
   mkdir /storage1/fs1/${STORAGE_ALLOCATION}/Active/R_libraries/
   ```
   Replace `${STORAGE_ALLOCATION}` with the name of your storage allocation.

3. **Edit the `.Rprofile` file** to specify the library path. Add the following content to the file:
   ```bash
   cat <<EOF > $HOME/.Rprofile
   vals <- paste('/storageN/fs1/${STORAGE_ALLOCATION}/Active/R_libraries/',paste(R.version$major,R.version$minor,sep="".""),sep="""")
   for (devlib in vals) {
   if (!file.exists(devlib))
   dir.create(devlib)
   x <- .libPaths()
   x <- .libPaths(c(devlib,x))
   }
   rm(x,vals)
   EOF
   ```

4. **Install your R packages** using the `install.packages()` function in R. The packages will be installed in the specified directory and will persist across sessions.

This setup ensures that your R packages are stored in a designated directory and are loaded automatically whenever you start RStudio.

**Sources:** R+Tutorial, Rstudio","5
",5,Rstudio.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Rstudio.md
119,"I tried to install a package and it failed because of a missing system dependency, like `libcurl` or `libssl`. How do I fix this?","To fix the issue of missing system dependencies like `libcurl` or `libssl` when installing an R package, you need to install these dependencies using the appropriate package manager for your operating system. Here’s how you can do it:

1. **Identify the Missing Dependencies**: Check the error message to determine which system libraries are missing. Common ones include `libcurl`, `libssl`, and `libxml2`.

2. **Install the Dependencies**:
   - If you are using a Debian-based system (like Ubuntu), you can install these libraries using `apt-get`:
     ```bash
     sudo apt-get update
     sudo apt-get install -y libcurl4-openssl-dev libssl-dev libxml2-dev
     ```
   - If you are using a Red Hat-based system (like CentOS), you can use `yum`:
     ```bash
     sudo yum install -y libcurl-devel openssl-devel libxml2-devel
     ```

3. **Retry Installing the R Package**: After installing the required system dependencies, try installing the R package again. For example:
   ```bash
   R -e ""install.packages('BiocManager', dependencies=TRUE, repos='http://cran.rstudio.com/')""
   R -e ""BiocManager::install('DESeq2')""
   ```

4. **Using Docker**: If you are building a Docker image and encountering these issues, you can include the installation of these dependencies in your Dockerfile. For example:
   ```dockerfile
   FROM r-base:4.0.3

   # Install library dependencies
   RUN apt-get update \
       && apt-get install -y --no-install-recommends libcurl4-openssl-dev libssl-dev libxml2-dev \
       && apt-get clean

   # Install needed R packages
   RUN R -e ""install.packages('BiocManager', dependencies=TRUE, repos='http://cran.rstudio.com/')""
   RUN R -e ""BiocManager::install('DESeq2')""
   ```

By following these steps, you should be able to resolve the missing system dependency issues and successfully install your R package.

**Sources:** Create+Custom+Conda+Environment, R+Tutorial","4
",4,Rstudio.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Rstudio.md
120,"What is the difference between launching RStudio through the ""RStudio"" interactive app versus the ""Custom noVNC Image"" app in Open On Demand?","The difference between launching RStudio through the ""RStudio"" interactive app and the ""Custom noVNC Image"" app in Open On Demand (OOD) is as follows:

1. **RStudio Interactive App**:
   - This is a direct way to start an instance of RStudio.
   - RStudio will start immediately upon launch.

2. **Custom noVNC Image App**:
   - This method involves starting an instance of the Custom noVNC Image in OOD.
   - You need to specify the preferred RStudio image and tag.
   - After launching the session, you must enter `rstudio` in the terminal to start RStudio.

In summary, the ""RStudio"" interactive app provides a more straightforward way to launch RStudio, while the ""Custom noVNC Image"" app offers more flexibility and customization options.

**Sources:** Rstudio, noVNC","5
",5,Rstudio.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Rstudio.md
121,How do I access the pre-downloaded RFdiffusion models for my job?,"To access the pre-downloaded RFdiffusion models for your job, you need to specify the path to the model directory in your configuration. The models are available at the following path:

```
/scratch1/fs1/ris/references/RFdiffusion/models/
```

To use these models, add the above path to the `inference.model_directory_path` configuration in your job script. This ensures that RFdiffusion will use the pre-downloaded models without requiring you to download them again.

**Sources:** AlphaFold, RFdiffusion","5
",5,RFdiffusion.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/RFdiffusion.md
122,What specific fields in the example `bsub` command must I change for my own analysis?,"To customize the example `bsub` command for your own analysis, you should modify the following fields:

1. **`-n`**: Number of CPU slots (e.g., `-n 2`).
2. **`-q`**: Queue name (e.g., `-q foo-condo`).
3. **`-m`**: Host group (e.g., `-m foo-condo`).
4. **`-g`**: Group name (e.g., `-g my_group`).
5. **`-J`**: Job name (e.g., `-J my_name`).
6. **`-M`**: Memory limit (e.g., `-M 8000000`).
7. **`-a`**: Docker image (e.g., `-a 'docker(ubuntu)'`).
8. **`-W`**: Walltime (e.g., `-W 10`).
9. **`-N`**: Notify on job completion (e.g., `-N`).
10. **`-u`**: Email address (e.g., `-u myemail@wustl.edu`).
11. **`-i`**: Input file (e.g., `-i input_file`).
12. **`-oo`**: Output file path (e.g., `-oo ${HOME}/path/to/output_file`).
13. **`-R`**: Resource requirements (e.g., `-R 'select[mem>8000 && tmp>2] rusage[mem=8000, tmp=2, matlab=1] span[hosts=1]'`).
14. **Command**: The actual command or script to run (e.g., `/usr/bin/some_program`).

Adjust these fields according to your specific resource requirements and job details.

**Sources:** Job+Execution+Examples, LSF+to+Slurm+Translator","1
",1,RFdiffusion.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/RFdiffusion.md
123,Why is my job failing if the only thing I've configured is the path to the provided models?,"The context provided does not contain specific information about job failures related to model paths. However, here are some general reasons why your job might be failing:

1. **Incorrect Path Configuration**: Ensure the path to the models is correctly specified and accessible from the compute environment.
2. **Permission Issues**: Verify that the user running the job has the necessary permissions to access the model files.
3. **Missing Dependencies**: The job might require additional dependencies or modules that are not loaded.
4. **Storage Mounting**: If using containers, ensure that storage allocations are properly mounted.
5. **Job Script Errors**: Check the job script for any syntax errors or misconfigurations.

To diagnose the issue further, you can:
- Check the job's output logs for specific error messages.
- Verify the path exists and is accessible using commands like `ls` or `pwd`.
- Ensure all required modules are loaded using the `ml` command.

If you provide more details about the error message or the specific setup, I can offer more targeted assistance.

**Sources:** Compute2+Nextflow, Compute2+Quickstart","3
",3,RFdiffusion.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/RFdiffusion.md
124,How should I choose the number of cores to request with `-n` and does it need to match the number of threads (`-t`)?,"The context provided does not explicitly address the relationship between the number of cores requested with `-n` and the number of threads (`-t`). However, based on general knowledge of Slurm and the options mentioned in the context, here are some guidelines:

1. **Number of Cores (`-n`)**:
   - The `-n` flag specifies the total number of tasks (or processes) you want to run. Each task typically uses one core.
   - For example, `-n 16` requests 16 tasks, which would generally use 16 cores if each task is single-threaded.

2. **Number of Threads (`-t`)**:
   - The context does not mention a `-t` flag for specifying threads. Instead, it mentions options like `--cpus-per-task` or `--threads-per-core`, which are more commonly used to control threading.
   - If you need to specify threads per task, you might use `--cpus-per-task` to indicate how many CPU cores each task should use. For example, `--cpus-per-task=4` would allow each task to use 4 threads (assuming hyper-threading is enabled).

3. **Matching Cores and Threads**:
   - The number of cores (`-n`) does not necessarily need to match the number of threads (`-t` or `--cpus-per-task`). It depends on your workload:
     - If your tasks are single-threaded, `-n` should match the number of cores you need.
     - If your tasks are multi-threaded, you can have fewer tasks (`-n`) but each task can use multiple threads (`--cpus-per-task`).

For more specific information, you might want to refer to the [Slurm documentation on srun](https://slurm.schedmd.com/srun.html) or [sbatch](https://slurm.schedmd.com/sbatch.html) for detailed options related to threading and core allocation.

**Sources:** Batch+Jobs+(sbatch), Interactive+Jobs+(srun)","3
",3,Parallel Tar.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Parallel Tar.md
125,How can I set a custom name for the output archive instead of the default?,"To set a custom name for the output archive using the Parallel Tar tool, you can use the `-n` or `--tar-file-name` option followed by your desired filename (without the `.tar.gz` extension). For example:

```
parallel-tar.py -s path/to/source_dir/ -d path/to/dest_dir/ -n custom_name
```

This will create an archive named `custom_name.tar.gz` in the destination directory.

**Sources:** Parallel+Tar, WashU+ELN+FAQ","5
",5,Parallel Tar.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Parallel Tar.md
126,"After creating the archive, what command do I use to extract its contents?","The provided context does not contain information about extracting contents from an archive. However, based on general knowledge, you can use the following commands depending on your operating system:

- **On Mac/Linux**: Use the `tar` command. For example:
  ```bash
  tar -xvf archive_name.tar
  ```
  If it's a compressed archive (like `.tar.gz` or `.tar.bz2`), use:
  ```bash
  tar -xzvf archive_name.tar.gz
  ```

- **On Windows**: Use tools like WinRAR, 7-Zip, or the built-in extract feature in File Explorer. For command-line extraction with 7-Zip, you can use:
  ```bash
  7z x archive_name.zip
  ```

**Sources:** Recovering+Data+From+Snapshots, Storage+Platforms","1
",1,Parallel Tar.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Parallel Tar.md
127,How do I use my own custom-built TensorFlow image in an interactive session?,"To use your own custom-built TensorFlow image in an interactive session, you can follow these steps:

1. **Build and Push Your Custom Docker Image**: Ensure your custom TensorFlow Docker image is built and pushed to a repository. You can refer to the [Docker Basics: Building, Tagging, & Pushing A Custom Docker Image](https://washu.atlassian.net/wiki/spaces/RUD/pages/1782612257/TensorFlow) guide for detailed instructions.

2. **Interactive GUI Session**:
   - Use the `Custom noVNC Image` application in Open On Demand (OOD).
   - In the Docker Image field, specify your custom Docker image path, e.g., `your-repository/your-tensorflow-image:<tag>`.
   - Fill out the other fields as required (refer to the [Compute1 Quickstart](https://washu.atlassian.net/wiki/spaces/RUD/pages/1782612257/TensorFlow) guide).
   - Select a GPU in the appropriate field since TensorFlow uses GPUs.
   - Launch the job through the methods described in the quick start guide.

3. **Interactive Command-Line Session**:
   - Use the following command to start an interactive session with your custom Docker image:
     ```bash
     bsub -Is -R 'gpuhost' -gpu ""num=1"" -q general-interactive -a 'docker(your-repository/your-tensorflow-image:<tag>)' /bin/bash
     ```

Replace `your-repository/your-tensorflow-image:<tag>` with the actual path and tag of your custom TensorFlow Docker image.

For more details, refer to the [TensorFlow documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1782612257/TensorFlow).

**Sources:** AFNI_TORTOISE, TensorFlow","4
",4,TensorFlow.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/TensorFlow.md
128,Can I install more Python packages temporarily without having to build a new Docker image?,"Based on the provided context, the documentation does not explicitly address installing Python packages temporarily without rebuilding a Docker image. However, you can typically install Python packages temporarily in a running Docker container using the following steps:

1. **Access the running container**:
   ```bash
   docker exec -it <container_name_or_id> /bin/bash
   ```

2. **Install the package using pip**:
   ```bash
   pip install <package_name>
   ```

3. **Use the package as needed**.

Keep in mind that these changes will not persist after the container is stopped or removed. If you need the package to be available permanently, you will need to rebuild the Docker image with the package included.

**Sources:** Docker, Installing+Python+Packages+In+Your+Docker+Container","3
",3,TensorFlow.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/TensorFlow.md
129,What if I need a version of TensorFlow that isn't listed as available?,"If you need a version of TensorFlow that isn't listed as available, you have a couple of options:

1. **Extend the TensorFlow Image**: You can create your own Docker image based on the existing TensorFlow image and add the specific version you need. The documentation provides an example of how to extend the TensorFlow image with additional Python packages. You can modify this example to include the specific TensorFlow version you require.

2. **Request a New Version**: You can reach out to the support team for help with compute resources, including activation or issues. You can follow [this link](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/43) to submit a request for a new version of TensorFlow to be made available.

For more details on extending the TensorFlow image, refer to the [Extend the TensorFlow Image](#extend-the-tensorflow-image) section in the documentation.

**Sources:** AFNI_TORTOISE, TensorFlow","5
",5,TensorFlow.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/TensorFlow.md
130,Why am I getting a `PermissionError` when I try to run a Globus command?,"The `PermissionError` you're encountering when trying to run a Globus command is likely due to the Globus CLI attempting to access or modify a configuration file in your home directory without the necessary permissions. According to the context, this specific error occurs when you try to run `globus login --no-local-server` and the system cannot access or create the `~/.globus.cfg` file.

To resolve this issue, ensure that:
1. You are running the Globus CLI in the same directory where you initially started the Globus CLI job.
2. The environment variables are properly set, especially `LSF_DOCKER_PRESERVE_ENVIRONMENT=false`.
3. You have the necessary permissions to access or create files in your home directory within the container.

If the issue persists, you may need to check the permissions of your home directory or consult the [Moving Data With Globus CLI documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1786576970/Globus+CLI) for more detailed troubleshooting steps.

**Sources:** Globus+CLI, Globus+Connect+Personal","5
",5,Globus CLI.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Globus CLI.md
131,Do I need to keep my interactive job running while my files are transferring?,"No, you do not need to keep your interactive job running while your files are transferring. Once you request a file transfer using Globus, the service takes over and handles the transfer on your behalf. You can navigate away from the File Manager, close the browser window, or even log out. Globus will optimize the transfer for performance, monitor it for completion and correctness, and recover from any network errors or collection downtime. If your credentials expire before the transfer completes, Globus will notify you to re-authenticate, after which it will continue the transfer from where it was paused. This makes data transfer with Globus a ""fire-and-forget"" process.

**Sources:** Job+Execution+Examples, Moving+Data+With+Globus","5
",5,Globus CLI.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Globus CLI.md
132,How do I correctly start an interactive job with this container and ensure my environment variables are set properly?,"To start an interactive job with a container and ensure your environment variables are set properly, follow these steps:

1. **Set the necessary environment variables** before submitting the job. For example, to preserve your environment variables, use:
   ```bash
   export LSF_DOCKER_PRESERVE_ENVIRONMENT=true
   ```

2. **Submit the interactive job** using the `bsub` command with the `-Is` flag and specify the container image. For example:
   ```bash
   bsub -Is -G ${group_name} -q general-interactive -a 'docker(ubuntu)' /bin/bash
   ```

3. **If you need to mount additional volumes** (like your data directories), set the `LSF_DOCKER_VOLUMES` variable:
   ```bash
   export LSF_DOCKER_VOLUMES=""/storageN/fs1/${STORAGE_ALLOCATION}/Active:/storageN/fs1/${STORAGE_ALLOCATION}/Active /scratch1/fs1/${COMPUTE_ALLOCATION}:/scratch1/fs1/${COMPUTE_ALLOCATION}""
   bsub -G ${group_name} -q general-interactive -Is -a 'docker(ubuntu)' /bin/bash
   ```

4. **If you need to set specific environment variables inside the container**, use the `LSF_DOCKER_ENV_FILE` variable:
   ```bash
   LSF_DOCKER_ENV_FILE=""env_file1 env_file2"" bsub -q general -a 'docker(...)' ...
   ```

5. **For host networking** (if required), set:
   ```bash
   export LSF_DOCKER_NETWORK=host
   bsub -Is -G ${group_name} -q general-interactive -a 'docker(ubuntu)' /bin/bash
   ```

By following these steps, you can ensure that your interactive job starts with the correct container and that your environment variables are properly set.

**Sources:** Docker+Wrapper+Environment+Variables, Job+Execution+Examples","2
",2,Globus CLI.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Globus CLI.md
133,How can I load a specific version of MATLAB?,"To load a specific version of MATLAB, you can use the `module avail MATLAB` command to see the available versions. Once you identify the version you want, load it using the `module load MATLAB/<version>` command. For example, if you want to load version 2023a, you would use:

```
module load MATLAB/2023a
```

This will load the specified version of MATLAB for your session.

**Sources:** MATLAB, Stata","5
",5,MATLAB.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/MATLAB.md
134,What is the difference between launching MATLAB via the `Matlab` application versus the `Compute RIS Desktop` application?,"The difference between launching MATLAB via the `Matlab` application and the `Compute RIS Desktop` application is as follows:

- **`Matlab` application**: MATLAB is started up automatically once you launch the job through Open On Demand (OOD). This is a straightforward method where MATLAB is directly initiated without additional steps.

- **`Compute RIS Desktop` application**: You need to manually load and launch MATLAB within the interactive session. This involves using the commands `module load MATLAB` followed by `matlab` after accessing the session. This method provides more flexibility if you need to use other tools or applications alongside MATLAB.

Both methods are accessible through Open On Demand (OOD), but the `Matlab` application is more streamlined for direct MATLAB usage, while the `Compute RIS Desktop` application offers a broader environment for additional tasks.

**Sources:** Compute2+Applications, MATLAB","5
",5,MATLAB.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/MATLAB.md
135,Do I need to know Docker to follow the Quick Start instructions for an interactive session?,"No, you do not need to know Docker to follow the Quick Start instructions for an interactive session with SAS. The instructions for launching an interactive GUI session via Open On Demand (OOD) or an interactive command-line session via the THPC terminal do not require Docker knowledge. The Docker information mentioned in the context is relevant for creating and managing Docker images, which is not necessary for simply using SAS interactively.

**Sources:** Docker+Tutorial, SAS","1
",1,MATLAB.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/MATLAB.md
136,How do I configure Huygens to use my license file and avoid running in freeware mode?,"To configure Huygens to use your license file and avoid running in freeware mode, you need to mount your license folder to a specific path in the Docker container. Follow these steps:

1. **Mount the License Folder**: Ensure your license file is named `huygensLicense` and is located in a folder on your system. You need to mount this folder to `/opt/huygens/huygensLicense` in the Docker container. The mount command should look like this:
   ```
   /path/to/huygens_license_folder/:/opt/huygens/huygensLicense
   ```
   Replace `/path/to/huygens_license_folder/` with the actual path to your license folder.

2. **Manual Login**: According to the SVI support team, manual login is required unless a node-locked license string is properly set up. This functionality does not have a universal solution for the RIS community/cluster.

3. **Run Huygens**: After mounting the license folder, you can run the appropriate Huygens command (e.g., `huygenspro`, `essential`, or `huygensloc`) in your interactive session.

If you do not have a trial license or purchased license available, the software will run in freeware version.

**Sources:** Huygens, Stata","5
",5,Huygens.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Huygens.md
137,How do I access my own image files from within the Huygens application?,"The provided context does not include specific instructions on how to access your own image files from within the Huygens application. However, based on general knowledge of Docker and similar applications, you would typically mount your local directory containing the image files to a directory within the Docker container. This can be done using the `-v` or `--volume` flag when running the Docker container.

For example, you might use a command like this to mount your local `images` directory to `/opt/huygens/images` in the container:

```
bsub -Is -a 'docker(ghcr.io/washu-it-ris/huygens:<tag>)-v /path/to/your/images:/opt/huygens/images' -q general-interactive /bin/bash
```

Once inside the container, you should be able to access your image files in the `/opt/huygens/images` directory. If you need more specific instructions, I recommend checking the Huygens documentation or contacting the support team mentioned in the context.

**Sources:** Huygens, Integrative+Genomics+Viewer","3
",3,Huygens.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Huygens.md
138,Can I run the command-line version (hucore) as a non-interactive batch job?,"Yes, you can run the command-line version of Huygens (`hucore`) as a non-interactive batch job. To do this, you would submit a batch job using the `bsub` command without the interactive flags (`-Is`). Here is an example of how you might structure the command:

```bash
bsub -a 'docker(ghcr.io/washu-it-ris/huygens:<tag>)' -q general-interactive -J hucore_job -o hucore_output.txt hucore
```

Replace `<tag>` with the appropriate version tag for the Huygens Docker image. This command will submit a batch job that runs `hucore` in the specified Docker container and writes the output to `hucore_output.txt`. You can adjust the queue (`-q`), job name (`-J`), and output file (`-o`) as needed.

**Sources:** Huygens, Job+Execution+Examples","3
",3,Huygens.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Huygens.md
139,"My command-line job is failing with a ""file not found"" error. How do I correctly set the `LSF_DOCKER_VOLUMES` variable for my project?","To correctly set the `LSF_DOCKER_VOLUMES` variable for your project and avoid the ""file not found"" error, follow these guidelines based on the context provided:

1. **Format**: Use the format `src:dst` where `src` is the source directory outside the container and `dst` is the destination directory inside the container. For example:
   ```
   LSF_DOCKER_VOLUMES=""/storageN/fs1/${STORAGE_ALLOCATION}:/storageN/fs1/${STORAGE_ALLOCATION} /scratch1/fs1/${COMPUTE_ALLOCATION}:/scratch1/fs1/${COMPUTE_ALLOCATION}""
   ```

2. **Directories Only**: Ensure that the elements in `LSF_DOCKER_VOLUMES` are directories. Docker allows files, but the RIS environment explicitly prevents this due to security vulnerabilities. For example, avoid:
   ```
   LSF_DOCKER_VOLUMES=""$HOME/etc/myfile:/etc/somefile""
   ```

3. **Read-Only Option**: You can mount a volume as read-only by appending `:ro` to the mount. For example:
   ```
   LSF_DOCKER_VOLUMES=""/path/to/source:path/in/container:ro""
   ```

4. **Example Command**: Here is an example of how to set the `LSF_DOCKER_VOLUMES` variable and submit a job:
   ```
   export LSF_DOCKER_VOLUMES=""/storageN/fs1/${STORAGE_ALLOCATION}:/storageN/fs1/${STORAGE_ALLOCATION} /scratch1/fs1/${COMPUTE_ALLOCATION}:/scratch1/fs1/${COMPUTE_ALLOCATION}""
   bsub -Is -G ${group_name} -q general-interactive -a 'docker(ubuntu)' /bin/bash
   ```

5. **Important Notes**:
   - Ensure that the directories you specify are accessible within the container.
   - The `storageN` placeholder can be replaced with `storage1` or `storage2` based on your storage platform.

By following these guidelines, you should be able to correctly set the `LSF_DOCKER_VOLUMES` variable and resolve the ""file not found"" error. If you continue to experience issues, double-check the paths and ensure they are correctly mapped.

**Sources:** Docker+Wrapper+Environment+Variables, Docker+and+the+RIS+Compute1+Platform","2
",2,Nextflow.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Nextflow.md
140,"In the example config file, what do I need to change in the `clusterOptions` line to run jobs using my own compute group?","To run jobs using your own compute group in the example configuration file, you need to modify the `clusterOptions` line to include the `-G` flag followed by your compute group name. Here's how you can do it:

1. Locate the `clusterOptions` line in your configuration file.
2. Add `-G your_compute_group_name` to the options.

For example, if your compute group is named `compute-labname`, the modified line would look like this:

```bash
clusterOptions: ""-G compute-labname -Is -q labname-interactive -a 'docker(alpine)'""
```

This ensures that your jobs are submitted to the correct compute group.

**Sources:** Compute2+Quickstart, Job+Execution+Examples","3
",3,Nextflow.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Nextflow.md
141,"The example config file shows `docker(quay.io/...)`. Can I use a pipeline that pulls its containers from a different repository, like Docker Hub?","Yes, you can use containers from a different repository like Docker Hub instead of Quay.io. The example configuration uses `docker(quay.io/...)`, but you can replace this with any valid Docker image path from Docker Hub or other repositories. For example, you can use `docker(username/repository:tag)` where `username` is your Docker Hub username, `repository` is the name of your repository, and `tag` is the specific version of the image you want to use. The context provided includes instructions on how to build, tag, and push your own Docker images to Docker Hub, which you can then reference in your pipeline.

**Sources:** Docker, Docker+Tutorial","5
",5,Nextflow.md,RIS User Documentation/RIS User Documentation/Applications/Compute1 Applications/RIS Developed Containers/Nextflow.md
142,"Can I store protected or confidential information, like HIPAA data, in my GitHub repository?","No, you should not store protected or confidential information, including HIPAA data, in your GitHub repository. According to the provided context, GitHub is not approved for confidential, HIPAA, or any protected information. This restriction applies to both regular GitHub repositories and GitHub Copilot usage.

**Sources:** Github+Enterprise, WashU+ELN+FAQ","5
",5,Github Enterprise.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/Github Enterprise.md
143,Is there a cost for me or my lab to use GitHub Copilot?,"Based on the provided context, there is no individual billing required for using GitHub Copilot due to the WashU Enterprise Licensing configuration. This means there is no cost for you or your lab to use GitHub Copilot.

**Sources:** Github+Enterprise, Research+Applications+and+Services","5
",5,Github Enterprise.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/Github Enterprise.md
144,"I'm an organization owner, but I don't see the option to enable Copilot for my team. What do I need to do?","To enable GitHub Copilot for your team, you need to request access through the RIS Service Desk. Here are the steps:

1. Go to the [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/10/create/45).
2. Click on ""Request Provisioning or Migration of GitHub Organization.""
3. Provide the required information, including your existing organization name (of which you are an owner).

Once access is granted at the Enterprise level, you will be able to manage Copilot settings for your team. If you still don't see the option after requesting access, you may need to check with your organization's administrator or the RIS support team for further assistance.

**Sources:** Github+Enterprise, Using+Globus+Connect+Personal","4
",4,Github Enterprise.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/Github Enterprise.md
145,What are the prerequisites for using the WashU Omero service?,"To use the WashU OMERO service, you need to meet the following prerequisites:

1. Be a member of the Washington University user community.
2. Have an RIS storage allocation. If you don't have one, you can request it [here](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/53).
3. Have a WashU Key Identity and access to the WashU VPN.
4. Agree to abide by the following WashU policies:
   - [Computer Use Policy](https://informationsecurity.wustl.edu/policies/computer-use-policy)
   - [Information Security Policy](https://informationsecurity.wustl.edu/faculty-staff-students/security-policies-2/information-security-policy)
   - [System Access Control Policy](https://wustl.edu/about/compliance-policies/computers-internet-policies)

Once these prerequisites are met, you can create a ticket for provisioning through the service desk portal to access the WashU OMERO Portal.

**Sources:** Omero, Research+Applications+and+Services","5
",5,Omero.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/Omero.md
146,I've installed the Omero Insight client; how do I configure it to connect to the WashU server?,"To configure the OMERO Insight client to connect to the WashU server, follow these steps:

1. Launch the OMERO Insight client.
2. Click on the configuration lever button.
3. Click on the ""ADD server"" button.
4. Enter `omero.ris.wustl.edu:4064` in the server address dialog box.
5. Click on ""OK"" and then click on ""Apply"".

This will configure the client to use the RIS OMERO server.

**Sources:** Omero, Research+Applications+and+Services","5
",5,Omero.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/Omero.md
147,Who should I contact for support if I have an error in the Omero application versus a problem logging in?,"For errors specific to the OMERO application, you should follow OMERO’s official [support](https://www.openmicroscopy.org/support/).

If you face issues accessing the OMERO portal or logging in, please raise a ticket through the RIS user support [portal](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/53).

**Sources:** Omero, Research+Applications+and+Services","5
",5,Omero.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/Omero.md
148,"Do I use my WashU Key to log in, or do I need to create a separate LabArchives account?","You can use your WashU Key to log in to LabArchives. If you already have a LabArchives account associated with your @wustl.edu email address, you can integrate it with the WashU ELN service to enable WashU Key single sign-on access. This allows you to use your institutional credentials to log in.

**Sources:** LabArchives+Notebook, WashU+ELN+FAQ","4
",4,LabArchives Notebook.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/LabArchives Notebook.md
149,Is there a cost for WashU members to use the LabArchives ELN?,"No, there is no cost for WashU members to use the LabArchives ELN. It is University-funded and available at no cost to the WashU community.

**Sources:** LabArchives+Notebook, WashU+ELN+FAQ","3
",3,LabArchives Notebook.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/LabArchives Notebook.md
150,Who should I contact for technical support if I run into an issue?,"For technical support, you should contact the RIS Service Desk. They handle inquiries related to getting started with services, asking questions about services, and reporting problems. You can access the RIS Service Desk [here](https://servicedesk.ris.wustl.edu/).

**Sources:** FAQ, RIS+Services+Policies","1
",1,LabArchives Notebook.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/LabArchives Notebook.md
151,What happens to my data and notebooks if I leave WashU?,"The provided context does not explicitly address what happens to your data and notebooks if you leave WashU. However, based on general practices and the information available, here are some relevant points:

1. **Data Ownership**: Typically, the data and notebooks you create belong to you, but the terms of service and university policies may apply. You should review the [LabArchives Terms of Use](https://www.labarchives.com/terms-of-use/) and WashU's policies for specific details.

2. **Account Access**: Since WashU ELN is integrated with WashU Key single sign-on, your access to the notebooks may be tied to your WashU credentials. If you leave WashU, you might lose access to the platform unless you have made alternative arrangements.

3. **Data Export**: It's advisable to export or backup your data before leaving the institution. LabArchives may provide options to download or transfer your data, but this should be confirmed with LabArchives Support or WashU's IT services.

For precise information, you should contact [LabArchives Support](https://www.labarchives.com/support/#supportcontact) or WashU's Research Applications and Services for guidance.

**Sources:** LabArchives+Notebook, WashU+ELN+FAQ","4
",4,WashU ELN FAQ.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/LabArchives Notebook/WashU ELN FAQ.md
152,Is there a storage limit for my account or notebooks?,"Based on the provided context, there is indeed a storage limit for your account and notebooks. Here are the key details:

1. **Home Directory Space**: Every Compute Service user is assigned a limit of 9 GiB of home directory space on the Compute Platform. This space is restricted at the user level.

2. **Scratch Space**: High-performance Scratch Space is typically allocated for each lab as it is onboarded to the Compute Service. This space is restricted at the group level, which should represent an eponymous lab.

To check your storage usage, you can use the following commands:

- For home directory usage:
  ```
  mmlsquota -u $(id -nu) --block-size auto rdcw-fs2:home1
  ```

- For scratch space usage (for all compute groups you belong to):
  ```
  groups | grep -Po 'compute-\S+' | while read COMPUTE_GROUP
  do ls -ld ""/scratch1/fs1/${COMPUTE_ALLOCATION}""
  mmlsquota -g ""$COMPUTE_GROUP"" --block-size auto scratch1-fs1
  done
  ```

If you need more information or assistance, you can contact the RIS Service Desk.

**Sources:** FAQ, Space+Management","1
",1,WashU ELN FAQ.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/LabArchives Notebook/WashU ELN FAQ.md
153,Are there specific configurations I must use to ensure my notebook is HIPAA compliant?,"Based on the provided context, LabArchives does support HIPAA data and has the proper security features to be HIPAA compliant. However, the context does not specify any particular configurations you must use to ensure your notebook is HIPAA compliant.

For detailed guidance on configuring your LabArchives notebook to meet HIPAA requirements, you should refer to LabArchives' official documentation or contact their support team directly. You can find more information in the [LabArchives Knowledgebase](https://labarchives.kayako.com/) or by reaching out to [LabArchives Support](https://www.labarchives.com/support/#supportcontact).

**Sources:** LabArchives+Notebook, WashU+ELN+FAQ","4
",4,WashU ELN FAQ.md,RIS User Documentation/RIS User Documentation/Research Applications and Services/LabArchives Notebook/WashU ELN FAQ.md
154,Is there a way for me to connect using my Windows computer?,"Based on the provided context, there is no direct support for connecting to RIS compute services from a Windows computer using the Local VNC Viewer method. The guide specifically mentions that the Windows platform is currently not supported for this purpose.

However, for accessing storage from a Windows computer, you can follow the instructions in the ""Connecting to Storage from Windows"" section. This involves entering the UNC folder path in the Windows File Explorer address bar and using your WashU Key credentials to access the storage drive.

If you need to connect to RIS compute services from a Windows computer, you may need to explore alternative methods or contact the appropriate support team for further assistance.

**Sources:** Connecting+to+Storage+from+Windows, Local+VNC+Viewer","4
",4,Local VNC Viewer.md,RIS User Documentation/RIS User Documentation/Compute1/Local VNC Viewer.md
155,The guide provides installation commands for MacOS. How do I install the required software on Linux?,"The provided context includes installation instructions for MacOS but does not specify the steps for Linux. However, based on general knowledge, here are the typical steps to install the required software on Linux:

1. **Install TurboVNC Viewer**:
   - Download the TurboVNC package for Linux from the [TurboVNC website](https://www.turbovnc.org/).
   - Follow the installation instructions provided for your specific Linux distribution (e.g., using `dpkg` for Debian-based systems or `rpm` for Red Hat-based systems).

2. **Install `expect`**:
   - Open a terminal and run the following command:
     ```bash
     sudo apt-get install expect
     ```
   - For other distributions, use the appropriate package manager (e.g., `yum` for CentOS/RHEL).

3. **Add SSH Private-Public Key Pair**:
   - Follow the guide [SSH Private-Public Key Pair](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705869414/SSH+Private-Public+Key+Pair) to set up your SSH keys if you haven't already.

For more specific instructions tailored to your Linux distribution, you may need to refer to the documentation for your particular version of Linux or contact your system administrator.

**Sources:** Compute2, Compute2+Applications, Local+VNC+Viewer","4
",4,Local VNC Viewer.md,RIS User Documentation/RIS User Documentation/Compute1/Local VNC Viewer.md
156,"I ran the `LaunchDesktop` file, but the terminal window closed immediately without giving me a command to copy. What should I do?","If the terminal window closed immediately after running the `LaunchDesktop` file, it's possible that the script encountered an error or completed its execution without displaying the expected output. Here are some steps you can take to troubleshoot and resolve the issue:

1. **Check the Script for Errors**:
   - Open the `LaunchDesktop` file with a text editor and ensure that your WashU Key is correctly entered in the first line as shown in the example:
     ```bash
     set WUSTLKEY ""WASHU_KEY""
     ```
   - Make sure there are no syntax errors or missing information in the script.

2. **Run the Script Manually**:
   - Open a terminal window and navigate to the directory where the `LaunchDesktop` file is located.
   - Run the script manually by typing `./LaunchDesktop` and press Enter.
   - If the script runs but the terminal closes immediately, try adding `read` at the end of the script to keep the terminal open until you press Enter. This will allow you to see any output or error messages.

3. **Check for Output or Error Messages**:
   - If the script runs but the terminal closes too quickly, you can redirect the output to a file to review it later. Run the following command:
     ```bash
     ./LaunchDesktop > output.txt 2>&1
     ```
   - Open the `output.txt` file to see if there are any error messages or the command you need to copy.

4. **Ensure Proper Permissions**:
   - Make sure the `LaunchDesktop` file has execute permissions. You can set the permissions by running:
     ```bash
     chmod +x LaunchDesktop
     ```

5. **Review the Documentation**:
   - Refer back to the [Local VNC Viewer documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1872986154/Local+VNC+Viewer) for any additional steps or prerequisites that might be necessary.

If you continue to experience issues, you may need to contact WashU IT support for further assistance.

**Sources:** Connecting+to+Storage+from+MacOS, Local+VNC+Viewer","4
",4,Local VNC Viewer.md,RIS User Documentation/RIS User Documentation/Compute1/Local VNC Viewer.md
157,"My container works on my own machine, but on Compute1 it fails with a ""no such file or directory"" error. What's wrong?","The error ""no such file or directory"" on Compute1 typically occurs because the Docker container you're using does not include `/bin/sh`. The IBM Spectrum LSF job scheduler on Compute1 requires that Docker containers have `/bin/sh` present. If your container is based on an image like `hello-world` that doesn't include `/bin/sh`, it will fail with this error.

To resolve this issue, you can either:
1. Use a Docker image that includes `/bin/sh` (e.g., `alpine`, `ubuntu`, or `centos`).
2. Modify your Dockerfile to include `/bin/sh`.
3. Use an LSF variable to bypass this requirement (as mentioned in the [Docker Wrapper Environment Variables](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705115761/Docker+and+the+RIS+Compute1+Platform) documentation).

If you provide more details about your container or Dockerfile, I can offer more specific guidance.

**Sources:** Compute1, Docker+and+the+RIS+Compute1+Platform","5
",5,Docker and the RIS Compute1 Platform.md,RIS User Documentation/RIS User Documentation/Compute1/Docker and the RIS Compute1 Platform.md
158,I'm used to running `docker run` with my own options like `-v` to mount volumes. Why doesn't that work here and what are the limitations?,"When running Docker on the RIS Compute1 Platform, you cannot directly use `docker run` options like `-v` because the platform uses a wrapper script that constructs a ""safe"" `docker run` command with specific security and configuration settings. Here are the key limitations and reasons why your custom options may not work:

1. **Volume Mounting Restrictions**:
   - You can only mount directories, not individual files, due to security vulnerabilities.
   - Volumes must be from GPFS storage platforms like `storage1` or `scratch1`. You cannot access the execution node's local filesystem.
   - Use the `LSF_DOCKER_VOLUMES` environment variable to specify mounts in the format `src:dst` (e.g., `LSF_DOCKER_VOLUMES=""/storage1/fs1/path:/container/path""`).

2. **User and Permissions**:
   - Jobs run as your user (not root) and with restricted capabilities. Pre-built containers expecting specific users may not work as intended.

3. **Required `/bin/sh`**:
   - The platform requires the Docker container to have `/bin/sh` or omit an `ENTRYPOINT`. Containers like `hello-world` (which lack `/bin/sh`) will fail unless you use a workaround.

4. **Networking and Ports**:
   - Ports must be from the approved range (8000-8999) and reserved via the job scheduler. Use `LSF_DOCKER_PORTS` to expose ports (e.g., `LSF_DOCKER_PORTS='8001:80'`).
   - Port forwarding does not work with `--network=host`.

5. **Environment Variables**:
   - By default, most environment variables from your shell are preserved, but some (like `HOSTNAME`) are modified or removed. Use `LSF_DOCKER_PRESERVE_ENVIRONMENT=false` to disable this.

6. **Other Restrictions**:
   - The platform automatically mounts directories like your home directory, job scripts, and `/tmp` (isolated per job).
   - SELinux contexts and OS capabilities are set by the wrapper, and you cannot override them.

For details on available environment variables and their usage, refer to the [Docker Wrapper Environment Variables](https://washu.atlassian.net/wiki/spaces/RUD/pages/1704984661/Docker+Wrapper+Environment+Variables) documentation.

**Sources:** Docker+Wrapper+Environment+Variables, Docker+and+the+RIS+Compute1+Platform","4
",4,Docker and the RIS Compute1 Platform.md,RIS User Documentation/RIS User Documentation/Compute1/Docker and the RIS Compute1 Platform.md
159,How do I use my private Docker Hub repository to pull images for jobs or push new images that I build on the cluster?,"To use your private Docker Hub repository to pull images for jobs or push new images that you build on the RIS Compute1 Platform, follow these steps:

### Pulling Images from a Private Registry
1. **Log in to Docker Hub**:
   - Use the `docker login` command interactively with `LSB_DOCKER_LOGIN_ONLY` enabled. This will store your credentials in `$HOME/.docker/config.json`.
   - Example for Docker Hub:
     ```bash
     LSB_DOCKER_LOGIN_ONLY=1 \
     bsub -G ${group_name} -q general-interactive -Is -a 'docker_build' -- .
     ```
   - For other repositories, specify the server:
     ```bash
     LSB_DOCKER_LOGIN_ONLY=1 \
     LSB_DOCKER_LOGIN_SERVER=repo.example.com \
     bsub -G ${group_name} -q general-interactive -Is -a 'docker_build' -- .
     ```

2. **Submit a Job to Pull the Image**:
   - Use the `bsub` command with the `docker` application to pull and run the image:
     ```bash
     bsub -G ${group_name} -Is -q general-interactive -a 'docker(your_dockerhub_username/private_image_name:tag)' command
     ```

### Pushing New Images to a Private Registry
1. **Log in to Docker Hub** (as described above).

2. **Build and Push the Image**:
   - Use the `docker_build` LSF application to build and push the image to your private registry. Specify the tags and the build context directory.
   - Example:
     ```bash
     bsub -G ${group_name} -q general-interactive -Is -a 'docker_build(your_dockerhub_username/private_image_name:tag)' -- --tag your_dockerhub_username/private_image_name:tag build_context_directory
     ```
   - Ensure the `--` separates `bsub` arguments from `docker build` arguments, and the build context directory is the final argument.

### Important Notes
- The Docker images must include `/bin/sh` to avoid errors during execution.
- The wrapper script constructs the `docker run` command, so some Docker features may be mitigated or disallowed for security reasons.
- For more details, refer to the [Docker Wrapper Environment Variables](Docker%20Wrapper%20Environment%20Variables.md) documentation.

**Sources:** Docker+Basics_+Building,+Tagging,+&+Pushing+A+Custom+Docker+Image, Docker+and+the+RIS+Compute1+Platform","5
",5,Docker and the RIS Compute1 Platform.md,RIS User Documentation/RIS User Documentation/Compute1/Docker and the RIS Compute1 Platform.md
160,I only want to use the web-based Open On Demand (OOD). Do I still need to connect via SSH first?,"Yes, you still need to connect via SSH at least once before using Open OnDemand (OOD). This initial SSH connection is required to properly set up your user home directory on the compute platform. Once you have done this, you can access OOD by pointing your browser to [http://ood.ris.wustl.edu](http://ood.ris.wustl.edu).

**Sources:** Compute1+Quickstart, Open+OnDemand","5
",5,Compute1 Quickstart.md,RIS User Documentation/RIS User Documentation/Compute1/Compute1 Quickstart.md
161,"I'm trying to start an application in OOD, but I'm confused by the ""Mounts"" field. How do I find my storage allocation name and which `storageN` platform should I use?","To find your storage allocation name and determine which `storageN` platform to use in the OOD ""Mounts"" field, follow these steps:

1. **Identify Your Storage Allocation**:
   - Your storage allocation name is typically provided by your organization or IT department. It often follows a naming convention like `storage-username` or `storage-groupname`. For example, in the context, you see names like `storage-cpohl-rw`, `storage-shin-rw`, etc.

2. **Determine the Storage Platform**:
   - The context mentions two primary storage platforms: `storage1` and `storage2`. The platform you use depends on your specific allocation and setup.
   - If you are unsure which platform your allocation is on, check with your IT support or refer to any documentation provided to you.

3. **Accessing Storage**:
   - For `storage2`, you can access your allocation via the SMB protocol by mounting `//storage2.ris.wustl.edu/your-allocation-name`.
   - From a Compute1 node, you can access it at `/storage2/fs1/your-allocation-name`.

4. **Viewing ACLs (Optional)**:
   - If you need to view the access control list (ACL) for your allocation on `storage2`, you can use the `nfs4_getfacl` command for NFS or `smbclient` for SMB as described in the context.

If you still have trouble determining your allocation name or the correct platform, reach out to your IT support or the RIS Service Desk for assistance.

**Sources:** Space+Management, Storage2+Access+Control","2
",2,Compute1 Quickstart.md,RIS User Documentation/RIS User Documentation/Compute1/Compute1 Quickstart.md
162,The application I need isn't listed on the Open On Demand main page. How can I see a full list of all available software?,"To see a full list of all available software on Open OnDemand, you can refer to the [THPC Installed Applications](https://washu.atlassian.net/wiki/spaces/RUD/pages/1696039041/THPC+Installed+Applications) page. This page contains a comprehensive list of applications installed to persistent storage and accessed via Docker containers. If you don't find the application you need, you can request it through the [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/55).

**Sources:** Open+OnDemand, THPC+Installed+Applications","2
",2,Compute1 Quickstart.md,RIS User Documentation/RIS User Documentation/Compute1/Compute1 Quickstart.md
163,I'm getting an error when I try to log into Open OnDemand for the first time. What is the most common reason for this?,"The most common reason for encountering an error when trying to log into Open OnDemand for the first time is that you haven't connected to the compute platform via SSH at least once before. This initial SSH connection is necessary to properly set up your user home directory.

To resolve this, follow these steps:
1. Connect to the compute platform via SSH using your WashU Key and password. The command is:
   ```
   ssh washukey@compute1-client-1.ris.wustl.edu
   ```
2. Once you've successfully connected via SSH, you should be able to log into Open OnDemand by pointing your browser to [http://ood.ris.wustl.edu](http://ood.ris.wustl.edu).

**Sources:** Compute1+Quickstart, Open+OnDemand","5
",5,Open OnDemand.md,RIS User Documentation/RIS User Documentation/Compute1/Open OnDemand.md
164,The 'Files' app only shows my home directory. How do I access my data on storage1 or scratch1?,"To access your data on `storage1` or `scratch1`, you need to use the appropriate paths in the terminal or command line interface, as these storage locations are not typically visible in the 'Files' app by default. Here’s how you can access them:

1. **For `storage1`:**
   - The cached view of `storage1` is available at `/storage1/fs1/${STORAGE_ALLOCATION}`.
   - You can access this path directly in the terminal. For example:
     ```bash
     cd /storage1/fs1/your_storage_allocation
     ```

2. **For `scratch1`:**
   - The scratch space is available at `/scratch1/fs1/${COMPUTE_ALLOCATION}`.
   - You can access this path directly in the terminal. For example:
     ```bash
     cd /scratch1/fs1/your_compute_allocation
     ```

If you need to mount these paths in a Docker container, you can use the `LSF_DOCKER_VOLUMES` environment variable. For example:
```bash
export LSF_DOCKER_VOLUMES=""/storage1/fs1/your_storage_allocation:/storage1/fs1/your_storage_allocation /scratch1/fs1/your_compute_allocation:/scratch1/fs1/your_compute_allocation""
```

Replace `your_storage_allocation` and `your_compute_allocation` with your actual allocation names. This will make the storage volumes accessible within your Docker container.

**Sources:** Access+Storage+Volumes, Recovering+Data+From+Snapshots","1
",1,Open OnDemand.md,RIS User Documentation/RIS User Documentation/Compute1/Open OnDemand.md
165,"My interactive session has been in the ""Queued"" status for a long time. Is this normal?","It's not uncommon for interactive sessions to remain in the ""Queued"" status for some time, especially if the system is busy or if there are resource constraints. Here are a few possible reasons and suggestions based on the context provided:

1. **Resource Availability**: The system may not have enough resources (CPU, GPU, memory) available to start your interactive session immediately. You can check the host status and resource availability using commands like `bhosts` or `lsid`.

2. **Queue Priority**: If you're using a subscription tier queue, ensure you've specified the correct `-sla` option. Without the correct SLA, your job might not be prioritized correctly.

3. **Job Group Limits**: If you're part of a job group, you might have reached the limit of concurrent jobs allowed for your group. You can check your job group status with `bjgroup -s /${compute_username}/${group_name}`.

4. **General Queue**: If you're using the general queue, there are no guaranteed resources, so your job might wait until resources become available.

To troubleshoot, you can:
- Check the status of your job with `bjobs -l <job_id>`.
- Check the queue status with `bqueues`.
- If you need immediate access, consider requesting fewer resources or checking if there are less busy queues available.

If the delay is unusually long, you might want to contact your system administrator for further assistance.

**Sources:** Compute1+Condo+and+Subscription+Queues, Job+Execution+Examples","2
",2,Open OnDemand.md,RIS User Documentation/RIS User Documentation/Compute1/Open OnDemand.md
166,My job can't find my data files in /storage1 or /scratch1. How do I make them available to my job?,"To make your data files available to your job in `/storage1` or `/scratch1`, you need to specify the storage volumes using the `LSF_DOCKER_VOLUMES` environment variable. This variable allows you to mount the required directories into your job's container.

Here’s how you can do it:

1. **Set the Environment Variables:**
   Define the paths to your storage and scratch directories. For example:
   ```bash
   export SCRATCH1=/scratch1/fs1/${COMPUTE_ALLOCATION}
   export STORAGEN=/storageN/fs1/${STORAGE_ALLOCATION}/Active
   ```

2. **Specify the Volumes:**
   Use the `LSF_DOCKER_VOLUMES` variable to mount these directories into your job. For example:
   ```bash
   export LSF_DOCKER_VOLUMES=""$HOME:$HOME $STORAGEN:$STORAGEN $SCRATCH1:$SCRATCH1""
   ```

3. **Submit Your Job:**
   When submitting your job, ensure that the `LSF_DOCKER_VOLUMES` variable is set. For example:
   ```bash
   bsub -G ${group_name} -Is -q general-interactive -a 'docker(ubuntu)' /bin/bash
   ```

This will make the specified directories (`/storage1` and `/scratch1`) available to your job. If you encounter any issues, ensure that the paths are correct and that you have the necessary permissions to access these directories.

**Sources:** Access+Storage+Volumes, FAQ","4
",4,Access Storage Volumes.md,RIS User Documentation/RIS User Documentation/Compute1/Access Storage Volumes.md
167,"I just finished a job that wrote files to my /storage1 directory, but I don't see the new files when I connect from my computer. Where are they?","If you've written files to your `/storage1` directory but don't see them when connecting from your computer, there are a few possible reasons and solutions:

1. **Cache Issues**: Your computer might be displaying a cached version of the directory. Try refreshing the view in your file explorer (e.g., pressing F5 in Windows or Command + R in Mac).

2. **Connection Issues**: Ensure you have a stable network connection to the storage platform. If the connection drops, the files might not appear until you reconnect.

3. **Permissions**: Verify that you have the necessary permissions to access the files. If you don't have read permissions, the files won't be visible.

4. **Sync Delay**: Sometimes, there might be a delay in syncing files across the network. Wait a few moments and check again.

5. **Path Verification**: Double-check that you are connecting to the correct path (`\\storage1.ris.wustl.edu\PI WashU Key or Share Name` for Windows or `smb://storage1.ris.wustl.edu/PI WashU Key ID` for Mac).

If none of these solutions work, you might want to contact your IT support for further assistance.

**Sources:** Connecting+to+Storage+from+MacOS, Connecting+to+Storage+from+Windows","1
",1,Access Storage Volumes.md,RIS User Documentation/RIS User Documentation/Compute1/Access Storage Volumes.md
168,"What is the difference between scratch space and storage space, and when should I use each one?","The difference between scratch space and storage space in the RIS Compute Platform is as follows:

- **Scratch Space**:
  - Located at `/scratch1/fs1/${COMPUTE_ALLOCATION}`.
  - Considered temporary and manually managed up to a quota.
  - Best for short-term, high-performance storage needs.
  - Data is automatically deleted if not modified for 28 days.
  - Quotas are set based on compute-group membership.

- **Storage Space**:
  - Located at `/storage1/fs1/${STORAGE_ALLOCATION}` (a cached view of the storage platform).
  - Data is continuously synced between the cache and the storage cluster.
  - Designed for longer-term storage and regular data access.
  - Not manually managed, but subject to soft and hard quotas.

### When to Use Each:
- **Use Scratch Space** for temporary files, intermediate results, or data that doesn't need long-term retention. It offers faster performance but is not a reliable place for permanent storage.
- **Use Storage Space** for data that needs to be retained long-term or accessed regularly. It provides a more stable storage solution, though with potential latency due to caching.

For more details, refer to the [RIS User Documentation on Access Storage Volumes](https://washu.atlassian.net/wiki/spaces/RUD/pages/1794965573/Access+Storage+Volumes).

**Sources:** Access+Storage+Volumes, Space+Management","4
",4,Access Storage Volumes.md,RIS User Documentation/RIS User Documentation/Compute1/Access Storage Volumes.md
169,What should I do if my software requires a different OFED version than the one shown in the example?,"If your software requires a different OFED version than the one shown in the example, you should modify the `MOFED_VERSION` environment variable in your Dockerfile to match the required version. The example provided in the documentation shows how to install OFED 5.8-4.1.5.0 for both RedHat 8.9 and Ubuntu 22.04. You can follow the same structure but replace the version number with the one your software needs.

For example, if your software requires OFED version 6.0-1.0.0.0, your Dockerfile instructions for RedHat 8.9 would look like this:

```dockerfile
ENV MOFED_VERSION 6.0-1.0.0.0
ENV OS_VERSION rhel8.9
ENV PLATFORM x86_64
RUN cd /tmp/ && yum install -y pciutils numactl-libs gtk2 atk cairo gcc-gfortran tcsh lsof libnl3 libmnl ethtool tcl tk perl make libusbx fuse-libs && \
    wget -q http://content.mellanox.com/ofed/MLNX_OFED-${MOFED_VERSION}/MLNX_OFED_LINUX-${MOFED_VERSION}-${OS_VERSION}-${PLATFORM}.tgz && \
    tar -xvf MLNX_OFED_LINUX-${MOFED_VERSION}-${OS_VERSION}-${PLATFORM}.tgz && \
    MLNX_OFED_LINUX-${MOFED_VERSION}-${OS_VERSION}-${PLATFORM}/mlnxofedinstall --user-space-only --without-fw-update -q  --distro rhel8.9 && \
    cd .. && \
    rm -rf ${MOFED_DIR} && \
    rm -rf *.tgz && \
    yum clean all
```

Similarly, for Ubuntu 22.04, you would adjust the version in the corresponding Dockerfile instructions. Once you have the correct OFED version installation code in your Dockerfile, you can build and push the image as you normally would.

**Sources:** Compute1+Quickstart, Using+OFED+in+Docker+Images","5
",5,Using OFED in Docker Images.md,RIS User Documentation/RIS User Documentation/Compute1/Using OFED in Docker Images.md
170,Are there pre-built base images available that already include the correct OFED drivers?,"Based on the provided context, there are no explicit mentions of pre-built base images that already include the correct OFED drivers. However, the documentation does provide instructions on how to install the correct version of OFED in Docker images for both RedHat and Ubuntu systems.

If you are looking for pre-built images, you may need to check the Docker Hub or other repositories for images that include the OFED drivers. Alternatively, you can follow the instructions provided in the context to create your own Docker image with the correct OFED drivers.

**Sources:** Intel®+Compiler+Base, Using+OFED+in+Docker+Images","5
",5,Using OFED in Docker Images.md,RIS User Documentation/RIS User Documentation/Compute1/Using OFED in Docker Images.md
171,The examples are for RedHat 8.9 and Ubuntu 22.04. What instructions should I use for a different Linux distribution?,"The provided context does not include specific instructions for different Linux distributions other than RedHat 8.9 and Ubuntu 22.04. However, the general approach for using Docker images and running R or other applications on different Linux distributions would involve the following steps:

1. **Identify the Base OS of the Docker Image**: Check the base OS of the Docker image you are using. For example, the `r-base` images use Debian.

2. **Install Required Dependencies**: If you encounter dependency errors, you will need to install the required libraries for your specific Linux distribution. For example:
   - On Debian/Ubuntu: Use `apt-get`.
   - On RedHat/CentOS: Use `yum` or `dnf`.
   - On Arch Linux: Use `pacman`.
   - On Fedora: Use `dnf`.

3. **Modify the Dockerfile**: Adjust the Dockerfile to include the appropriate commands for installing dependencies on your specific Linux distribution.

4. **Build and Run the Docker Image**: Use the standard Docker commands to build and run the image on your system.

If you provide the specific Linux distribution you are using, I can offer more tailored instructions.

**Sources:** Non-RIS+Docker+Images, R+Tutorial","2
",2,Using OFED in Docker Images.md,RIS User Documentation/RIS User Documentation/Compute1/Using OFED in Docker Images.md
172,Can I follow this process for other programming languages like Python or R?,"Yes, you can follow a similar process for other programming languages like Python or R. The context provided outlines steps for setting up and using Docker images for R, but the same principles apply to other languages. Here’s a general approach:

1. **Find a Base Image**: Locate a suitable Docker image for your programming language (e.g., `python:3.9` for Python or `r-base` for R).

2. **Create a Dockerfile**: Write a Dockerfile to install necessary packages and dependencies. For Python, you might use `pip` to install packages, while for R, you might use `install.packages` or `BiocManager`.

3. **Build the Image**: Use the Dockerfile to build your custom image. For example:
   ```bash
   docker build -t yourusername/yourimage:tag .
   ```

4. **Push to a Registry**: Push your image to a Docker registry like Docker Hub so it can be accessed from the RIS Compute Platform.

5. **Run the Image**: Use the RIS Compute Platform to run your custom Docker image. For example:
   ```bash
   bsub -Is -G yourgroup -q yourqueue -a 'docker(yourusername/yourimage:tag)' /bin/bash
   ```

The context provides detailed steps for R, but the same workflow can be adapted for Python or other languages by adjusting the package managers and dependencies accordingly.

**Sources:** Non-RIS+Docker+Images, R+Tutorial","3
",3,Software Development Using Compute1.md,RIS User Documentation/RIS User Documentation/Compute1/Software Development Using Compute1.md
173,The document says not to put `LSF_DOCKER_VOLUMES` in my `.bash_profile`. What is the recommended way to define it for my jobs?,"The recommended way to define `LSF_DOCKER_VOLUMES` for your jobs is to set it directly in the command line when submitting your job using `bsub`, rather than placing it in your `.bash_profile`. This ensures that the environment variable is only applied to the specific job and not globally to your shell environment.

Here’s an example of how to set it:

```bash
export LSF_DOCKER_VOLUMES=""/storageN/fs1/${STORAGE_ALLOCATION}:/storageN/fs1/${STORAGE_ALLOCATION} /scratch1/fs1/${COMPUTE_ALLOCATION}:/scratch1/fs1/${COMPUTE_ALLOCATION}""
bsub -Is -G ${group_name} -q general-interactive -a 'docker(ubuntu)' /bin/bash
```

This approach keeps the environment variable scoped to the job submission, avoiding potential conflicts or unintended side effects in your shell environment.

**Sources:** Docker+Wrapper+Environment+Variables, Docker+and+the+RIS+Compute1+Platform","5
",5,Software Development Using Compute1.md,RIS User Documentation/RIS User Documentation/Compute1/Software Development Using Compute1.md
174,How do I modify the Dockerfile to install my own software dependencies instead of just `git`?,"To modify the Dockerfile to install your own software dependencies instead of just `git`, you can follow these steps based on the provided context:

1. **Choose a Base Image**: Start with a base image that suits your needs. For example, you can use `debian:stretch-slim` as shown in the context.

2. **Install Dependencies**: Use the `apt-get` package manager to install your desired software dependencies. Here’s an example of how to do it:

```dockerfile
FROM debian:stretch-slim

RUN apt-get update \
    && apt-get install -y --no-install-recommends r-base r-base-dev git python3 your-software-dependency1 your-software-dependency2 \
    && apt-get clean
```

Replace `your-software-dependency1` and `your-software-dependency2` with the actual package names of the software you want to install.

3. **Add Additional Commands**: If your software requires additional setup commands, you can add them after the installation commands. For example:

```dockerfile
RUN your-setup-command
```

4. **Build and Tag the Image**: After modifying the Dockerfile, build and tag your image using the following command:

```bash
docker build -t your_dockerhub_username/image_name:tag .
```

5. **Run and Test the Image**: Test your image locally before pushing it to Docker Hub:

```bash
docker run -it your_dockerhub_username/image_name:optional_tag command /file_path/file
```

6. **Push to Docker Hub**: Once you’ve verified that your image works correctly, push it to Docker Hub:

```bash
docker push your_dockerhub_username/image_name:optional_tag
```

By following these steps, you can customize the Dockerfile to install and configure your own software dependencies.

**Sources:** Docker+Basics_+Building,+Tagging,+&+Pushing+A+Custom+Docker+Image, Docker+Tutorial","3
",3,Software Development Using Compute1.md,RIS User Documentation/RIS User Documentation/Compute1/Software Development Using Compute1.md
175,Why did my job fail with an error that my user group does not match the queue type?,"Your job failed because the user group specified in your job submission does not match the type of queue you are trying to use. According to the context, the group name and queue must match for the job to be submitted successfully. For example:

- If you are using a **condo queue**, you must specify the correct **condo group name** (`${compute-group}`).
- If you are using a **subscription queue**, you must specify the correct **subscription group name** (`${compute-subscription-group}`).

The error occurs when there is a mismatch between these values. For instance, if you submitted a job to a condo queue but used a subscription group name, the system will reject the job with the error message you encountered.

To resolve this issue, ensure that the group name (`-G`) and queue name (`-q`) in your job submission command match the type of queue you are using.

**Sources:** Compute1+Condo+and+Subscription+Queues, RIS+Services+Policies","5
",5,Compute1 Condo and Subscription Queues.md,RIS User Documentation/RIS User Documentation/Compute1/Compute1 Condo and Subscription Queues.md
176,"How do I find the correct group name, condo name, or SLA name for my account?","To find the correct group name, condo name, or SLA name for your account, you should refer to the information provided to you at the time of activation for your group. Here’s how you can identify each:

1. **Group Name (`${compute-group}` or `${compute-subscription-group}`)**:
   - This is typically based on your lab or research group's designation, often using a PI's WashU Key ID (e.g., `compute-ris` for the RIS group).
   - You should have been provided with this name when your compute access was activated.

2. **Condo Name (`${condo-name}`)**:
   - This is the name of the specific condo queue associated with your purchased condo.
   - This information is provided when your condo is set up and activated.

3. **SLA Name (`${sla-name}`)**:
   - This is the name associated with your subscription tier (Tier 1, Tier 2, or Tier 3).
   - You can check the SLA used by a job with the command `bjobs -al ${job-id}` and look for the `Service Class` entry in the output.
   - This name is provided when your subscription is activated.

If you do not have this information, you should contact your group's administrator or the RIS support team for assistance.

**Sources:** Compute1+Condo+and+Subscription+Queues, Storage1+Access+Control","5
",5,Compute1 Condo and Subscription Queues.md,RIS User Documentation/RIS User Documentation/Compute1/Compute1 Condo and Subscription Queues.md
177,What happens if I submit a job to the subscription queue but request more CPUs or GPUs than my tier provides?,"If you submit a job to the subscription queue but request more CPUs or GPUs than your tier provides, the following will happen:

- If you exceed the guaranteed number of vCPUs for your tier, your job will not be guaranteed to run.
- If you exceed the guaranteed number of GPUs for your tier, the job will stay in pending and will not run.

This is because the subscription tiers have specific resource allocations, and jobs that exceed these allocations are either not prioritized or remain pending.

**Sources:** Compute1+Condo+and+Subscription+Queues, Compute2+General+Guidelines","5
",5,Compute1 Condo and Subscription Queues.md,RIS User Documentation/RIS User Documentation/Compute1/Compute1 Condo and Subscription Queues.md
178,What happens if I request more than the suggested 10 GPUs?,"If you request more than the suggested 10 GPUs, you may encounter limitations or restrictions imposed by the system administrators to ensure fair resource allocation across all users. While the context does not specify exact consequences, it is likely that such requests could be denied or delayed to prevent over-allocation of GPU resources. It is advisable to adhere to the suggested limit of 10 GPUs per user to avoid potential issues.

**Sources:** Parabricks, Parallel+Computing","4
",4,Parallel Computing.md,RIS User Documentation/RIS User Documentation/Compute1/Parallel Computing.md
179,"For MPI jobs, what happens if I forget to set the `LSF_DOCKER_NETWORK` and `LSF_DOCKER_IPC` environment variables?","If you forget to set the `LSF_DOCKER_NETWORK` and `LSF_DOCKER_IPC` environment variables for MPI jobs, the communication between parallel processes may fail. These variables are crucial for enabling host networking and IPC (Inter-Process Communication) modes, which facilitate communication between MPI processes running on the same or different hosts. Without them, the MPI application may not function correctly or may not be able to coordinate between processes.

**Sources:** Docker+Wrapper+Environment+Variables, Parallel+Computing","5
",5,Parallel Computing.md,RIS User Documentation/RIS User Documentation/Compute1/Parallel Computing.md
180,How can I find out what types of GPUs are available in the cluster to use with the `gmodel` option?,"To find out what types of GPUs are available in the cluster for use with the `gmodel` option, you can use the `sinfo` command. This command provides information about the available partitions and nodes, including those with GPUs.

Here’s how you can do it:

1. **Run the `sinfo` command**:
   ```bash
   sinfo
   ```

2. **Look for the GPU partition**:
   In the output, identify the partition labeled `gpu`. This partition will list the available GPU nodes and their specifications.

3. **Check the GPU model**:
   The output will show details about the GPU nodes, including the model of the GPUs (e.g., `Tesla V100-SXM2-32GB`). You can use this information to specify the `gmodel` option in your job submission.

For example, if the output shows:
```
gpu              up   infinite     94   idle c2-gpu-[001-009,011,013-016],c2-node-[001-080]
```
You can then use the `gmodel` option in your job submission like this:
```bash
bsub -Is -q general-interactive -R 'gpuhost' -gpu ""num=4:gmodel=TeslaV100_SXM2_32GB"" -a 'docker(tensorflow/tensorflow:latest-gpu)' /bin/bash -c ""CUDA_VISIBLE_DEVICES=0,1,2,3; python tf2.py""
```

**Sources:** Compute2+Quickstart, Parallel+Computing","1
",1,Parallel Computing.md,RIS User Documentation/RIS User Documentation/Compute1/Parallel Computing.md
181,I tried building a Docker image with a different R package and it failed. How do I figure out which system libraries (`apt-get install`) it needs?,"When building a Docker image with an R package and encountering errors, you can determine the required system libraries by examining the error messages. Here’s how to troubleshoot and identify the necessary libraries:

1. **Check Error Messages**: Look at the error output during the package installation. Errors often indicate missing system libraries or dependencies.

2. **Identify Missing Libraries**: The error messages typically specify which libraries are missing. For example, if you see an error related to `curl`, you might need to install `libcurl4-openssl-dev`.

3. **Install Required Libraries**: Use `apt-get` to install the identified libraries. For example:
   ```dockerfile
   RUN apt-get update \
       && apt-get install -y --no-install-recommends libcurl4-openssl-dev libssl-dev libxml2-dev \
       && apt-get clean
   ```

4. **Rebuild the Image**: After adding the necessary libraries, rebuild the Docker image to see if the errors are resolved.

For example, if you are installing `DESeq2` and encounter errors, you might need to install `libcurl4-openssl-dev`, `libssl-dev`, and `libxml2-dev` as shown in the context.

If you provide the specific error messages you encountered, I can help you identify the required libraries more precisely.

**Sources:** Docker+Tutorial, R+Tutorial","5
",5,R Tutorial.md,RIS User Documentation/RIS User Documentation/Compute1/R Tutorial.md
182,The tutorial shows how to create phenotype data from scratch. How do I load my own existing metadata file for the DESeq2 analysis?,"To load your own existing metadata file for DESeq2 analysis instead of creating phenotype data from scratch, follow these steps:

1. **Prepare Your Metadata File**: Ensure your metadata file is in a tab-separated or comma-separated format with sample names as row names and relevant columns for your experimental design (e.g., group, treatment, disease, weight).

2. **Load the Metadata File in R**:
   ```R
   pheno.data <- read.delim(""path/to/your/metadata_file.txt"", header = TRUE, sep = ""\t"")
   ```
   Replace `""path/to/your/metadata_file.txt""` with the actual path to your metadata file.

3. **Ensure Sample Names Match**: Make sure the sample names in your metadata file match the column names in your expression data (`data`).

4. **Proceed with DESeq2 Analysis**:
   ```R
   library(DESeq2)
   deseq2.data <- DESeqDataSetFromMatrix(countData = data, colData = pheno.data, design = ~ group + treatment + disease + weight)
   dds <- DESeq(deseq2.data)
   ```

This approach allows you to use your existing metadata file directly in the DESeq2 workflow.

**Sources:** R+Tutorial","5
",5,R Tutorial.md,RIS User Documentation/RIS User Documentation/Compute1/R Tutorial.md
183,Can I use the custom Docker image I built with my specific packages in an RStudio session instead of from the command line?,"Yes, you can use your custom Docker image with specific packages in an RStudio session. To do this, you will need to start an instance of the ""Custom noVNC Image"" application in Open On Demand (OOD) and specify your custom Docker image and tag. Once the session is launched, you can start RStudio by entering the command `rstudio` in the terminal. This allows you to use your custom Docker image within an interactive RStudio GUI session.

**Sources:** Docker, Rstudio","1
",1,R Tutorial.md,RIS User Documentation/RIS User Documentation/Compute1/R Tutorial.md
184,My job is stuck in a pending state. Where in RTM can I find the reason why?,"To find the reason why your job is stuck in a pending state, you should check the **JobIQ Dashboard** in RTM. This dashboard provides detailed information about your current and recent jobs, including the **Pending Reasons by Queue for My User** section, which gives a summary of all jobs with the Pending status and the reason for the status.

You can access the JobIQ Dashboard at the following link:
<https://compute1-rtm-1.ris.wustl.edu/cacti/index.php>

**Sources:** FAQ, RTM","5
",5,RTM.md,RIS User Documentation/RIS User Documentation/Compute1/RTM.md
185,"What is the difference between 'Max Slots', 'Num Slots', and 'Run Slots' on the Cluster Dashboard?","On the Cluster Dashboard, the terms ""Max Slots,"" ""Num Slots,"" and ""Run Slots"" refer to different aspects of job slots on an execution node (host):

- **Max Slots**: This is the maximum number of job slots available on the execution node (host).
- **Num Slots**: This is the number of job slots currently in use on the execution node (host).
- **Run Slots**: This is the number of job slots that have a running job on the execution node (host).

These metrics help monitor the utilization and status of job slots on each execution node.

**Sources:** Compute2+General+Guidelines, Monitoring+Jobs+and+Partitions_Queues, RTM","5
",5,RTM.md,RIS User Documentation/RIS User Documentation/Compute1/RTM.md
186,I don't see my specific software license on the License Dashboard. How can I check if it's available?,"If you don't see your specific software license on the License Dashboard, it might not be listed among the RIS-supported licenses. The License Dashboard only shows the licenses that are available and supported by RIS. If your license is managed through other methods or is not supported by RIS, it won't appear on this dashboard.

For further assistance, you can contact the RIS support team through the [RIS Services Portal](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/43) to inquire about your specific software license.

**Sources:** RTM, Stata","4
",4,RTM.md,RIS User Documentation/RIS User Documentation/Compute1/RTM.md
187,"How do I request specific resources for my job, like more memory or a GPU?","To request specific resources for your job, such as more memory or a GPU, you can use the `bsub` command with appropriate options. Here’s how you can do it:

### **Memory Resources**
- The default memory reservation is 4GB. To specify a custom memory reservation, use the `rusage` expression and the `-M` option:
  ```bash
  bsub -G ${group_name} -q foo-condo -R 'rusage[mem=8GB]' -M 6GB -a 'docker(alpine)' /bin/true
  ```
  - This reserves 8GB of memory, but the job will be killed if it uses more than 6GB.
  - If `-M` is omitted, it defaults to the reservation amount.

### **GPU Resources**
- To use a GPU, include the `-gpu` flag and the `-R 'gpuhost'` option:
  ```bash
  bsub -q foo-condo -R 'gpuhost' -gpu ""num=1:gmodel=TeslaV100_SXM2_32GB"" -a 'docker(alpine)' /bin/true
  ```
  - `gmodel` specifies the GPU model (e.g., `TeslaV100_SXM2_32GB`).
  - `num` specifies the number of GPUs required.
  - You can also reserve GPU memory with `gmem`:
    ```bash
    bsub -q foo-condo -R 'gpuhost' -gpu ""num=1:gmem=8G"" -a 'docker(alpine)' /bin/true
    ```

### **CPU Resources**
- To specify a CPU architecture, use the `rusage` expression:
  ```bash
  bsub -q foo-condo -R 'select[model==Intel_Xeon_Gold6242CPU280GHz]' -a 'docker(alpine)' /bin/true
  ```
  - Replace `Intel_Xeon_Gold6242CPU280GHz` with the desired CPU model.

### **Additional Notes**
- **Slot Limits**: The Compute Platform has a limit of 500 slots (vCPUs) per user.
- **GPU Best Practices**: Avoid over-allocating GPUs; a single user should use no more than 10 GPUs at a time.

For more details, refer to the [Job Execution Examples](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705182249/Job+Execution+Examples) documentation.

**Sources:** Interactive+Jobs+(srun), Job+Execution+Examples","5
",5,Job Execution Examples.md,RIS User Documentation/RIS User Documentation/Compute1/Job Execution Examples.md
188,My job can't find my files. How do I make my data in `/storage1` and `/scratch1` available to my container?,"To make your data in `/storage1` and `/scratch1` available to your container, you need to use the `LSF_DOCKER_VOLUMES` environment variable to specify which storage volumes to mount. Here’s how you can do it:

1. **Set the Environment Variables**:
   Define the paths to your storage and scratch directories. For example:
   ```bash
   export SCRATCH1=/scratch1/fs1/${COMPUTE_ALLOCATION}
   export STORAGEN=/storageN/fs1/${STORAGE_ALLOCATION}/Active
   ```

2. **Specify the Volumes**:
   Use the `LSF_DOCKER_VOLUMES` variable to mount these directories into your container. For example:
   ```bash
   export LSF_DOCKER_VOLUMES=""$HOME:$HOME $STORAGEN:$STORAGEN $SCRATCH1:$SCRATCH1""
   ```

3. **Submit Your Job**:
   When submitting your job, ensure that the `LSF_DOCKER_VOLUMES` variable is set. For example:
   ```bash
   bsub -G ${group_name} -Is -q general-interactive -a 'docker(ubuntu)' /bin/bash
   ```

This will make the specified directories available inside your container, allowing your job to access the files stored in `/storage1` and `/scratch1`.

**Sources:** Access+Storage+Volumes, FAQ","4
",4,Job Execution Examples.md,RIS User Documentation/RIS User Documentation/Compute1/Job Execution Examples.md
189,"I submitted hundreds of jobs, but why are only a few of them running at the same time?","The reason only a few of your jobs are running at the same time is likely due to job group limits or queue restrictions. Here are the key points from the context that explain this:

1. **Job Groups**: If you're using job groups (created with `bgadd`), they have a limit on the number of running jobs at once. For example, if you set a limit of 10 running jobs with `bgadd -L 10`, only 10 jobs from that group will run simultaneously.

2. **Default Job Group**: Even if you didn't explicitly create a job group, a default one is created with a limit of 5 running jobs. This means only 5 jobs will run at a time unless you modify this limit.

3. **Queue Restrictions**: Some queues may have their own limits on the number of concurrent jobs to prevent resource monopolization.

To check or modify your job group limits, you can use the `bjgroup` and `bgmod` commands as shown in the context. For example:
- Check job group status: `bjgroup -s /${compute_username}/${group_name}`
- Modify running job limit: `bgmod -L 5 /${compute_username}/${group_name}`

**Sources:** Compute2+General+Guidelines, Job+Execution+Examples","5
",5,Job Execution Examples.md,RIS User Documentation/RIS User Documentation/Compute1/Job Execution Examples.md
190,How do I get my data into my home or scratch directory to run a job?,"To get your data into your home or scratch directory to run a job, you can follow these steps:

1. **Access Your Home Directory**: Your home directory is located at `/home/${compute_username}` or a path like `/home/IDC-ID-123456`. This is where you can store scripts and configuration files.

2. **Access Your Storage Allocation**: Your storage allocation is located at `/storageN/fs1/${STORAGE_ALLOCATION}/Active`. This is a major storage location for your data.

3. **Access Your Scratch Space**: Your scratch space is located at `/scratch1/fs1/${COMPUTE_ALLOCATION}`. This is another storage location for your data.

4. **Mount Volumes in Interactive Jobs**: When starting an interactive job, you can mount these directories by setting the `LSF_DOCKER_VOLUMES` environment variable. For example:
   ```bash
   cd  # switch to your home directory
   export LSF_DOCKER_VOLUMES='/storageN/fs1/${STORAGE_ALLOCATION}/Active:/storageN/fs1/${STORAGE_ALLOCATION}/Active /scratch1/fs1/${COMPUTE_ALLOCATION}:/scratch1/fs1/${COMPUTE_ALLOCATION}'
   bsub -G ${group_name} -q general-interactive -Is -a 'docker(ubuntu)' /bin/bash
   ```

This will start an interactive job with access to your home directory, storage allocation, and scratch space, allowing you to run jobs with your data.

**Sources:** Job+Execution+Examples","3
",3,Space Management.md,RIS User Documentation/RIS User Documentation/Compute1/Space Management.md
191,Why do the `df` command and my mounted network drive show different amounts of used space for my storage allocation?,"The difference in reported used space between the `df` command and your mounted network drive can occur due to several reasons:

1. **Caching**: The Compute Service uses a caching interface to access data, which can affect how usage and quota are reported. The `df` command might show cached data, while the mounted drive shows actual usage.

2. **Protocol Differences**: The `df` command typically interacts with the POSIX filesystem, while the mounted network drive uses the SMB protocol. These protocols can report space differently due to how they handle metadata and file attributes.

3. **Timing**: There might be a delay in syncing data between the cached view (shown by `df`) and the actual storage (shown by the mounted drive).

For more detailed information on how caching affects usage and quota, you can refer to the [cache interfaces documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1683882153/FAQ#How-much-space-is-in-my-Storage-Allocation%3F). If the discrepancy persists, you may want to check the specific documentation for your storage platform or contact support for further assistance.

**Sources:** FAQ, Storage2+Access+Control","4
",4,Space Management.md,RIS User Documentation/RIS User Documentation/Compute1/Space Management.md
192,"What is the correct command to check my usage and quota for my home directory, scratch space, and storage allocation?","To check your usage and quota for your home directory, scratch space, and storage allocation, use the following commands:

1. **Home Directory Usage**:
   ```
   mmlsquota -u $(id -nu) --block-size auto rdcw-fs2:home1
   ```

2. **Scratch Space Usage** (for your compute group):
   ```
   groups | grep -Po 'compute-\S+' | while read COMPUTE_GROUP; do mmlsquota -g ""$COMPUTE_GROUP"" --block-size auto scratch1-fs1; done
   ```

3. **Storage Allocation Usage** (via SMB):
   ```
   smbclient -A .smb_creds -k //storageN.ris.wustl.edu/ris -c du
   ```
   Replace `storageN` with your specific storage platform (e.g., `storage1` or `storage2`).

For more details, refer to the [Space Management](https://washu.atlassian.net/wiki/spaces/RUD/pages/1844445524/Space+Management) documentation.

**Sources:** FAQ, Space+Management","5
",5,Space Management.md,RIS User Documentation/RIS User Documentation/Compute1/Space Management.md
193,How do I find which execution node my job is running on?,"To find out which execution node your job is running on, you can use the `squeue` command with the `--format` option to display the node information. Here’s how you can do it:

```java
squeue --job <job_id> --format=""%.18i %.9P %.8j %.8u %.2t %.10M %.6D %.12S %.10R""
```

Replace `<job_id>` with your actual job ID. This command will show you the node where your job is running under the `NODELIST` column.

Alternatively, you can use the `scontrol show job=<job_id>` command to get detailed information about your job, including the node it is running on:

```java
scontrol show job=<job_id>
```

This will display detailed job information, including the node name in the output.

**Sources:** Job+Execution+Examples, Monitoring+Jobs+and+Partitions_Queues","1
",1,Port Forwarding.md,RIS User Documentation/RIS User Documentation/Compute1/Port Forwarding.md
194,"Is the remote port always 8901, or how can I find the correct port for my application?","The remote port is not always 8901. The correct port for your application depends on how the job is configured. You can find the correct port by checking the port specified when running the GUI job or by using the `LSF_DOCKER_PORTS` environment variable to expose the necessary ports from within the container. For example, if you set `LSF_DOCKER_PORTS='8001:80'`, port 8001 on the host will forward to port 80 in the container. The context also mentions that ports 8000-8999 are available for containers to use.

**Sources:** Job+Execution+Examples, Port+Forwarding","2
",2,Port Forwarding.md,RIS User Documentation/RIS User Documentation/Compute1/Port Forwarding.md
195,"I followed the steps, but my browser says the connection was refused. What are common reasons for this?","Based on the context provided, a connection refusal in your browser could be due to several reasons:

1. **Port Forwarding Not Set Up Correctly**: Ensure that you have correctly set up port forwarding using the SSH command provided. Double-check the local port (e.g., 8080) and the remote port (e.g., 8901) in the command.

2. **Job Not Running on the Specified Exec Node**: Verify that the job you are trying to access is actually running on the exec node (compute1-exec-N) you specified in the SSH command.

3. **Firewall or Network Restrictions**: There might be firewall rules or network restrictions blocking the connection. Check with your local network administrator or WashU IT Support if you suspect this is the issue.

4. **Incorrect URL or Port**: Ensure that you are using the correct URL (`https://localhost:8080`) and the port you specified during the forwarding setup.

If you've verified these steps and are still facing issues, you may need to contact [WashU IT Support](https://it.wustl.edu/help/washu-it-support/) or your local network administrator for further assistance.

**Sources:** Port+Forwarding, Troubleshooting+Connection+to+the+Storage+Platforms","4
",4,Port Forwarding.md,RIS User Documentation/RIS User Documentation/Compute1/Port Forwarding.md
196,"Do I have to use CUDA 12.4, or can I use an older version for my application?","The context provided does not explicitly state that you must use CUDA 12.4. It mentions examples of appropriate base images that include CUDA 12.4, but it does not specify that older versions are not allowed.

If you have a specific reason to use an older version of CUDA, you may want to check with the [RIS Service Desk](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/43) to ensure compatibility with the Scientific Compute Platform. They can provide guidance on which versions are supported and recommended for your application.

**Sources:** THPC+Installed+Applications, Using+CUDA+in+Docker+Images","4
",4,Using CUDA in Docker Images.md,RIS User Documentation/RIS User Documentation/Compute1/Using CUDA in Docker Images.md
197,Is there a simpler command than running the OSU benchmark to quickly verify that my Docker container can access the GPU?,"To quickly verify that your Docker container can access the GPU without running the full OSU benchmark, you can use a simpler command like `nvidia-smi` inside your container. This command will display information about the GPU, confirming that your container has access to it.

Here’s how you can do it:

1. Start an interactive job with your Docker image.
2. Run the following command inside the container:
   ```bash
   nvidia-smi
   ```

This will show you the GPU status, driver version, and other relevant information, confirming that your container can access the GPU.

**Sources:** Using+CUDA+in+Docker+Images, Using+OFED+in+Docker+Images","5
",5,Using CUDA in Docker Images.md,RIS User Documentation/RIS User Documentation/Compute1/Using CUDA in Docker Images.md
198,"What NVIDIA driver version is installed on the compute nodes, and how does that affect which CUDA version I can use in my container?","The provided context does not specify the NVIDIA driver version installed on the compute nodes or provide direct information on how that affects the CUDA version you can use in your container. However, it does mention that you should use appropriate base images for CUDA, such as `nvidia/cuda:12.4.1-base-ubuntu22.04` or `nvidia/cuda:12.4.1-runtime-ubuntu22.04`.

For the most accurate and up-to-date information, you may need to consult the official documentation or contact the support team using the provided link: [Compute Resources Support](https://washu.atlassian.net/servicedesk/customer/portal/2/group/6/create/43).

**Sources:** Parabricks, Using+CUDA+in+Docker+Images","5
",5,Using CUDA in Docker Images.md,RIS User Documentation/RIS User Documentation/Compute1/Using CUDA in Docker Images.md
199,I'm having trouble connecting to Compute1 and I'm sure my password is correct. What else could be wrong?,"If you're sure your password is correct but still having trouble connecting to Compute1, here are a few things to check:

1. **VPN Connection**: Ensure you are connected to the correct VPN. Compute1 can be accessed via `msvpn.wusm.wustl.edu` or `danforthvpn.wustl.edu`. If you're off-campus, this is a requirement.

2. **SSH Command**: Verify that you are using the correct SSH command. It should look like this:
   ```
   ssh washukey@compute1-client-1.ris.wustl.edu
   ```
   Make sure you're using the correct client number (e.g., `compute1-client-1`).

3. **WashU Key**: Ensure you are using your WashU Key (single sign-on) and the associated password.

4. **Firewall or Network Issues**: Check if your firewall or network settings are blocking the connection. Try connecting from a different network if possible.

5. **Software**: If you're using Windows, ensure you have the correct software (e.g., PuTTY, MobaXterm, or the built-in command prompt/PowerShell). For Mac or Linux, use the terminal.

If you've checked all of these and are still having issues, you may need to contact WashU IT for further assistance. You can follow the directions [here](https://it.wustl.edu/items/connect/) to get help.

**Sources:** Compute1+Quickstart, Visual+Studio+Code+on+Compute1","3
",3,Visual Studio Code on Compute1.md,RIS User Documentation/RIS User Documentation/Compute1/Visual Studio Code on Compute1.md
200,"After opening a terminal, can I run my analysis or code directly?","Based on the provided context, you cannot run your analysis or code directly after opening a terminal in Visual Studio Code (VSC) on Compute1. You need to start an interactive job using a Docker image that contains the necessary software or development environment for your code. Here’s how you can do it:

1. **Open a Terminal in VSC**: Follow the steps to open a terminal in VSC as described in the context.
2. **Start an Interactive Job**: Use the `bsub` command with the appropriate Docker image to launch an interactive job. For example, if you are using R, you can run:
   ```bash
   bsub -G ${group_name} -Is -q general-interactive -a 'docker(r-base:<tag>)' R
   ```
   Replace `<tag>` with the version of R you want to use.

3. **Run Your Analysis or Code**: Once the interactive job is running, you can execute your analysis or code within that environment.

For more specific instructions, refer to the relevant sections in the context, such as the documentation for R, Python, or other tools you might be using.

**Sources:** Non-RIS+Docker+Images, Visual+Studio+Code+on+Compute1","4
",4,Visual Studio Code on Compute1.md,RIS User Documentation/RIS User Documentation/Compute1/Visual Studio Code on Compute1.md
201,"I've successfully connected, but how do I open my project folder to see my files?","To open your project folder and see your files after connecting to Compute1 via Visual Studio Code, follow these steps:

1. **Open Folder**: On the left side of the Visual Studio Code window, click the ""Open Folder"" button.
2. **Select Storage Allocation**: Choose your Storage allocation (e.g., `\\storageN.ris.wustl.edu\PI WashU Key or Share Name`). This will allow you to access your files and directories.
3. **Navigate and Edit**: Once the Storage allocation is selected, you can open and edit your files directly within Visual Studio Code.

If you need further assistance, feel free to ask!

**Sources:** Connecting+to+Storage+from+Windows, Visual+Studio+Code+on+Compute1","4
",4,Visual Studio Code on Compute1.md,RIS User Documentation/RIS User Documentation/Compute1/Visual Studio Code on Compute1.md
202,How can I access my files from the host machine inside my Docker container?,"To access files from the host machine inside your Docker container, you can use Docker's volume mounting feature. This allows you to share directories or files between the host and the container. Here’s how you can do it:

1. **Mount a Host Directory as a Data Volume**:
   Use the `-v` or `--volume` flag with the `docker run` command to mount a host directory into the container. For example:
   ```bash
   docker run -v /path/on/host:/path/in/container username/container-name:tag
   ```
   Replace `/path/on/host` with the directory path on your host machine and `/path/in/container` with the desired path inside the container.

2. **Using the Compute Platform**:
   If you are using the RIS Compute Platform, you can specify the volume mount in your `bsub` command. For example:
   ```bash
   bsub -Is -q workshop-interactive -G compute-workshop -a 'docker(username/container-name:tag)' -v /path/on/host:/path/in/container /bin/bash
   ```

3. **Environment Variables**:
   Ensure that any environment variables needed to access the files are correctly set within the container. You can use the `LSF_DOCKER_ENV_FILE` environment variable to set container-specific environment variables if needed.

By mounting the host directory as a volume, any changes made to the files in the container will be reflected on the host and vice versa. This is useful for sharing data, configuration files, or any other files between the host and the container.

**Sources:** Docker+Tutorial, Job+Execution+Examples","2
",2,Docker Wrapper Environment Variables.md,RIS User Documentation/RIS User Documentation/Compute1/Docker Wrapper Environment Variables.md
203,My Docker image already has a default command. How do I run it without specifying a command to bsub?,"To run a Docker image with its default command without specifying a command to `bsub`, you can simply omit the command part of the `bsub` command. The Docker image's default command (as defined in its `Dockerfile` or `ENTRYPOINT`) will be executed automatically.

Here is an example of how to submit a job using `bsub` without specifying a command:

```bash
bsub -G ${group_name} -q general-interactive -a 'docker(repo.example.com/repo_username/example_container_name:latest)'
```

This command will run the Docker image `repo.example.com/repo_username/example_container_name:latest` with its default command as defined in the image.

**Sources:** Docker+and+the+RIS+Compute1+Platform, Job+Execution+Examples","2
",2,Docker Wrapper Environment Variables.md,RIS User Documentation/RIS User Documentation/Compute1/Docker Wrapper Environment Variables.md
204,"I'm trying to expose a port from my container, but it's not working. Are there any restrictions?","Based on the provided context, there are specific restrictions and guidelines for exposing ports from containers on the compute cluster:

1. **Port Range**: Only ports 8000-8999 are available for containers to use.

2. **Resource Request**: You need to explicitly request the ports using the `-R` option with `bsub`. For example, to use port 8001, you would include `-R 'select[port8001=1]'` in your command.

3. **Port Forwarding**: The `LSF_DOCKER_PORTS` environment variable is used to map external ports to internal container ports. For example, `LSF_DOCKER_PORTS='8001:80'` maps external port 8001 to internal port 80.

4. **Multiple Ports**: For multiple ports, you must specify each port individually in the resource request, joined with `&&`. For example, `LSF_DOCKER_PORTS='8001-8003'` would require `-R 'select[port8001=1 && port8002=1 && port8003=1]'`.

If you're still having issues, ensure that:
- You are using the correct port range (8000-8999).
- You have properly requested the ports in your job submission.
- The port mapping is correctly specified in `LSF_DOCKER_PORTS`.

**Sources:** Job+Execution+Examples, Port+Forwarding","4
",4,Docker Wrapper Environment Variables.md,RIS User Documentation/RIS User Documentation/Compute1/Docker Wrapper Environment Variables.md
205,"What's the difference between the multi-stage and single-stage build, and why is the multi-stage build recommended?","The multi-stage and single-stage builds differ primarily in how they handle the compilation and final image composition:

1. **Multi-Stage Build**:
   - **Process**: Uses two separate stages. The first stage compiles the code using the Intel® Compiler Base image, and the second stage copies only the necessary binaries and runtime dependencies to a new, minimal base image.
   - **Advantages**:
     - Results in a smaller Docker image, reducing computing time, resources, and cost.
     - Keeps source code private by not including it in the final image.
   - **Example Use**: Compiling an MPI-enabled `Hello World` program and then creating a runtime image with only the compiled binary.

2. **Single-Stage Build**:
   - **Process**: Compiles the code and includes all build dependencies, binaries, and source code in a single image.
   - **Disadvantages**:
     - Results in a larger Docker image, which can increase computing time, resources, and cost.
     - Source code is exposed in the final image, which may be undesirable for proprietary or sensitive code.

The multi-stage build is recommended because it optimizes the final image size and security by separating the build environment from the runtime environment, thus improving efficiency and reducing potential vulnerabilities.

**Sources:** Intel®+Compiler+Base, Intel®+Compiler+Base+Tutorial","5
",5,Intel® Compiler Base Tutorial.md,RIS User Documentation/RIS User Documentation/Compute1/Intel® Compiler Base Tutorial.md
206,"How do I modify the provided Dockerfile to compile my own C or MPI source code instead of the ""Hello World"" example?","To modify the provided Dockerfile to compile your own C or MPI source code instead of the ""Hello World"" example, follow these steps:

1. **Replace the Source Code**:
   - Replace the `mpi_hello_world.c` file with your own C or MPI source code file. Ensure the new file has the same name or update the `COPY` command in the Dockerfile to reflect the new filename.

2. **Update the Compilation Command**:
   - In the Dockerfile, locate the line that compiles the source code (e.g., `mpiicx -o mpi_hello_world mpi_hello_world.c`). Update this command to use your source code filename and desired output binary name.
   - For example, if your source code is named `my_mpi_program.c` and you want the output binary to be named `my_mpi_program`, the command would be:
     ```dockerfile
     mpiicx -o my_mpi_program my_mpi_program.c
     ```

3. **Update the Copy Command**:
   - Ensure the compiled binary is copied to the desired location. For example:
     ```dockerfile
     cp -f my_mpi_program /usr/local/bin
     ```

4. **Example Modified Dockerfile**:
   Here is an example of a modified Dockerfile for a multi-stage build:

   ```dockerfile
   # Begin Stage 1 with the base compiler image.
   FROM ghcr.io/washu-it-ris/compiler-base:ubuntu22-mofed5.8-oneapi2025 AS build

   ENV DEBIAN_FRONTEND=noninteractive

   # Additional dependencies for the Intel® MPI C compiler
   RUN apt-get update && apt-get install -y gcc

   # Change working directory
   WORKDIR /opt/my_mpi_program/src/

   # Copy the source code to the working directory
   COPY my_mpi_program.c .

   # Compile the source code
   RUN cd /opt/my_mpi_program/src/ && \
       . /opt/intel/oneapi/setvars.sh --force && \
       mpiicx -o my_mpi_program my_mpi_program.c && \
       cp -f my_mpi_program /usr/local/bin

   # Begin Stage 2 with a new base image.
   FROM ubuntu:22.04 AS runtime

   ENV DEBIAN_FRONTEND=noninteractive
   # Copy only the needed parts of Stage 1.
   COPY --from=build /usr/local/bin/my_mpi_program /usr/local/bin

   # Add any additional runtime dependencies here
   RUN apt-get update && apt-get install -y wget perl-base libnuma-dev \
       libgtk2.0 libatk1.0-0 libcairo2 gfortran tcsh libnl-3-dev \
       libmnl0 tcl tk \
       libusb-1.0-0-dev pciutils lsof ethtool libfuse2

   # Set MOFED version, OS version and platform
   ENV MOFED_VERSION=5.8-6.0.4.2
   ENV OS_VERSION=ubuntu22.04
   ENV PLATFORM=x86_64
   RUN mkdir /tmp/mofed && \
       cd /tmp/mofed && \
       wget -q http://content.mellanox.com/ofed/MLNX_OFED-${MOFED_VERSION}/MLNX_OFED_LINUX-${MOFED_VERSION}-${OS_VERSION}-${PLATFORM}.tgz && \
       tar -xvf MLNX_OFED_LINUX-${MOFED_VERSION}-${OS_VERSION}-${PLATFORM}.tgz && \
       MLNX_OFED_LINUX-${MOFED_VERSION}-${OS_VERSION}-${PLATFORM}/mlnxofedinstall \
         --user-space-only \
         --without-fw-update  \
         -q && \
       rm -rf /tmp/mofed

   # Install Intel OneAPI
   RUN apt-get update -y && apt-get install -y gpg-agent wget curl software-properties-common ca-certificates
   RUN wget -qO- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --yes --dearmor |  tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null
   RUN echo ""deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main"" |  tee /etc/apt/sources.list.d/oneAPI.list
   RUN apt-get update -y && apt-get install -y intel-oneapi-hpc-toolkit
   ```

5. **Build and Run**:
   - Build the Docker image using the modified Dockerfile.
   - Run the compiled binary using the appropriate command, such as:
     ```bash
     mpirun -np 20 /usr/local/bin/my_mpi_program
     ```

By following these steps, you can compile and run your own C or MPI source code using the provided Dockerfile structure.

**Sources:** Intel®+Compiler+Base, Intel®+Compiler+Base+Tutorial","5
",5,Intel® Compiler Base Tutorial.md,RIS User Documentation/RIS User Documentation/Compute1/Intel® Compiler Base Tutorial.md
207,"Can I change the number of processors for my job, and how would I modify the `bsub` and `mpirun` commands to do so?","Yes, you can change the number of processors for your job. Here’s how you can modify the `bsub` and `mpirun` commands to adjust the number of processors:

### For `bsub` (Compute1):
To specify the number of processors (slots) for your job, use the `-n` option in the `bsub` command. For example, to request 4 processors:
```bash
bsub -n 4 -q foo-condo -a 'docker(ubuntu)' /usr/bin/some_program
```

### For `mpirun` (Compute2):
For MPI jobs on Compute2, you can specify the number of tasks (processors) using the `--ntasks` option in the `sbatch` script. For example, to request 32 tasks:
```bash
#!/bin/bash
#SBATCH -N 2
#SBATCH --mem=10G
#SBATCH --ntasks=32
srun --mpi=pmix mpirun ./mympiprogram
```

For Intel MPI, the command would be similar:
```bash
#!/bin/bash
#SBATCH -N 2
#SBATCH --mem=10G
#SBATCH --ntasks=32
export I_MPI_PMI_LIBRARY=/cm/shared/apps/slurm/23.02.5/lib64/libpmi2.so
srun --mpi=pmi2 mpirun ./mympiprogram
```

### Important Notes:
- Ensure you do not exceed the slot limits (500 slots per user for Compute1).
- Adjust the memory and other resources accordingly based on your job requirements.

**Sources:** Compute2+MPI, Job+Execution+Examples","1
",1,Intel® Compiler Base Tutorial.md,RIS User Documentation/RIS User Documentation/Compute1/Intel® Compiler Base Tutorial.md
208,How do I use my custom Conda environment in a non-interactive batch script?,"To use your custom Conda environment in a non-interactive batch script, follow these steps:

1. **Define Environment Variables**: Set the necessary environment variables in your batch script to specify the paths for Conda environments and packages. For example:
   ```bash
   export CONDA_ENVS_DIRS=""/storageN/fs1/${STORAGE_ALLOCATION}/Active/conda/envs/""
   export CONDA_PKGS_DIRS=""/storageN/fs1/${STORAGE_ALLOCATION}/Active/conda/pkgs/""
   export LSF_DOCKER_VOLUMES=""/storageN/fs1/${STORAGE_ALLOCATION}/Active:/storageN/fs1/${STORAGE_ALLOCATION}/Active""
   export PATH=""/opt/conda/bin:$PATH""
   ```

2. **Initialize Conda**: Ensure Conda is initialized in your script. Add the following to your batch script:
   ```bash
   # >>> conda initialize >>>
   # !! Contents within this block are managed by 'conda init' !!
   __conda_setup=""$('/opt/conda/bin/conda' 'shell.bash' 'hook' 2> /dev/null)""
   if [ $? -eq 0 ]; then
       eval ""$__conda_setup""
   else
       if [ -f ""/opt/conda/etc/profile.d/conda.sh"" ]; then
           . ""/opt/conda/etc/profile.d/conda.sh""
       else
           export PATH=""/opt/conda/bin:$PATH""
       fi
   fi
   unset __conda_setup
   # <<< conda initialize <<<
   ```

3. **Activate Your Conda Environment**: Activate the specific Conda environment you want to use in your batch script:
   ```bash
   conda activate sequencing
   ```

4. **Run Your Commands**: Execute the commands or scripts that require the Conda environment.

5. **Submit the Batch Job**: Use the `bsub` command to submit your batch script. For example:
   ```bash
   bsub -q general-interactive -a 'docker(continuumio/anaconda3:2021.11)' /path/to/your/batch_script.sh
   ```

By following these steps, you can ensure that your custom Conda environment is properly set up and used within a non-interactive batch script.

**Sources:** Create+Custom+Conda+Environment","4
",4,Create Custom Conda Environment.md,RIS User Documentation/RIS User Documentation/Compute1/Create Custom Conda Environment.md
209,My interactive session ended and now my Conda environment is gone. What went wrong?,"Based on the provided context, your Conda environment is likely gone because it was created within an interactive session that has now ended. Here's what happened and how to fix it:

1. **Issue**: The Conda environment was created in a temporary interactive session using a Docker container. When the session ended, the environment was lost because it wasn't persisted in your RIS Data Storage Platform.

2. **Solution**: To prevent this, you need to:
   - Create the environment in your persistent storage (as shown in the guide)
   - Use the `CONDA_ENVS_DIRS` environment variable to point to your storage location
   - The correct path should be something like `/storageN/fs1/${STORAGE_ALLOCATION}/Active/conda/envs/`

3. **Next Steps**:
   - Recreate the environment using the same `environment.yml` file
   - Make sure to set the proper environment variables before creating the environment
   - The environment will then persist between sessions

The key is to ensure your Conda environments are created in your persistent storage space rather than in the temporary Docker container.

**Sources:** Create+Custom+Conda+Environment","5
",5,Create Custom Conda Environment.md,RIS User Documentation/RIS User Documentation/Compute1/Create Custom Conda Environment.md
210,Do I need to run the `ipykernel install` command every time I want to use my environment in a new Jupyter Lab session?,"No, you do not need to run the `ipykernel install` command every time you want to use your environment in a new Jupyter Lab session. Once you have installed the IPython kernel for your conda environment, it will be available for selection in future Jupyter Lab sessions. However, you will need to repeat the installation process if you want to use a different conda environment with Jupyter Lab.

**Sources:** Create+Custom+Conda+Environment","5
",5,Create Custom Conda Environment.md,RIS User Documentation/RIS User Documentation/Compute1/Create Custom Conda Environment.md
211,How do I use job arrays to efficiently run many small tasks?,"To efficiently run many small tasks using job arrays, follow these guidelines based on the provided context:

1. **Submit Jobs as an Array**: Use a single `bsub` command to submit multiple similar jobs as an array. This allows you to manage them collectively or individually.

2. **Array Syntax**: Specify the array range in the job name. For example, to submit 1000 jobs:
   ```bash
   bsub -J 'myArray[1-1000]' SCRIPT
   ```

3. **Access Individual Jobs**: Refer to specific jobs within the array using the job index. For instance, the tenth job can be accessed as `myArray[10]` or `123[10]` (if the job ID is 123).

4. **Use Environment Variables**: Each job in the array can determine its position using the `LSB_JOBINDEX` environment variable. This is useful for processing different input files:
   ```bash
   bsub -J 'ipr_array[1-800]' run_ipr ipr.\$LSB_JOBINDEX
   ```

5. **Input and Output Files**: For jobs requiring unique input files, name them with extensions matching the job index (e.g., `input.1`, `input.2`). Similarly, specify output files with placeholders for the job index and array job number:
   ```bash
   bsub -J 'chimp_submit[1-500]' -i 'chimp_stdin.%I' -o 'chimp_out.%J.%I' ncbi_submit chimp
   ```

6. **Maximum Array Size**: The maximum number of jobs allowed in a single array is 1000.

By following these steps, you can efficiently manage and run multiple small tasks using job arrays.

**Sources:** Compute2+General+Guidelines, Job+Execution+Examples","1
",1,Compute2 General Guidelines.md,RIS User Documentation/RIS User Documentation/Compute2/Compute2 General Guidelines.md
212,How can I use the temporary `/tmp/` directory inside my Slurm job script?,"To use the temporary `/tmp/` directory inside your Slurm job script on Compute2, you can follow these guidelines:

1. **Understand the Temporary Directory**: Every Slurm job gets its own temporary directory that is automatically cleaned up after the job completes. The path to this directory is `/tmp/`.

2. **Use `/tmp/` for Temporary Files**: You can use `/tmp/` to store temporary files or data that your job generates during its execution. For example:
   ```bash
   #!/bin/bash
   #SBATCH --job-name=example_job
   #SBATCH --output=example_output.txt

   # Create a temporary file in /tmp/
   echo ""This is a temporary file"" > /tmp/temp_file.txt

   # Use the temporary file in your job
   cat /tmp/temp_file.txt

   # The file will be automatically cleaned up after the job completes
   ```

3. **Best Practices**: Since `/tmp/` is cleaned up after the job, avoid storing important data there if you need to access it after the job finishes. Use other storage solutions for persistent data.

By following these steps, you can effectively use the `/tmp/` directory for temporary storage within your Slurm job scripts on Compute2.

**Sources:** Compute2+General+Guidelines, LSF+to+Slurm+Translator, Using+Slurm+in+Containers","5
",5,Compute2 General Guidelines.md,RIS User Documentation/RIS User Documentation/Compute2/Compute2 General Guidelines.md
213,What are the recommended methods for creating my own software environment for my jobs?,"Based on the provided context, the recommended methods for creating your own software environment for your jobs are:

1. **Using Container Technology**: Create your own environment using container technology like Docker. This allows you to encapsulate all your dependencies and ensures consistency across different environments.

2. **Encapsulating Dependencies**: If you choose not to use containers, make sure to encapsulate all your software dependencies within your job scripts or environment setup. This ensures that your jobs have everything they need to run correctly without relying on the host environment.

For specific steps on creating a Docker image and setting up your environment, you can refer to the ""Software Development Using Compute1"" documentation, which provides detailed instructions on using Docker for software development on the Compute Platform.

**Sources:** Compute2+General+Guidelines, Software+Development+Using+Compute1","4
",4,Compute2 General Guidelines.md,RIS User Documentation/RIS User Documentation/Compute2/Compute2 General Guidelines.md
214,Why is installing Slurm in the container recommended over mounting the host's installation?,"Installing Slurm in the container is recommended over mounting the host's installation because it ensures better compatibility and avoids potential issues with the container environment. Mounting the host's Slurm installation may not work well if the container environment cannot support the bare metal/host Slurm installation. By installing Slurm directly in the container, you ensure that the Slurm commands and dependencies are properly integrated with the container's environment, leading to more reliable and predictable behavior.

**Sources:** Compute2+General+Guidelines, Using+Slurm+in+Containers","5
",5,Using Slurm in Containers.md,RIS User Documentation/RIS User Documentation/Compute2/Using Slurm in Containers.md
215,What happens if the Slurm version in my container doesn't exactly match the host's version?,"If the Slurm version in your container doesn't exactly match the host's version, it could lead to compatibility issues. The documentation recommends installing the same version of Slurm in your containers as the one on the host to ensure proper functionality. Mismatched versions might cause unexpected behavior or errors when running Slurm commands within the container. It's best to align the versions to avoid potential problems.

**Sources:** Transitioning+Between+slurm+and+LSF, Using+Slurm+in+Containers","5
",5,Using Slurm in Containers.md,RIS User Documentation/RIS User Documentation/Compute2/Using Slurm in Containers.md
216,Is it necessary to add the `slurm` user to my container if I'm not the system administrator?,"Based on the provided context, if you're not the system administrator, it is still recommended to add the `slurm` user to your container. This is part of the best practice for installing Slurm in containers, as outlined in the documentation. The `slurm` user is necessary for the proper functioning of Slurm commands within your container environment. The example provided in the context shows how to add the `slurm` user to the `/etc/passwd` file in your container.

**Sources:** Compute2+General+Guidelines, Using+Slurm+in+Containers","4
",4,Using Slurm in Containers.md,RIS User Documentation/RIS User Documentation/Compute2/Using Slurm in Containers.md
217,"How do I start an interactive shell that has a specific amount of CPUs, memory, and a GPU?","To start an interactive shell with specific CPU, memory, and GPU resources using `srun`, you can use the following command:

```java
srun --pty --cpus-per-task=4 --mem=100G --gpus=1 /bin/bash
```

Here's what each option does:
- `--pty`: Allocates a pseudo-terminal for an interactive shell.
- `--cpus-per-task=4`: Requests 4 CPUs for the job.
- `--mem=100G`: Requests 100GB of memory.
- `--gpus=1`: Requests 1 GPU.
- `/bin/bash`: The shell you want to start.

You can adjust the values (4, 100G, 1) to match your specific requirements.

**Sources:** Interactive+Jobs+(srun), Job+Execution+Examples","4
",4,Interactive Jobs (srun).md,RIS User Documentation/RIS User Documentation/Compute2/Interactive Jobs (srun).md
218,"What are the default resource allocations (like CPUs, memory, and time limit) if I don't specify them in my `srun` command?","The provided context does not specify the default resource allocations (CPUs, memory, and time limit) for the `srun` command. To get this information, you may need to check the specific documentation for the Compute2 platform or consult the Slurm configuration on your system. If you have access to the system, you can also use the `sinfo` command to view the default partitions and their associated resource limits.

**Sources:** Compute2+General+Guidelines, Interactive+Jobs+(srun)","5
",5,Interactive Jobs (srun).md,RIS User Documentation/RIS User Documentation/Compute2/Interactive Jobs (srun).md
219,"How can I see which nodes have specific features, like 'amd' or 'intel' CPUs, before I submit my job?","To see which nodes have specific features like 'amd' or 'intel' CPUs before submitting your job, you can use the `sinfo` command in Slurm. This command provides information about the available nodes and their features.

Here’s how you can check for nodes with specific features:

1. **List all nodes with their features**:
   ```bash
   sinfo -o ""%N %C""
   ```
   This will display the node names (`%N`) and their CPU features (`%C`).

2. **Filter nodes with specific features**:
   - For AMD CPUs:
     ```bash
     sinfo -o ""%N %C"" | grep amd
     ```
   - For Intel CPUs:
     ```bash
     sinfo -o ""%N %C"" | grep intel
     ```

This will help you identify which nodes have the required features before you submit your job.

**Sources:** Interactive+Jobs+(srun), Job+Execution+Examples","4
",4,Interactive Jobs (srun).md,RIS User Documentation/RIS User Documentation/Compute2/Interactive Jobs (srun).md
220,What should I do if a specific LSF command option I'm using isn't working with the translator?,"If a specific LSF command option isn't working with the translator, you should be aware that the translator is not fully implemented and may not support all LSF commands and their options. In such cases, you have a few options:

1. **Use the `--dryrun` option**: This can help you see the equivalent Slurm command that the translator would generate, which might give you insight into how to manually adjust your command.

2. **Learn Slurm commands directly**: Since the translator has limitations, it might be beneficial to familiarize yourself with Slurm commands. You can refer to the documentation on [Transitioning Between slurm and LSF](https://washu.atlassian.net/wiki/spaces/RUD/pages/1705377883/Transitioning+Between+slurm+and+LSF) for a comparison of LSF and Slurm commands.

3. **Check the documentation**: The documentation mentions that the translator is a ""use at your own risk"" script, so if you encounter issues, it might be best to use native Slurm commands instead.

4. **Contact support**: If you're unsure how to proceed, you might want to reach out to the support team for assistance with specific commands or job submissions.

**Sources:** LSF+to+Slurm+Translator, Transitioning+Between+slurm+and+LSF","4
",4,LSF to Slurm Translator.md,RIS User Documentation/RIS User Documentation/Compute2/LSF to Slurm Translator.md
221,The documentation mentions the translator is 'use at your own risk'. How can I verify my LSF command is being translated correctly before running it?,"To verify your LSF command is being translated correctly before running it, you can use the `--dryrun` option with the `bsub` command. This will show you the equivalent Slurm command without actually executing it. For example:

```
bsub --dryrun -Is -a ""docker(ubuntu)"" bash -c ""cat /etc/os-release; hostname""
```

This will display the corresponding Slurm command (e.g., `srun --pty --container-image=ubuntu ...`) that would be executed, allowing you to verify the translation before running the actual job. The same approach works with `bjobs` and `bkill` commands by adding the `--dryrun` option.

**Sources:** LSF+to+Slurm+Translator, Transitioning+Between+slurm+and+LSF","5
",5,LSF to Slurm Translator.md,RIS User Documentation/RIS User Documentation/Compute2/LSF to Slurm Translator.md
222,"Does the translator support LSF commands other than `bsub`, `bjobs`, and `bkill`?","Based on the provided context, the LSF to Slurm translator primarily supports the translation of simple LSF commands such as `bsub`, `bjobs`, and `bkill` to their Slurm equivalents. The documentation explicitly states that the translator is not fully implemented for all LSF commands and their options. Therefore, it does not support LSF commands beyond these three.

**Sources:** LSF+to+Slurm+Translator, Transitioning+Between+slurm+and+LSF","5
",5,LSF to Slurm Translator.md,RIS User Documentation/RIS User Documentation/Compute2/LSF to Slurm Translator.md
223,"What is the difference between `srun` and `sbatch`, and when should I use each one?","The difference between `srun` and `sbatch` lies in how they are used for job submission in the Slurm workload manager:

1. **`srun` (Interactive Jobs)**:
   - Used for running interactive jobs where you want immediate access to a compute node.
   - Commands are executed directly on the allocated resources, and you get an interactive terminal session.
   - Suitable for quick tasks, debugging, or running commands that require user interaction.
   - Example: `srun --pty /bin/bash` starts an interactive shell on a compute node.

2. **`sbatch` (Batch Jobs)**:
   - Used for submitting non-interactive batch jobs that run in the background.
   - Jobs are defined in a script file, which includes the commands to be executed and Slurm directives (starting with `#SBATCH`).
   - Suitable for long-running tasks, automated workflows, or jobs that do not require immediate user interaction.
   - Example: `sbatch myscript.sh` submits a batch job defined in `myscript.sh`.

### When to Use Each:
- Use `srun` when you need an interactive session or want to test commands quickly on compute nodes.
- Use `sbatch` for submitting jobs that can run independently without user interaction, especially for long-running or resource-intensive tasks.

For more details, refer to the [Slurm documentation on `srun`](https://slurm.schedmd.com/srun.html) and [`sbatch`](https://slurm.schedmd.com/sbatch.html).

**Sources:** Batch+Jobs+(sbatch), Interactive+Jobs+(srun)","5
",5,Compute2 Quickstart.md,RIS User Documentation/RIS User Documentation/Compute2/Compute2 Quickstart.md
224,How do I access my storage files and data when running my job inside a container?,"To access your storage files and data when running your job inside a container on the RIS Compute Platform, you need to use the `LSF_DOCKER_VOLUMES` environment variable. This variable allows you to specify which storage volumes should be made visible to your compute job.

Here’s how you can do it:

1. **Set Up Environment Variables**:
   Define the paths to your storage and scratch spaces. For example:
   ```bash
   export SCRATCH1=/scratch1/fs1/${USER}
   export STORAGEN=/storageN/fs1/${USER}/Active
   ```

2. **Specify Volumes in `LSF_DOCKER_VOLUMES`**:
   Use the `LSF_DOCKER_VOLUMES` environment variable to mount the desired directories into your container. For example:
   ```bash
   export LSF_DOCKER_VOLUMES=""$HOME:$HOME $STORAGEN:$STORAGEN $SCRATCH1:$SCRATCH1""
   ```

3. **Submit Your Job**:
   When submitting your job, ensure that the `LSF_DOCKER_VOLUMES` variable is set. For example:
   ```bash
   bsub -G ${group_name} -Is -q general-interactive -a 'docker(ubuntu)' /bin/bash
   ```

4. **Access the Mounted Volumes**:
   Once your job is running inside the container, you can access the mounted volumes using the paths you specified. For example:
   ```bash
   ls -ld $HOME $STORAGEN $SCRATCH1
   ```

### Important Notes:
- The ""source"" pathname (left of the colon) must be a path that exists outside the container on a GPFS filesystem (e.g., `/storageN/fs1/${USER}/Active` or `/home/${USER}`).
- The ""destination"" path (right of the colon) designates where the source appears within the container.
- Avoid using spaces in directory names as they can cause issues with mounting.
- It is best practice to declare `LSF_DOCKER_VOLUMES` for each job individually rather than including it in your `.bashrc` or `.bash_profile`.

For more detailed information, refer to the [Access Storage Volumes](https://washu.atlassian.net/wiki/spaces/RUD/pages/1794965573/Access+Storage+Volumes) documentation.

**Sources:** Access+Storage+Volumes, FAQ","1
",1,Compute2 Quickstart.md,RIS User Documentation/RIS User Documentation/Compute2/Compute2 Quickstart.md
225,How do I know which partition to use for my job?,"To determine which partition to use for your job on the Compute2 platform, you can use the `sinfo` command to check the available partitions and their current status. Here’s how you can do it:

1. **Check Available Partitions**: Run the following command to list all available partitions, their availability, and other details:
   ```bash
   sinfo
   ```
   This will display information about each partition, including whether it is up, the time limit, the number of nodes, and their states (e.g., idle, allocated, drain).

2. **Understand Partition Types**: The output of `sinfo` will show different partitions like `general`, `general-short`, `subscription`, etc. Each partition may have specific characteristics such as time limits or resource allocations. For example:
   - `general`: Typically for longer-running jobs with no strict time limit.
   - `general-short`: For shorter jobs with a time limit (e.g., 30 minutes).
   - `subscription`: May be reserved for specific user groups or subscriptions.

3. **Choose the Appropriate Partition**: Based on your job requirements (e.g., runtime, resource needs), select the partition that best fits your needs. For instance:
   - If your job will run for a short duration, use `general-short`.
   - If your job requires more time or specific resources, use `general` or another appropriate partition.

4. **Additional Information**: You can also use the `sjstat` command for more detailed information about the scheduling pools and their resources:
   ```bash
   sjstat -c
   ```

By using these commands, you can make an informed decision about which partition to use for your job. If you need further assistance or have specific requirements, feel free to ask!

**Sources:** Job+Execution+Examples, Monitoring+Jobs+and+Partitions_Queues","4
",4,Compute2 Quickstart.md,RIS User Documentation/RIS User Documentation/Compute2/Compute2 Quickstart.md
226,Do I need to specify the number of processes with an option like `-np` for `mpirun`?,"Based on the provided context, the number of processes for `mpirun` is typically specified using Slurm directives in the batch script rather than an `-np` option directly with `mpirun`. For example, in the OpenMPI and Intel MPI examples, the number of tasks (processes) is controlled by the Slurm options like `-N` (number of nodes) and `--ntasks-per-node` (number of tasks per node).

Here's a summary from the context:
- The `-N` option specifies the number of nodes.
- The `--ntasks-per-node` option specifies the number of tasks (processes) per node.

For example, in the OpenMPI script:
```java
#!/bin/bash
#SBATCH -N 2          # Number of nodes
#SBATCH --mem=10G
#SBATCH --ntasks-per-node=16  # Number of tasks (processes) per node
srun --mpi=pmix mpirun ./mympiprogram
```

This setup will run a total of 32 processes (2 nodes × 16 tasks per node). The `-np` option is not explicitly used in these examples.

**Sources:** Compute2+MPI, Parallel+Computing","5
",5,Compute2 MPI.md,RIS User Documentation/RIS User Documentation/Compute2/Compute2 MPI.md
227,"For Intel MPI, will I need to update the `I_MPI_PMI_LIBRARY` path when the Slurm version changes?","Yes, you will need to update the `I_MPI_PMI_LIBRARY` path when the Slurm version changes. The path provided in the context (`/cm/shared/apps/slurm/23.02.5/lib64/libpmi2.so`) is specific to Slurm version 23.02.5. If the Slurm version is updated, the path to the `libpmi2.so` library will likely change to reflect the new version number. Therefore, you should verify and update the path accordingly to ensure Intel MPI can locate the correct library.

**Sources:** Compute2+MPI, Using+Slurm+in+Containers","5
",5,Compute2 MPI.md,RIS User Documentation/RIS User Documentation/Compute2/Compute2 MPI.md
228,Why do the examples use `srun` to launch `mpirun` instead of just using `mpirun` directly?,"The examples use `srun` to launch `mpirun` because the Compute2 Slurm cluster requires specific Process Management Interface (PMI) options (`--mpi=pmix` for OpenMPI and `--mpi=pmi2` for Intel MPI) that can only be specified with `srun`. These options are necessary for proper MPI job execution on the cluster. Simply using `mpirun` directly would not provide the required PMI integration with the Slurm workload manager.

**Sources:** Compute2+MPI, Interactive+Jobs+(srun)","5
",5,Compute2 MPI.md,RIS User Documentation/RIS User Documentation/Compute2/Compute2 MPI.md
229,Where can I find information about the available storage platforms?,"You can find information about the available storage platforms in the ""Storage Platforms"" section of the documentation. The current available storage platforms are `storage1` and `storage2`. This information is mentioned in the context under the ""Storage Platforms"" heading.

**Sources:** Storage+Platforms, Storage+Platforms+on+Compute2","1
",1,Storage Platforms on Compute2.md,RIS User Documentation/RIS User Documentation/Compute2/Storage Platforms on Compute2.md
230,When is this documentation expected to be complete?,"The documentation was last modified on 2025-07-11 for the ""Moving Data With Globus"" section and on 2025-07-10 for the ""LabArchives Notebook"" section. However, the context does not specify when the documentation is expected to be complete. If you need more up-to-date information, I recommend checking the latest updates on the [RIS User Documentation](https://washu.atlassian.net/wiki/spaces/RUD) page.

**Sources:** FAQ, LabArchives+Notebook, Moving+Data+With+Globus","4
",4,Storage Platforms on Compute2.md,RIS User Documentation/RIS User Documentation/Compute2/Storage Platforms on Compute2.md
231,Who can I contact for help with storage options in the meantime?,"For immediate assistance with storage options, you can contact the RIS Service Desk. They handle requests related to storage allocations, access control, and other storage-related inquiries. You can submit a service request through the RIS Service Desk portal.

**Sources:** Storage+Platforms, Storage2+Access+Control","5
",5,Storage Platforms on Compute2.md,RIS User Documentation/RIS User Documentation/Compute2/Storage Platforms on Compute2.md
232,How do I run a container on Compute2?,"To run a container on Compute2, you can use the `--container-image` option with either `srun` or `sbatch`. Here’s how:

1. **Using `srun` (for interactive jobs):**
   ```bash
   srun --container-image=IMAGE[:TAG] [command]
   ```
   Example:
   ```bash
   srun --container-image=python:3.9.21-alpine python /home/username/script.py
   ```

2. **Using `sbatch` (for batch jobs):**
   Create a job script (e.g., `job.slurm`) with the `--container-image` option:
   ```bash
   #!/bin/bash
   #SBATCH --job-name=container-job
   #SBATCH --container-image=IMAGE[:TAG]
   #SBATCH --output=output.log

   [command]
   ```
   Example:
   ```bash
   #!/bin/bash
   #SBATCH --job-name=python-job
   #SBATCH --container-image=python:3.9.21-alpine
   #SBATCH --output=python.log

   python /home/username/script.py
   ```
   Submit the job:
   ```bash
   sbatch job.slurm
   ```

### Additional Options:
- **Set the working directory inside the container:**
  ```bash
  --container-workdir=PATH
  ```
  Example:
  ```bash
  srun --container-image=python:3.9.21-alpine --container-workdir=/home/username python script.py
  ```

- **Mount storage allocations:**
  ```bash
  --container-mounts=SRC:DST[:FLAGS]
  ```
  Example:
  ```bash
  srun --container-image=python:3.9.21-alpine --container-mounts=/storage2/fs1/username/Active:/storage2/fs1/username/Active ls /storage2/fs1/username/Active
  ```

For more details, refer to the [Using Containers on Compute2](https://washu.atlassian.net/wiki/spaces/RUD/pages/1737392293/Using+Containers+on+Compute2) documentation.

**Sources:** Compute2+Quickstart, Using+Containers+on+Compute2","5
",5,Using Containers on Compute2.md,RIS User Documentation/RIS User Documentation/Compute2/Using Containers on Compute2.md
233,"What container technologies, like Docker or Singularity, are supported on Compute2?","Based on the provided context, Compute2 supports **Podman** as the preferred container technology. Podman is used to build and manage OCI containers (similar to Docker). The documentation does not mention support for Singularity on Compute2.

For more details, you can refer to the [Compute2 Podman documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/2087682094/Compute2+Podman).

**Sources:** Compute2+Podman, Using+Containers+on+Compute2","3
",3,Using Containers on Compute2.md,RIS User Documentation/RIS User Documentation/Compute2/Using Containers on Compute2.md
234,Where can I find the current documentation or examples since this page is still being developed?,"The current documentation for software development using Compute1 is available on the RIS User Documentation page. You can find it here: [Software Development Using Compute1](https://washu.atlassian.net/wiki/spaces/RUD/pages/1786937372/Software+Development+Using+Compute1).

This documentation provides an introduction to using the RIS Compute Platform for software development, including examples of creating a development environment and versioning software. All examples are done in Perl.

**Sources:** RIS+User+Documentation, Software+Development+Using+Compute1","1
",1,Using Containers on Compute2.md,RIS User Documentation/RIS User Documentation/Compute2/Using Containers on Compute2.md
235,How do I translate my slurm memory request (`--mem-per-cpu`) to an LSF memory request (`-M`)? What are the units for the `-M` flag?,"To translate a Slurm memory request (`--mem-per-cpu`) to an LSF memory request (`-M`), you need to understand the units for each flag.

- In Slurm, `--mem-per-cpu` specifies the memory per CPU in **megabytes (MB)**.
- In LSF, the `-M` flag specifies the total memory for the job in **kilobytes (KB)**.

For example, if you have a Slurm command like:
```bash
#SBATCH --mem-per-cpu=100
```
This requests 100 MB of memory per CPU. To translate this to LSF, you would use:
```bash
#BSUB -M 102400
```
Here, `102400 KB` is equivalent to `100 MB` (since \(100 \times 1024 = 102400\)).

So, to convert from Slurm's MB to LSF's KB, multiply the Slurm value by 1024.

**Sources:** LSF+to+Slurm+Translator, Transitioning+Between+slurm+and+LSF","4
",4,Transitioning Between slurm and LSF.md,RIS User Documentation/RIS User Documentation/Compute2/Transitioning Between slurm and LSF.md
236,"Is specifying a Docker container with the `-a` flag required for all jobs, or only on Compute1?","Specifying a Docker container with the `-a` flag is required for jobs on Compute1. The context discusses the use of Docker with the IBM Spectrum LSF software job scheduler specifically for the Compute1 platform, indicating that this is a requirement for jobs run on Compute1.

**Sources:** Docker+Wrapper+Environment+Variables, Docker+and+the+RIS+Compute1+Platform","4
",4,Transitioning Between slurm and LSF.md,RIS User Documentation/RIS User Documentation/Compute2/Transitioning Between slurm and LSF.md
237,How can I see a list of available submission queues (`-q`) and compute groups (`-G`) that I can use?,"To see a list of available submission queues (`-q`) and compute groups (`-G`) that you can use, you can follow these steps:

1. **For Compute Groups (`-G`)**:
   - Your compute group name is typically provided to you when your compute access is activated. It is often associated with your lab or research group, commonly in the format of a PI's washukey (e.g., `compute-ris` for the RIS group).
   - If you are unsure of your compute group name, you can check with your lab administrator or the RIS support team.

2. **For Submission Queues (`-q`)**:
   - The available queues depend on your subscription type. The main queues are:
     - `general`: Available to all active compute users.
     - `${condo-name}`: Available if you have a purchased condo.
     - `subscription`: Available if you have a subscription tier (Tier 1, Tier 2, or Tier 3).
   - To see the specific queues available to you, you can use the `bqueues` command, which lists all available queues and their statuses. For example:
     ```bash
     bqueues
     ```
   - This command will display a list of queues, and you can identify which ones you have access to based on your subscription.

3. **Additional Information**:
   - For more details on the different subscription types and their associated queues, you can refer to the [Compute1 Condo and Subscription Queues documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1704886361/Compute1+Condo+and+Subscription+Queues).
   - If you are part of a condo group, ensure that you use the correct condo queue name and compute group name when submitting jobs.

By following these steps, you can determine the available submission queues and compute groups for your use.

**Sources:** Compute1+Condo+and+Subscription+Queues, Job+Execution+Examples","2
",2,Transitioning Between slurm and LSF.md,RIS User Documentation/RIS User Documentation/Compute2/Transitioning Between slurm and LSF.md
238,"How do I request multiple resources like CPUs, memory, and GPUs in the same job?","To request multiple resources like CPUs, memory, and GPUs in the same job on the Compute2 platform, you can combine the relevant options in your `srun` command. Here’s how you can do it:

1. **CPU Resources**: Use `-n` for the number of tasks or `-N` for nodes along with `--ntasks-per-node`.
2. **Memory**: Use `--mem` to specify the total memory required.
3. **GPUs**: Use `--gpus` to specify the number of GPUs.

### Example Command:
```java
srun -n 16 --ntasks-per-node=4 --mem=100G --gpus=2 <command>
```

### Explanation:
- `-n 16`: Requests 16 tasks.
- `--ntasks-per-node=4`: Specifies 4 tasks per node (assuming 4 nodes).
- `--mem=100G`: Requests 100GB of memory.
- `--gpus=2`: Requests 2 GPUs.

### Additional Options:
- **Partition/Queue**: Use `--partition=general` to specify the partition.
- **Constraints**: Use `--constraint=amd` or `--constraint=intel` to specify CPU type.
- **Specific Host**: Use `-w c2-node-001` to specify a particular host.

### Example with Additional Options:
```java
srun --partition=general -n 16 --ntasks-per-node=4 --mem=100G --gpus=2 --constraint=amd -w c2-node-001 <command>
```

This command requests 16 tasks (4 per node), 100GB of memory, 2 GPUs, AMD CPUs, and specifies the host `c2-node-001` in the `general` partition. Adjust the values as needed for your specific job requirements.

**Sources:** Interactive+Jobs+(srun), Job+Execution+Examples","2
",2,Batch Jobs (sbatch).md,RIS User Documentation/RIS User Documentation/Compute2/Batch Jobs (sbatch).md
239,How can I run a second job only after my first job has finished successfully?,"To run a second job only after your first job has finished successfully on the Compute2 platform, you can use job dependencies in Slurm. Here's how you can do it:

1. Submit your first job using `sbatch` and note its job ID (let's call it `JOBID1`).
2. When submitting your second job, specify that it should depend on the successful completion of the first job using the `--dependency=afterok:JOBID1` option.

Here's an example command:

```bash
sbatch --dependency=afterok:JOBID1 your_second_job_script.sh
```

This ensures that your second job will only start after the first job (`JOBID1`) has completed successfully. If the first job fails, the second job will not run.

**Sources:** Compute2+General+Guidelines, Monitoring+Jobs+and+Partitions_Queues","5
",5,Batch Jobs (sbatch).md,RIS User Documentation/RIS User Documentation/Compute2/Batch Jobs (sbatch).md
240,I've created a job array; how can my script use the array index to process a different file for each task?,"To use the array index in your script to process a different file for each task in a job array, you can utilize the `LSB_JOBINDEX` environment variable. This variable contains the index of the current job within the array.

Here’s how you can do it:

1. **Submit the Job Array**: When submitting your job array, ensure each job in the array can access its specific file. For example, if your files are named `input.1`, `input.2`, etc., you can reference them in your script using `LSB_JOBINDEX`.

   Example command to submit a job array:
   ```bash
   bsub -J 'myArray[1-100]' my_script.sh
   ```

2. **Use `LSB_JOBINDEX` in Your Script**: In your script (`my_script.sh`), use `LSB_JOBINDEX` to determine which file to process. For example:
   ```bash
   #!/bin/bash
   input_file=""input.$LSB_JOBINDEX""
   output_file=""output.$LSB_JOBINDEX""
   # Process the input file and generate the output file
   ```

3. **Example with Specific File Names**: If your input files are named differently, adjust the script accordingly. For instance, if your files are named `data_001.txt`, `data_002.txt`, etc., you can format the index to match:
   ```bash
   #!/bin/bash
   index=$(printf ""%03d"" $LSB_JOBINDEX)
   input_file=""data_$index.txt""
   output_file=""result_$index.txt""
   # Process the input file and generate the output file
   ```

By using `LSB_JOBINDEX`, each job in the array will process a different file based on its position in the array. This approach ensures that each task handles the correct input file and generates the corresponding output file.

**Sources:** Job+Execution+Examples","1
",1,Batch Jobs (sbatch).md,RIS User Documentation/RIS User Documentation/Compute2/Batch Jobs (sbatch).md
241,My job is stuck in a PENDING state. How can I find out why it's not running?,"To determine why your job is stuck in a PENDING state, you can use the `sinfo` command to check the status of the partitions/queues. This will show you the availability and state of the nodes, which can help identify if there are any issues with the partition or if resources are currently unavailable.

Here is the command you can use:
```java
sinfo
```

This will display information about the partitions, including their availability, time limits, number of nodes, and their current state (e.g., idle, down, drain). Look for the partition your job is submitted to and check for any issues indicated in the NODELIST or STATE columns.

Additionally, you can use the `squeue` command with the `--job` option followed by your job ID to get more details about your specific job:
```java
squeue --job <your_job_id>
```

This will provide information about the job's status, partition, and any potential reasons for it being in a PENDING state.

**Sources:** Compute2+Quickstart, Monitoring+Jobs+and+Partitions_Queues","4
",4,Monitoring Jobs and Partitions_Queues.md,RIS User Documentation/RIS User Documentation/Compute2/Monitoring Jobs and Partitions_Queues.md
242,"What are the differences between the `general`, `general-short`, and other partitions, and which one should I use?","Based on the provided context, here are the differences between the `general`, `general-short`, and other partitions on the Compute2 platform:

1. **General Partitions**:
   - **General**: This partition has an infinite time limit and is suitable for long-running jobs. It includes various node states like `idle`, `alloc`, `drain`, etc.
   - **General-preemptable**: This partition is similar to the general partition but is designed for jobs that can be preempted if higher-priority jobs need resources.

2. **General-short Partition**:
   - This partition has a time limit of 30 minutes and is ideal for short, quick jobs. It also includes various node states similar to the general partition.

3. **Other Partitions**:
   - **Subscription**: This partition is for users with subscription tiers and has an infinite time limit. It is designed for guaranteed resources based on the subscription tier (e.g., Tier 1, Tier 2, Tier 3).
   - **Consumption**: This partition is mentioned but has no available nodes (`n/a`) in the example output.

### Which One Should You Use?
- Use the **general** partition for long-running jobs that do not have strict time constraints.
- Use the **general-short** partition for short jobs that can be completed within 30 minutes.
- Use the **subscription** partition if you have a subscription tier and need guaranteed resources.
- The **general-preemptable** partition is suitable for jobs that can be interrupted if necessary.

For more detailed information, you can use the `sinfo` command to check the current status and availability of each partition.

**Sources:** Compute1+Condo+and+Subscription+Queues, Monitoring+Jobs+and+Partitions_Queues","3
",3,Monitoring Jobs and Partitions_Queues.md,RIS User Documentation/RIS User Documentation/Compute2/Monitoring Jobs and Partitions_Queues.md
243,Where can I find the output file for my job after it has completed?,"To find the output file for your job after it has completed, you can use the `-oo` option in the `bsub` command to specify the output file path. The output file path can include the job ID using `%J`, which gets substituted by the submitted job ID. For example:

```
bsub -G ${group_name} -q foo-condo -oo ${HOME}/output.%J.out -eo ${HOME}/error_out.%J.out -a 'docker(alpine)' /bin/true
```

In this example, the standard output will be redirected to `${HOME}/output.<job_id>.out` and the error output to `${HOME}/error_out.<job_id>.out`. You can then locate these files in the specified directory.

**Sources:** Job+Execution+Examples, Monitoring+Jobs+and+Partitions_Queues","1
",1,Monitoring Jobs and Partitions_Queues.md,RIS User Documentation/RIS User Documentation/Compute2/Monitoring Jobs and Partitions_Queues.md
244,How do I give a new lab member access to a specific project directory?,"To give a new lab member access to a specific project directory, follow these steps:

1. **Add the lab member to the appropriate AD group**:
   - For read-write access, add them to the `storage-key-project-rw` group (e.g., `storage-joe.user-firstproject-rw`).
   - For read-only access, add them to the `storage-key-project-ro` group (e.g., `storage-joe.user-firstproject-ro`).

2. **Ensure the group has the correct permissions**:
   - The project directory's ACLs should already be set up to grant the respective permissions to these groups. You can verify this using the `mmgetacl` command.

3. **Manage group membership**:
   - Keep the primary `storage-key` group small and trusted.
   - Ensure project-specific groups are specific to the dataset they need access to.

These steps will allow the new lab member to access the project directory with the appropriate permissions.

**Sources:** Storage1+Access+Control","5
",5,Storage1 Access Control.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Storage1 Access Control.md
245,"I moved a file between two project directories, but the quota usage looks wrong. Why did this happen?","The quota usage may appear incorrect because using commands like `rsync -a` or `mv` to move data between directories with set project quotas does not update the group ownership properly. This results in a discrepancy in the expected project quota usage.

To avoid this issue, ensure that the group ownership is correctly updated when moving files between project directories.

**Sources:** Storage1+Access+Control, Storage2+Access+Control","5
",5,Storage1 Access Control.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Storage1 Access Control.md
246,Why does `ls -l` show that my directory is owned by 'root' even though I have access to it?,"The `ls -l` command showing your directory as owned by 'root' is due to the way permissions are managed on the storage platform. Access is governed by NFSv4 ACLs rather than traditional POSIX permissions. Even though the directory appears to be owned by 'root', the actual access control is handled through group memberships and specific ACL entries.

In the context provided, when a user creates a new directory, the filesystem permissions from inside a container might show the directory as owned by 'root', but the NFSv4 ACLs ensure that the user and relevant groups have the appropriate access rights. The ownership displayed by `ls -l` does not necessarily reflect the actual access permissions, which are managed through ACLs and group memberships.

**Sources:** Storage1+Access+Control, Storage2+Access+Control","5
",5,Storage1 Access Control.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Storage1 Access Control.md
247,How do I know whether to use `storage1` or `storage2` in the folder path?,"To determine whether to use `storage1` or `storage2` in the folder path, consider the following:

1. **Platform Differences**:
   - **Storage1** uses IBM’s Scale Storage with GPFS ACLs.
   - **Storage2** uses Qumulo Core with NFSv4-compatible ACLs.

2. **Access Methods**:
   - **Storage1**: Access via `//storage1.ris.wustl.edu/` (SMB) or `/storage1/fs1/` (VFS/POSIX).
   - **Storage2**: Access via `//storage2.ris.wustl.edu/` (SMB) or `/storage2/fs1/` (VFS/POSIX).

3. **Purpose**:
   - Use `storage1` if you are working with older allocations or systems that rely on GPFS.
   - Use `storage2` for newer allocations or systems that use Qumulo Core.

4. **Check Documentation**:
   - Refer to specific project or allocation documentation to confirm which storage platform is assigned.

If you are unsure, you can also check with your system administrator or the relevant IT support team for clarification.

**Sources:** Storage2+Access+Control","1
",1,Connecting to Storage from Windows.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Connecting to Storage from Windows.md
248,The login window isn't appearing after I entered the path and hit Enter. What should I check?,"If the login window isn't appearing after entering the path and hitting Enter, here are a few things to check:

1. **Network Connection**: Ensure you have a stable connection to the WashU network. The login window may take some time to appear the first time you connect, so be patient.

2. **Correct Path**: Double-check that you've entered the correct UNC path in the format `\\storageN.ris.wustl.edu\PI WashU Key or Share Name`. Replace `storageN` with the appropriate storage platform (e.g., `storage1` or `storage2`).

3. **Credentials Format**: If the login window eventually appears, make sure to enter your credentials in the format `accounts\WashU Key` followed by your WashU Key password.

4. **Domain Users**: If your workstation is joined to the DOM or another Windows Domain, click on `Use another account` when prompted for credentials and then enter your credentials as `accounts\WashU Key`.

If you've verified all these steps and the issue persists, there might be a temporary network issue or a problem with the storage platform. In that case, you may need to contact your IT support for further assistance.

**Sources:** Connecting+to+Storage+from+MacOS, Connecting+to+Storage+from+Windows","4
",4,Connecting to Storage from Windows.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Connecting to Storage from Windows.md
249,"Can I map this as a network drive with a drive letter (e.g., Z:) instead of creating a shortcut?","The provided context does not explicitly mention mapping a network drive with a drive letter (e.g., Z:) on Windows or MacOS. However, based on general knowledge:

- **On Windows**: Yes, you can map a network drive with a drive letter. You can do this by opening File Explorer, right-clicking ""This PC,"" selecting ""Map network drive,"" and then entering the UNC path (e.g., `\\storageN.ris.wustl.edu\PI WashU Key or Share Name`) and choosing a drive letter.

- **On MacOS**: MacOS does not natively support assigning drive letters like Windows does. Instead, you can connect to the server using the ""Connect to Server"" option (as described in the context) and create shortcuts or login items for easier access.

If you need specific instructions for either operating system, please let me know!

**Sources:** Connecting+to+Storage+from+MacOS, Connecting+to+Storage+from+Windows","3
",3,Connecting to Storage from Windows.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Connecting to Storage from Windows.md
250,Why do I have to install and configure rclone on my local machine instead of directly on compute1?,"You need to install and configure rclone on your local machine because the initial setup process involves authenticating with WashU Box, which requires user interaction such as logging in and approving access through a browser. This interactive process is not feasible directly on the compute1 nodes, which are typically headless (without a graphical interface) and may not support browser-based authentication.

Once the configuration file is created on your local machine, you can then transfer it to your compute1 home directory to use rclone within the compute environment. This approach ensures that the authentication tokens and configuration details are properly set up before being used on the compute nodes.

**Sources:** Local+VNC+Viewer, Moving+Data+With+Rclone","5
",5,Moving Data With Rclone.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data With Rclone.md
251,"Can I use this guide to connect to other cloud storage providers, like Google Drive or Dropbox, instead of Box?","Yes, you can use this guide to connect to other cloud storage providers like Google Drive or Dropbox instead of Box. The guide explains how to use Rclone, which supports multiple cloud storage providers, including Google Drive and Dropbox. You would follow a similar process to configure Rclone for these providers by selecting the appropriate storage type during the configuration process.

**Sources:** Moving+Data+With+Rclone, Using+Globus+Connect+Personal","5
",5,Moving Data With Rclone.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data With Rclone.md
252,"After I copy the configuration file to compute1, will the commands for transferring data be the same no matter which operating system (Windows, macOS, etc.) I used for the initial setup?","The commands for transferring data to Compute1 will generally be the same regardless of the operating system (Windows, macOS, etc.) you used for the initial setup, as long as you are using SSH (Secure Shell) for the connection. SSH is a protocol that works consistently across different operating systems.

Once you have copied the configuration file to Compute1 and established an SSH connection, you can use standard command-line tools like `scp` (secure copy) or `rsync` to transfer files. These commands are available on all major operating systems and function similarly.

For example, to transfer a file to Compute1 using `scp`, you would use a command like this:
```bash
scp /path/to/local/file washukey@compute1-client-1.ris.wustl.edu:/path/to/remote/directory
```

Similarly, `rsync` can be used for more efficient file transfers:
```bash
rsync -avz /path/to/local/file washukey@compute1-client-1.ris.wustl.edu:/path/to/remote/directory
```

These commands will work the same way whether you are on Windows, macOS, or Linux, as long as you have the necessary SSH tools installed. On Windows, you might need to use tools like PuTTY or MobaXterm if you are not using the built-in command line.

**Sources:** Compute1+Quickstart, Visual+Studio+Code+on+Compute1","3
",3,Moving Data With Rclone.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data With Rclone.md
253,What is the difference between using Globus Connect Personal for my own computer versus using the main `storage1` Globus endpoint?,"The main differences between using Globus Connect Personal for your own computer and using the main `storage1` Globus endpoint are:

1. **Scope of Use**:
   - **Globus Connect Personal**: Designed for transferring data to and from your personal computer or local/scratch storage. It allows you to set up a personal endpoint on your machine, giving you control over which directories are accessible.
   - **Main `storage1` Endpoint**: Intended for transferring data to and from the central `storage1` platform provided by the institution. This is typically used for shared or institutional storage.

2. **Setup and Access**:
   - **Globus Connect Personal**: Requires you to download and install the software on your computer, then set up a personal endpoint through the Globus app. You can specify which directories on your computer are accessible.
   - **Main `storage1` Endpoint**: Pre-configured by the institution. You access it directly through the Globus web app or client without needing to install additional software on your personal computer.

3. **Permissions and Control**:
   - **Globus Connect Personal**: You have full control over the directories and files you make available for transfer. You can add or remove directories and set permissions (read-only or read-write) through the settings.
   - **Main `storage1` Endpoint**: Access is managed by the institution, and you typically have permissions based on your role or group membership within the institution.

4. **Use Case**:
   - **Globus Connect Personal**: Ideal for transferring files between your personal computer and other endpoints, including institutional storage or collaborators' endpoints.
   - **Main `storage1` Endpoint**: Best for transferring data to and from the institution's central storage, often used for collaborative projects or shared resources.

In summary, Globus Connect Personal is for personal or local data management, while the main `storage1` endpoint is for institutional storage access.

**Sources:** Globus+Connect+Personal, Using+Globus+Connect+Personal","5
",5,Using Globus Connect Personal.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Using Globus Connect Personal.md
254,"After setting up my endpoint on the compute platform, I can't connect to it. What information from the setup process must I use in the `bsub` run command?","To connect to your endpoint on the compute platform after setting it up, you'll need to use specific information in your `bsub` run command. Based on the context provided, here are the key details you should include:

1. **Port Information**: If your endpoint requires exposing specific ports, you need to use the `LSF_DOCKER_PORTS` environment variable to map the container's internal ports to external ports. For example:
   ```bash
   LSF_DOCKER_PORTS='8001:80' bsub -G ${group_name} -q general-interactive -R 'select[port8001=1]' -a 'docker(httpd)' /usr/local/bin/httpd-foreground
   ```

2. **Queue and Group**: Specify the queue (`-q`) and group (`-G`) you are using. For example:
   ```bash
   bsub -G ${group_name} -q general-interactive ...
   ```

3. **Container Image**: If you are using a Docker container, specify the container image with the `-a` option. For example:
   ```bash
   bsub -a 'docker(ubuntu)' ...
   ```

4. **Resource Requirements**: If your endpoint requires specific resources like memory, CPU, or GPU, you need to specify these using the `-R` option. For example:
   ```bash
   bsub -R 'rusage[mem=8GB]' -M 6GB ...
   ```

5. **Output and Error Files**: Redirect the output and error logs to specific files using `-oo` and `-eo` options. For example:
   ```bash
   bsub -oo ${HOME}/output.%J.out -eo ${HOME}/error_out.%J.out ...
   ```

6. **Job Name and Email**: Set a job name (`-J`) and specify an email (`-u`) to receive notifications. For example:
   ```bash
   bsub -J my_name -u myemail@wustl.edu ...
   ```

Ensure you have the correct permissions and that the host you are trying to connect to is available and not in a closed or unavailable state. You can check the host status using the `bhosts -w` command.

**Sources:** Compute2+Quickstart, Job+Execution+Examples",1,1,Using Globus Connect Personal.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Using Globus Connect Personal.md
255,"I've added a new folder or external drive to my collection, but I can't transfer files to it. How do I enable write access?","To enable write access to a new folder or external drive on a Mac, you need to ensure that the necessary configurations are set correctly. Based on the provided context, here are the steps you should follow:

1. **Disable .DS_Store File Writing**: Run the following command in the Terminal to prevent the writing of .DS_Store files to network mounts:
   ```
   defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true
   ```

2. **Update the `nsmb.conf` File**: Create or update the `/etc/nsmb.conf` file with the following content. You can use a text editor or run the following command in the Terminal:
   ```
   sudo tee /etc/nsmb.conf <<EOF
   [default]
   smb_neg=smb2_only
   dir_cache_off=yes
   notify_off=yes
   soft=yes
   streams=yes
   file_ids_off=yes
   EOF
   ```

3. **Synchronize the SMB Config**: Run the following command in the Terminal to synchronize the SMB configuration:
   ```
   /usr/libexec/smb-sync-preferences
   ```

4. **Connect to the Server**: Use the ""Connect to Server"" option in Finder (⌘ K) and enter the server address as `smb://storageN.ris.wustl.edu/PI WashU Key ID`.

5. **Enter Credentials**: Provide your WashU Key credentials when prompted.

6. **Make Desktop Shortcuts Permanent**: Follow the steps to add the drive to your Login Items in System Preferences to ensure it connects automatically when you log in.

If you still encounter issues, ensure that you have the necessary administrative privileges and that your network connection is stable. If the problem persists, you may need to check the specific permissions set on the folder or drive you are trying to access.

**Sources:** Connecting+to+Storage+from+MacOS, Connecting+to+Storage+from+Windows","1
",1,Using Globus Connect Personal.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Using Globus Connect Personal.md
256,How do I find the correct path for my files on the RIS Storage1 endpoint?,"To find the correct path for your files on the RIS Storage1 endpoint, you can use the following POSIX interface path on the Compute Platforms:

```
/storage1/fs1/${STORAGE_ALLOCATION}
```

You can check the available space and verify the path using the `mmlsquota` command while logged into the compute platform:

```
mmlsquota --block-size auto -j washukey_active rdcw-fs1
```

This will help you confirm the correct path and manage your storage allocation on Storage1.

**Sources:** FAQ, RIS+User+Documentation","2
",2,Moving Data With Globus CLI.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data With Globus CLI.md
257,The document shows how to find the 'RIS Storage1' endpoint. How can I find the endpoint ID for my local machine?,"The provided context does not contain specific information about finding the endpoint ID for 'RIS Storage1' on a local machine. However, based on the context, you can access the Storage Service via the SMB interface at `smb://storageN.ris.wustl.edu/${STORAGE_ALLOCATION}`.

For more detailed instructions or specific endpoint IDs, you may need to refer to additional documentation or contact the RIS Service Desk for assistance. You can reach them via the [RIS Service Desk](https://servicedesk.ris.wustl.edu/).

**Sources:** FAQ, RIS+User+Documentation","2
",2,Moving Data With Globus CLI.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data With Globus CLI.md
258,"After I run the `globus transfer` command, where do I find the task ID to check on its status?","After running the `globus transfer` command, the task ID is displayed in the command output. You can use this task ID with the `globus task show` command to check the status of your transfer. For example:

```
globus task show <task_id>
```

This will provide detailed information about the transfer, including its status, completion time, and other relevant details.

**Sources:** Moving+Data+With+Globus, Moving+Data+With+Globus+CLI","5
",5,Moving Data With Globus CLI.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data With Globus CLI.md
259,Why can't I see the `.snapshots` or `.snapshot` folder in my storage drive?,"The `.snapshots` or `.snapshot` folder is hidden by default, which is why you can't see it in your storage drive. To view it, you need to enable hidden files and folders on your system.

### For Mac:
Press `Command + Shift + Period` to show hidden files.

### For Windows 10:
Go to the `View` tab in File Explorer and check the box for `Hidden Items`.

Once you enable hidden files, the `.snapshots` or `.snapshot` folder should become visible.

**Sources:** Connecting+to+Storage+from+MacOS, Recovering+Data+From+Snapshots","5
",5,Recovering Data From Snapshots.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Recovering Data From Snapshots.md
260,How far back can I recover data from? How long are old snapshots kept?,"Based on the provided context, you can recover data from snapshots taken within the last 7 days. The snapshots are kept for self-service recovery for this duration. After that, the data may still be available on tape backups, but the process for recovery would be different and likely require assistance from the support team.

For Active data, if deleted from disk, it remains on tape for 30 days. For Archive data, if deleted from disk, it remains on tape for 10 days. However, these tape backups are not part of the self-service snapshot recovery process.

**Sources:** Recovering+Data+From+Snapshots, Storage+Platforms","1
",1,Recovering Data From Snapshots.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Recovering Data From Snapshots.md
261,Can I use the Globus recovery method for a storage allocation on Storage2?,"Based on the provided context, the Globus recovery method is specifically mentioned for Storage1. There is no explicit mention of using Globus for recovering data from Storage2. Therefore, it is not clear whether the Globus recovery method can be used for a storage allocation on Storage2.

For Storage2, the context describes manual and command line recovery methods, but does not mention Globus. You may want to check with your storage administrator or refer to additional documentation for clarification on using Globus with Storage2.

**Sources:** Recovering+Data+From+Snapshots, Storage2+Access+Control","4
",4,Recovering Data From Snapshots.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Recovering Data From Snapshots.md
262,What values should I use for `storageN` and `${VOLUME_NAME}`?,"Based on the provided context, the values you should use for `storageN` and `${VOLUME_NAME}` are as follows:

- **`storageN`**: This can be either `storage1` or `storage2`, as these are the currently available storage platforms mentioned in the documentation.

- **`${VOLUME_NAME}`**: This should be replaced with the specific name of your storage allocation. For example, if your storage allocation is named `example1`, you would use `/storage1/fs1/example1` or `/storage2/fs1/example1` depending on which storage platform you are using.

For example, if you are using `storage1` and your allocation name is `joe.user`, the path would be `/storage1/fs1/joe.user/Active`.

**Sources:** Access+Storage+Volumes, Storage2+Access+Control","3
",3,Connecting to Storage from Linux.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Connecting to Storage from Linux.md
263,"What is the difference between using `mount` and `smbclient`, and when should I use one over the other?","The difference between using `mount` and `smbclient` for connecting to storage from Linux is as follows:

- **Using `mount`**:
  - This method mounts the storage volume directly to your file system, making it accessible as if it were a local directory.
  - It is useful for persistent access to the storage, allowing you to interact with files using standard file operations.
  - Requires the `cifs.utils` package and involves creating a credentials file for authentication.

- **Using `smbclient`**:
  - This method provides a command-line interface to interact with the storage volume, allowing you to list, upload, and download files.
  - It is useful for one-off operations or when you need to perform specific actions without mounting the entire volume.
  - Requires the `samba-client` package and can use either a credentials file or direct input of credentials.

**When to use one over the other**:
- Use `mount` if you need frequent and persistent access to the storage, treating it like a local directory.
- Use `smbclient` if you need to perform specific operations occasionally or prefer a command-line interface for file management.

**Sources:** Connecting+to+Storage+from+Linux, Connecting+to+Storage+from+MacOS","4
",4,Connecting to Storage from Linux.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Connecting to Storage from Linux.md
264,How can I make the storage mount automatically every time my computer reboots?,"To make the storage mount automatically every time your computer reboots, follow these steps based on your operating system:

### For MacOS:
1. Open **System Preferences** from the Dock or search for it using Spotlight.
2. Click on **Users and Groups**.
3. Select the **Login Items** tab.
4. Click the **+** button at the bottom left.
5. Navigate to your Research Drive using the sidebar and press **Add**.
6. The drive will now be listed in Login Items and will mount automatically upon reboot.

Additionally, to ensure the shortcut is permanent and you don't have to re-enter credentials, check the box for **Remember this password in my keychain** when prompted for your username and password.

### For Windows:
The context provided does not include specific instructions for Windows. However, you can typically achieve this by mapping the network drive with the **""Reconnect at sign-in""** option checked. Here’s how:
1. Open **File Explorer** and go to **This PC**.
2. Click **Map network drive** from the ribbon menu.
3. Enter the server address (e.g., `\\storage1.ris.wustl.edu\YourDriveName`).
4. Check the box for **Reconnect at sign-in**.
5. Enter your credentials if prompted and check **Remember my credentials**.

### For Linux:
The context does not provide instructions for Linux, but you can use `autofs` or add an entry to `/etc/fstab` to mount the drive automatically.

**Sources:** Connecting+to+Storage+from+MacOS, Recovering+Data+From+Snapshots","2
",2,Connecting to Storage from Linux.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Connecting to Storage from Linux.md
265,What should I use for `storageN` and my `PI WashU Key ID` in the server address?,"Based on the provided context, you should use one of the available storage platforms for `storageN`. The current available storage platforms are:

- `storage1`
- `storage2`

For your `PI WashU Key ID`, you should use your WashU Key ID, which is your WashU username. When prompted for credentials, enter it as `accounts\WashU Key` followed by your WashU Key password.

So, the server address would look like:
```
\\storage1.ris.wustl.edu\PI WashU Key or Share Name
```
or
```
\\storage2.ris.wustl.edu\PI WashU Key or Share Name
```

**Sources:** Connecting+to+Storage+from+Windows, FAQ","3
",3,Connecting to Storage from MacOS.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Connecting to Storage from MacOS.md
266,What should I do if I don't have administrative privileges to run the initial setup commands in Terminal?,"If you don't have administrative privileges to run the initial setup commands in Terminal, you won't be able to complete the steps that require `sudo` (such as creating or updating the `/etc/nsmb.conf` file). Here are your options:

1. **Request Administrative Access**: Contact your system administrator or IT support to request temporary administrative privileges to perform these tasks.

2. **Use a Different Method**: If administrative access is not possible, you may need to use an alternative method to connect to the storage, such as contacting your IT department for assistance or using a different device where you have the necessary permissions.

3. **Check for Alternative Instructions**: There might be alternative instructions or tools provided by your organization that do not require administrative privileges. Consult your IT support or documentation for these options.

**Sources:** Connecting+to+Storage+from+MacOS, Local+VNC+Viewer","4
",4,Connecting to Storage from MacOS.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Connecting to Storage from MacOS.md
267,"I followed the steps to make the connection permanent, but it still disappears when I lose my internet connection or asks for my password every time I log in. How can I fix this?","To address the issue of the storage connection disappearing when you lose your internet connection or asking for your password every time you log in, follow these additional steps:

1. **Ensure ""Always Allow"" is Selected**: When prompted for your username and password, make sure to select ""Always Allow"" to save your credentials in the keychain. This should prevent the system from asking for your password each time you log in.

2. **Check Keychain Settings**:
   - Open the ""Keychain Access"" application on your Mac.
   - Search for the entry related to the storage connection.
   - Ensure that the entry is set to ""Allow all applications to access this item.""

3. **Re-add the Login Item**:
   - Go to **System Preferences** > **Users and Groups** > **Login Items**.
   - Remove the existing Research Drive entry if it exists.
   - Click the ""+"" button and navigate to your Research Drive again, then add it back.

4. **Verify Network Settings**:
   - Ensure that your Mac is set to automatically reconnect to the network when the internet connection is restored.
   - Check your network settings to confirm that there are no issues with the connection stability.

If the issue persists after following these steps, you may need to contact the [Service Desk](https://servicedesk.ris.wustl.edu/) for further assistance.

**Sources:** Connecting+to+Storage+from+MacOS, Troubleshooting+Connection+to+the+Storage+Platforms","3
",3,Connecting to Storage from MacOS.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Connecting to Storage from MacOS.md
268,What should I use for `storageN` in the command?,"To determine the correct value for `storageN` in the command, you need to specify whether you are referring to **Storage1** or **Storage2**. The context provided explains the differences between these two storage platforms:

- **Storage1** is accessed via `//storage1.ris.wustl.edu/`.
- **Storage2** is accessed via `//storage2.ris.wustl.edu/`.

If you are working with **Storage2**, you should use `storage2` in your command. For example:
- For SMB protocol: `//storage2.ris.wustl.edu/example2`
- For VFS/”POSIX” protocol: `/storage2/fs1/example2`

If you are unsure which storage platform you are using, please clarify your context or provide more details about your specific use case.

**Sources:** Storage2+Access+Control","1
",1,Troubleshooting Connection to the Storage Platforms.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Troubleshooting Connection to the Storage Platforms.md
269,"My connection test was successful, but I still can't access my files. What should I do now?","Since your connection test was successful but you're still unable to access your files, the next step is to contact the Service Desk for further assistance. According to the documentation, you should reach out to them if you encounter issues with accessing your storage allocation after a successful connection test.

You can contact the Service Desk [here](https://servicedesk.ris.wustl.edu/).

**Sources:** Connecting+to+Storage+from+Windows, Troubleshooting+Connection+to+the+Storage+Platforms","5
",5,Troubleshooting Connection to the Storage Platforms.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Troubleshooting Connection to the Storage Platforms.md
270,Why do I need to contact a different support team if the test fails versus if it succeeds?,"If the connection test to the storage service succeeds (e.g., `TcpTestSucceeded : True` on Windows or a successful `nc` test on Mac/Linux), it indicates that the network connection to the storage service is working correctly. In this case, if you're still experiencing issues accessing your storage allocation, the problem might be specific to your account, permissions, or the storage service itself. Therefore, you should contact the RIS Service Desk for further assistance.

If the test fails (e.g., `TcpTestSucceeded : False` on Windows or a failed `nc` test on Mac/Linux), it suggests there is a broader network connectivity issue. This could be due to firewall settings, network configuration, or other infrastructure problems. In such cases, you should contact WashU IT Support or your local network administrator, as they can help troubleshoot and resolve network-related issues.

In summary, the support team you contact depends on whether the problem is likely related to your account or the storage service (RIS Service Desk) or to network connectivity (WashU IT Support/local network administrator).

**Sources:** FAQ, Troubleshooting+Connection+to+the+Storage+Platforms","5
",5,Troubleshooting Connection to the Storage Platforms.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Troubleshooting Connection to the Storage Platforms.md
271,"I see how to view the permissions (ACLs) on my files in Storage2, but how do I change them or give access to a new user?","The provided context does not include information on how to change permissions (ACLs) or grant access to new users on Storage2. However, based on general knowledge, you typically use commands like `nfs4_setfacl` for NFS or `smbcacls` for SMB to modify ACLs. For specific instructions, you may need to consult additional documentation or contact your system administrator.

**Sources:** Storage2+Access+Control","3
",3,Storage2 Access Control.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Storage2 Access Control.md
272,What are the specific steps to connect to my Storage2 share from a Windows or macOS computer?,"To connect to your Storage2 share from a Windows or macOS computer, follow these steps:

### For Windows:
1. **Browse the Network Folder**:
   - Open Windows File Explorer.
   - Enter the UNC path in the address bar: `\\storage2.ris.wustl.edu\PI WashU Key or Share Name` and press Enter.
   - If prompted for credentials, enter `accounts\WashU Key` and your WashU Key password.

2. **Create a Desktop Shortcut (Optional)**:
   - With the storage drive open, hover over the drive name and click and drag to create a shortcut on your Desktop, Start menu, taskbar, or File Explorer Quick Access tab.

### For macOS:
1. **Prepare Your Mac**:
   - Disable the writing of `.DS_Store` files by running in Terminal:
     ```bash
     defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true
     ```
   - Create or update the `/etc/nsmb.conf` file:
     ```bash
     sudo tee /etc/nsmb.conf <<EOF
     [default]
     smb_neg=smb2_only
     dir_cache_off=yes
     notify_off=yes
     soft=yes
     streams=yes
     file_ids_off=yes
     EOF
     ```
   - Synchronize the SMB config:
     ```bash
     /usr/libexec/smb-sync-preferences
     ```

2. **Connect to the Server**:
   - Click on the desktop background to open Finder.
   - From the menu, select ""Go"" > ""Connect to Server"" (or press ⌘ K).
   - Enter the server address: `smb://storage2.ris.wustl.edu/PI WashU Key ID`.
   - Enter your WashU Key credentials when prompted.

3. **Make the Shortcut Permanent (Optional)**:
   - Open System Preferences > ""Users and Groups"" > ""Login Items"".
   - Click the ""+"" button and navigate to your Research Drive to add it to the login items.

These steps will help you connect to your Storage2 share from either a Windows or macOS computer.

**Sources:** Connecting+to+Storage+from+MacOS, Connecting+to+Storage+from+Windows","3
",3,Storage2 Access Control.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Storage2 Access Control.md
273,The document warns me not to confuse Storage2 and fs2. What is fs2 and how is it different?,"The document warns about distinguishing between **Storage2** and **fs2** because they are distinct platforms. However, the context provided does not include specific details about what **fs2** is or how it differs from Storage2.

If you need more information about **fs2**, I recommend checking the relevant documentation or resources that specifically address **fs2** and its differences from Storage2.

**Sources:** Storage2+Access+Control","5
",5,Storage2 Access Control.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Storage2 Access Control.md
274,"I'm getting a ""permission denied"" error when I run `gsutil rsync`, even though I logged in successfully. What could be the problem?","The ""permission denied"" error when running `gsutil rsync` could be due to several reasons. Here are some potential causes and solutions based on the context provided:

1. **Incorrect Google Account Permissions**:
   - Ensure that the Google account you used (`washukey@wustl.edu`) has the necessary permissions to access the data in the Google Storage bucket. The account must be granted access by the data owner.

2. **Incorrect Destination Directory Permissions**:
   - Verify that you have write permissions to the destination directory on the RIS storage platform (`/storageN/fs1/${STORAGE_ALLOCATION}/Active/path/to/directory`). The context mentions that `storageN` can be either `storage1` or `storage2`, so ensure you are using the correct one.

3. **Docker Environment Issues**:
   - The context specifies that you need to launch a `bsub` job with the `google/cloud-sdk` Docker image. Ensure that you have correctly set up the Docker environment and volumes as described:
     ```bash
     bsub -Is -q general-interactive -a 'docker(google/cloud-sdk)' /bin/bash
     ```

4. **Active Account**:
   - If you have multiple Google accounts logged in, ensure that the correct account is set as active:
     ```bash
     gcloud config set account $GOOGLE_ACCOUNT
     ```

5. **Network or Firewall Issues**:
   - Ensure that there are no network or firewall restrictions preventing access to the Google Storage bucket or the RIS storage platform.

If you have verified all the above and are still facing issues, you may need to contact your system administrator or the data owner for further assistance.

**Sources:** Moving+Data+From+Google+Storage+to+RIS+Storage+via+gsutil+on+Compute1, Storage2+Access+Control","4
",4,Moving Data From Google Storage to RIS Storage via gsutil on Compute1.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data From Google Storage to RIS Storage via gsutil on Compute1.md
275,Why is it necessary to launch a `bsub` interactive job? Can't I just run the `gsutil` command directly on the login node?,"Launching a `bsub` interactive job is necessary for several reasons, especially when running resource-intensive commands like `gsutil` directly on the login node:

1. **Resource Limitations**: Login nodes are typically intended for light workloads and administrative tasks. Running heavy commands like `gsutil` directly on the login node can monopolize resources, affecting other users and potentially leading to performance issues or restrictions.

2. **Job Management**: Using `bsub` allows you to manage your jobs more effectively. You can monitor, pause, resume, or terminate jobs as needed. This is particularly useful for long-running or resource-intensive tasks.

3. **Access to Compute Resources**: Interactive jobs via `bsub` give you access to more substantial hardware resources, such as additional CPUs, memory, and storage, which are necessary for running commands like `gsutil` efficiently.

4. **Isolation and Stability**: Running jobs in a dedicated environment ensures that your tasks are isolated from the login node's primary functions, reducing the risk of instability or interference with other users' sessions.

5. **Scalability**: If your task requires significant resources or needs to be scaled, using `bsub` allows you to specify the required resources (CPU, memory, etc.) and ensures that your job is scheduled appropriately.

In summary, while you might be able to run `gsutil` directly on the login node for very light tasks, using `bsub` is the recommended approach for more demanding operations to ensure optimal performance and resource management.

**Sources:** Job+Execution+Examples","3
",3,Moving Data From Google Storage to RIS Storage via gsutil on Compute1.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data From Google Storage to RIS Storage via gsutil on Compute1.md
276,"When should I use the parallel transfer option (`-m`), and what kind of errors does the warning refer to?","The provided context does not contain any information about the parallel transfer option (`-m`) or related warnings and errors. Therefore, I cannot provide an accurate answer to your question based on the given context.

If you have any other questions or need information on a different topic covered in the context, feel free to ask!

**Sources:** Compute2+MPI, Parallel+Computing","3
",3,Moving Data From Google Storage to RIS Storage via gsutil on Compute1.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data From Google Storage to RIS Storage via gsutil on Compute1.md
277,How do I transfer files between my own computer and the RIS storage?,"To transfer files between your own computer and the RIS storage, you can use several methods depending on your operating system and preferences. Here are the common ways to do so:

1. **Using Globus**:
   - Globus is a high-performance data transfer service that can be used to move data to and from RIS storage. You can find detailed instructions on how to use Globus with RIS storage in the [Moving Data With Globus](https://washu.atlassian.net/wiki/spaces/RUD/pages/1795588152/Moving+Data+With+Globus) documentation.

2. **Using Rclone**:
   - Rclone is a command-line program to manage files on cloud storage. Instructions on how to use Rclone for transferring data to RIS storage can be found in the [Moving Data With Rclone](https://washu.atlassian.net/wiki/spaces/RUD/pages/1795588152/Moving+Data+With+Rclone) documentation.

3. **Using gsutil (for Google Storage to RIS Storage)**:
   - If you need to move data from Google Storage to RIS Storage, you can use the `gsutil` tool on Compute1. Detailed steps are provided in the [Moving Data From Google Storage to RIS Storage via gsutil on Compute1](https://washu.atlassian.net/wiki/spaces/RUD/pages/1795588152/Moving+Data+From+Google+Storage+to+RIS+Storage+via+gsutil+on+Compute1) documentation.

4. **Connecting to Storage from Your Operating System**:
   - **MacOS**: Follow the instructions in the [Connecting to Storage from MacOS](https://washu.atlassian.net/wiki/spaces/RUD/pages/1795588152/Connecting+to+Storage+from+MacOS) documentation.
   - **Linux**: Follow the instructions in the [Connecting to Storage from Linux](https://washu.atlassian.net/wiki/spaces/RUD/pages/1795588152/Connecting+to+Storage+from+Linux) documentation.
   - **Windows**: Follow the instructions in the [Connecting to Storage from Windows](https://washu.atlassian.net/wiki/spaces/RUD/pages/1795588152/Connecting+to+Storage+from+Windows) documentation.

For more detailed information, you can refer to the [RIS User Documentation](https://washu.atlassian.net/wiki/spaces/RUD/pages/1621885891/RIS+User+Documentation) or the [FAQ](https://washu.atlassian.net/wiki/spaces/RUD/pages/1683882153/FAQ) page.

**Sources:** FAQ, RIS+User+Documentation","1
",1,Moving Data With Globus.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data With Globus.md
278,"I've connected to the `RIS Storage1` collection, but I don't see my data. How do I find my specific storage allocation?","To find your specific storage allocation on `RIS Storage1`, you can use the following methods:

1. **Using the POSIX interface on Compute Platforms**:
   - Log into the compute platform.
   - Use the command:
     ```bash
     mmlsquota --block-size auto -j washukey_active rdcw-fs1
     ```
   - This will show your storage allocation and usage details.

2. **Using the SMB interface**:
   - If you have mounted the SMB share, you can use the `df` command on your workstation:
     ```bash
     df --output -h /storage2/fs1/${STORAGE_ALLOCATION}
     ```
   - Replace `${STORAGE_ALLOCATION}` with your specific allocation path.

3. **Check your allocation path**:
   - Your storage allocation is typically accessible at:
     ```bash
     /storage1/fs1/${STORAGE_ALLOCATION}
     ```
   - Replace `${STORAGE_ALLOCATION}` with your specific allocation name.

If you still can't find your data, you may need to verify your allocation details with the RIS Service Desk.

**Sources:** FAQ","1
",1,Moving Data With Globus.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data With Globus.md
279,"My transfer completed successfully, but where can I see a list of all the individual files that were moved?","To see a list of all the individual files that were moved in your completed transfer, you can use the Globus CLI (Command Line Interface). The `task show` command with the `-t/--successful-transfers` option will provide this information. Here’s how you can do it:

1. **Use the Globus CLI**: Ensure you have the Globus CLI installed or use the RIS-developed Docker image for Globus CLI.
2. **Run the Command**: Execute the following command, replacing `TASK_ID` with the ID of your completed transfer task:
   ```bash
   globus task show -t TASK_ID
   ```
3. **View the Output**: The command will display a list of successfully transferred files, showing the source and destination paths for each file.

For example:
```bash
globus task show -t 543e219c-01c1-11eb-81a3-0e2f230cc907
```
This will output something like:
```
Source Path         | Destination Path
------------------- | --------------------------------------------------------
/~/godata/file1.txt | /storage1/fs1/corcoran.william.p/Active/godata/file1.txt
/~/godata/file2.txt | /storage1/fs1/corcoran.william.p/Active/godata/file2.txt
/~/godata/file3.txt | /storage1/fs1/corcoran.william.p/Active/godata/file3.txt
```

**Important Notes**:
- This information is only available through the CLI and SDK, not the web UI.
- The task information is retained by Globus for 90 days.

**Sources:** Moving+Data+With+Globus, Moving+Data+With+Globus+CLI","5
",5,Moving Data With Globus.md,RIS User Documentation/RIS User Documentation/Storage Platforms/Moving Data With Globus.md
